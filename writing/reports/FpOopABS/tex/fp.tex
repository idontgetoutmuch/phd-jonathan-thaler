\chapter{Functional Programming}
MacLennan \cite{maclennan_functional_1990} defines Functional Programming as a methodology and identifies it with the following properties:

\begin{enumerate}
	\item It is programming without the assignment-operator.
	\item It allows for higher levels of abstraction.
	\item It allows to develop executable specifications and prototype implementations.
	\item It is connected to computer science theory.
	\item Parallel Programming.
	\item Suitable for AI.
\end{enumerate}

The last two points don't weight as heavy today as back in 1990 as other languages came up with features for better parallel programming but they all do it by introducing functional features.

MacLennan \cite{maclennan_functional_1990} defines properties of pure expressions 
\begin{itemize}
	\item Value is independent of the evaluation order.
	\item Expressions can be evaluated in parallel.
	\item Referential transparency.
	\item No side effects.
	\item Inputs to an operation are obvious from the written form.
	\item Effects to an operation are obvious from the written form.
\end{itemize}

Thus functional programming is identified as programming without the assignment operator and with pure expressions instead. Further characteristics are the missing of orderings as in imperative programming, caused by assignments: in functional programming the style is applicative which means we apply values to functions. The fundamental theoretical root is in the lambda calculus.

- cite critics

TODO: The question is then if we could implement in a functional style in an imperative object-oriented programming language? Or put otherwise: are these properties unique to functional programming or can we program functional in an imperative language (be it OO or not)? We can say that the functional style can be described as applicative programming: applying values to functions as described above. This functional style or applicative style applies to imperative languages as well and is not restricted to functional languages alone

distinguish between functional style and functional programming. when using a specific style one abuses language features to emulate a different paradigm than the one of the host language - so it is also possible to emulate oop in C or Haskell but this does not make them oop languages, one just emulate a programming-style (with potentially disastrous consequences as the code will probably become quite unreadable)

\cite{maclennan_functional_1990} defines functional programming as applicative programming with \textit{higher-order} functions. These are functions which operate themselves on functions: they can take functions as arguments, construct new functions and return them as values. This is in stark contrast to the \textit{first-order} functions as used in applicative or imperative programming which just operate on data alone.
Higher-order functions allow to capture frequently recurring patterns in functional programming in the same way like imperative languages captured patterns like GOTO, while-do, if-then-else, for. Common patterns in functional programming are the map, fold, zip, operators.

TODO: discuss function-composition, which is the equivalent of the ; operator in imperative languages. Function composition has no side-effects as opposed to the imperative ; operator which simply composes destructive assignment statements which are executed after another resulting in side-effects.
At the heart of modern functional programming is monadic programming which is polymorphic function composition: one can implement a user-defined function composition by allowing to run some code in-between function composition (TODO: explain using some code) - this code of course depends on the type of the Monad one runs in. This allows to emulate all kind of effectful programming in an imperative style within a pure functional language. Although it might seem strange wanting to have imperative style in a pure functional language, some problems are inherently imperative in the way that computations need to be executed in a given sequence with some effects. Also a pure functional language needs to have some way to deal with effects otherewise it would never be able to interact with the outside-world and would be practically useless. The real benefit of monadic programming is that it is explicit about side-effects and allows only effects which are fixed by the type of the monad - the side-effects which are possible are determined statically during compile-time by the type-system. Some general patterns can be extracted e.g. a map, zip, fold over monads which results in polymorphic behaviour - this is the meaning when one says that a language is polymorphic in its side-effects.

TODO: so functional programming is not really possible in this way in classic imperative languages e.g. C as you cannot construct new functions and return them as results from functions. The question is if it is possible in OO by using some OO features to work around the limitations of procedural languages.

TODO: algebraic reasoning. \cite{maclennan_functional_1990} page 233: "Much of the power of functional programming derives from its ability to manipulate programs algebraically by means of identities such as Eq. (6.6)"

\cite{allen_haskell_2016} defines Functional Programming as "a computer programming paradigm that relies on functions modelled on mathematical functions." Further they explicate that it is 
\begin{itemize}
	\item in Functional programming programs are combinations of expressions
	\item Functions are \textit{first-class} which means the can be treated like values, passed as arguments and returned from functions.
\end{itemize}

\section{Theoretical Foundation}
The theoretical foundation of Functional Programming is the Lambda Calculus, which was introduced by Alonzo Church in the 1930s. After some revision due to logical inconsistencies which were shown by Kleene and Rosser, Church published the untyped Lambda Calculus in 1936 which, together with a type-system (e.g. Hindler-Milner like in Haskell) on top is taken as the foundation of functional programming today.

\cite{maclennan_functional_1990} defines a calculus to be "... a notation that can be manipulated mechanically to achieve some end;...". The Lambda Calculus can thus be understood to be a notation for expressing computation based on the concepts of \textit{function abstraction}, \textit{function application}, \textit{variable binding} and \textit{variable substitution}. It is fundamentally different from the notation of a Turing Machine in the way it is applicative whereas the Turing Machine is imperative / operative. To give a complete definition is out of the scope of this text, thus we will only give a basic overview of the concepts and how the Lambda Calculus works. For an exhaustive discussion of the Lambda Calculus we refer to \cite{maclennan_functional_1990} and \cite{barendregt_lambda_1984}.

\paragraph{Function Abstraction}
Function abstraction allows to define functions in the Lambda Calculus. If we take for example the function $f(x) = x^2 - 3x + a$ we can translate this into the Lambda Calculus where it denotes: $\lambda x.x^2 - 3x + a$. The $\lambda$ symbol denotes an expression of a function which takes exactly one argument which is used in the body-expression of the function to calculate something which is then the result. Functions with more than one argument are defined by using nested $\lambda$ expressions. The function $f(x, y) = x^2 + y^2$ is written in the Lambda Calculus as $\lambda x.\lambda y.x^2 + y^2$.

\paragraph{Function Application}
When wants to get the result of a function then one applies arguments to the function e.g. applying $x = 3, y = 4$ to $f(x, y) = x^2 + y^2$ results in $f(3, 4) = 25$. Function application works the same in Lambda Calculus: $((\lambda x.\lambda y.x^2 + y^2) 3) 4 = 25$ - the question is how the result is actually computed - this brings us to the next step of variable binding and substitution.

\paragraph{Variable Binding}
In the function $f(x) = x^2 - 3x + a$ the variable $x$ is \textit{bound} in the body of the function whereas $a$ is said to be \textit{free}. The same applies to the lambda expression of $\lambda x.x^2 - 3x + a$. An important property is that bound variables can be renamed within their scope without changing the meaning of the expression: $\lambda y.y^2 - 3y + a$ has the same meaning as the expression $\lambda x.x^2 - 3x + a$. Note that free variable \textit{must not be renamed} as this would change the meaning of the expression. This process is called $\alpha$-conversion and it becomes sometimes necessary to avoid name-conflicts in variable substitution.

\paragraph{Variable Substitution}
To compute the result of a Lambda Expression - also called evaluating the expression - it is necessary to substitute the bound variable by the argument to the function. This process is called $\beta$-reduction and works as follows. When we want to evaluate the expression $((\lambda x.\lambda y.x^2 + y^2) 3) 4$ we first substitute 4 for x, rendering $(\lambda y.4^2 + y^2) 3$ and then 3 for y, resulting in $(4^2 + 3^2)$ which then ultimately evaluates to 25. Sometimes $\alpha$-conversion becomes necessary e.g. in the case of the expression $((\lambda x.\lambda y.x^2 + y^2) 3) y$ we must not substitute y directly for x. The result would be $(\lambda y.y^2 + y^2) 3 = 3^2 + 3^2 = 18$ - clearly a different meaning than intended (the first y value is simply thrown away). Here we have to perform $\alpha$-conversion before substituting y for x. \\ 
$((\lambda x.\lambda y.x^2 + y^2) 3) y = ((\lambda x.\lambda z.x^2 + z^2) 3) y$ and now we can substitute safely without risking a name-clash: $((\lambda x.\lambda z.x^2 + z^2) 3) y = (\lambda z.y^2 + z^2) 3) = (y^2 + 3^2) 3) = y^2 + 9$ where y occurs free.

\subsection*{Examples}
$(\lambda x.x)$ denotes the identity function - it simply evaluates to the argument. 

\medskip

$(\lambda x.y)$ denotes the constant function - it throws away the argument and evaluates to the free variable $y$. 

\medskip

$(\lambda x.xx)(\lambda x.xx)$ applies the function to itself (note that functions can be passed as arguments to functions - they are \textit{first class} in the Lambda Calculus) - this results in the same expression again and is thus a non-terminating expression.

\medskip

We can formulate simple arithmetic operations like addition of natural numbers using the Lambda Calculus. For this we need to find a way how to express natural numbers \footnote{In the short introduction for sake of simplicity we assumed the existence of natural numbers and the operations on them but in a pure lambda calculus they are not available. In programming languages which build on the Lambda Calculus e.g. Haskell, (natural) numbers and operations on them are built into the language and map to machine-instructions, primarily for performance reasons.}. This problem was already solved by Alonzo Church by introducing the Church numerals: a natural number is a function of an n-fold composition of an arbitrary function f. The number 0 would be encoded as $0 = \lambda f . \lambda x.x$, 1 would be encoded as $1 = \lambda f . \lambda x . f x$ and so on. This is a way of \textit{unary notation}: the natural number n is represented by n function compositions - n things denote the natural number of n.
When we want to add two such encoded numbers we make use of the identity $f^{(m+n)}(x) = f^m(f^n(x))$. Adding 2 to 3 gives us the following lambda expressions (note that we are using a sugared version allowing multiple arguments to a function abstraction) and reduces after 7 steps to the final result:

\medskip

$2 = \lambda f x . f(f x)$ \\
$3 = \lambda f x . f(f(f x))$ \\
$ADD = \lambda m n f x . m f (n f x)$ \\

ADD 2 3 \\
$1: (\lambda m n f x. m f (n f x)) (\lambda f x.f(f(f x))) (\lambda f x.f(f x))$ \\
$2:  (\lambda n f x. (\lambda f x.f(f(f x))) f (n f x))   (\lambda f x.f(f x))$ \\
$3:     (\lambda f x. (\lambda f x.f(f(f x))) f ((\lambda f x.f(f x)) f x)) $ \\ 
$4:     (\lambda f x.   (\lambda x.f(f(f x)))   ((\lambda f x.f(f x)) f x)) $ \\
$5:     (\lambda f x.       f(f(f(\lambda f x.f(f x)) f x)))))$ \\
$6:     (\lambda f x.       f(f(f  (\lambda x.f(f x)) x)))))$ \\
$7:     (\lambda f x.       f(f(f     (f(f x))  )))))$

\subsection{Types}
The Lambda Calculus as initially introduced by Church and presented above is \textit{untyped}. This means that the data one passes around and upon one operates has no type: there are no restriction on the operations on the data, one can apply all data to all function abstractions. This allows for example to add a string to a number which behaviour may be undefined thus leading to a non-reducible expression.
This led to the introduction of the simply typed Lambda Calculus which can be understood to add tags to a lambda-expression which identifies its type. One can then only perform function application on data which matches the given type thus ensuring that one can only operate in a defined way on data e.g. adding a string to a number is then not possible any-more because it is a semantically wrong expression.
The simply typed lambda calculus is but only one type-system and there are much more evolved and more powerful type-system e.g. \textit{System F} and \textit{Hindley-Milner Type System} which is the type-system used in Haskell. It is completely out of the scope of this text to discuss type systems in depth but we give a short overview of the most important properties.

Generally speaking, a type system defines types on data and functions. Raw data can be interpreted in arbitrary ways but a type system associates raw data with a type which tells the compiler (and the programmer) how this raw data is to be interpreted e.g. as a number, a character,... Functions have also types on their arguments and their return values which defines upon which types the function can operate. Thus ultimately the main purpose of a type system is to reduce bugs in a program. Very roughly one can distinguish between static / dynamic and strong / weak typing.

\paragraph{Static and dynamic typing}
A statically typed language performs all type checking at compile time and no type checking at runtime, thus the data has no type-information attached at all. Dynamic typing on the other hand performs type checking during run-time using type-information attached to values. Some languages use a mix of both e.g. Java performs some static type checking at compile time but also supports dynamic typing during run-time for downcasting, dynamic dispatch, late binding and reflection to implement object-orientation. Haskell on the other hand is strictly statically typed with no type checks at runtime.

\paragraph{Strong and weak typing}
A strong type system guarantees that one cannot bypass the type system in any way and can thus completely rule out type errors at runtime. Pointers as available in C are considered to be weakly typed because they can be used to completely bypass the type system e.g. by casting to and from a (void*) pointer. Other indications of weak typing are implicit type conversions and untagged unions which allow values of a given typed to be viewed as being a different type.
There is not a general accepted definition of strong and weak typing but it is agreed that programming languages vary across the strength of their typing: e.g. Haskell is seen as very strongly typed, C very weakly, Java more strongly typed than C whereas Assembly is considered to be untyped.

\section{Haskell}
The language of choice for discussing real implementations of ABS in the pure functional programming paradigm we select Haskell. The reason is that it is a mature language with lots of useful and stable libraries and because it has been proved to be useful in Real-World applications as well (TODO: take from 1st year report). Also the reason why selecting Haskell over e.g. Scala, Clojure is its purity, strong static type-system, non-strictness.

Being \textit{pure} describes that all Haskells features are directly translatable to the Lambda-Calculus. Other functional languages (e.g. Scala), although also based on the Lambda-Calculus like all functional languages, have features which are not directly translatable into the Lambda-Calculus and are thus considered to be impure. Another meaning of \textit{pure} in the context of Haskell and of functional programming can be understood as \textit{free from side-effects} or more precisely as having the property of \textit{referential transparency}. 

haskells real power: side-effect polymorph. enabled through monadic progranming which becomes possible through type parameters, typeclasses, higher-order functions, lambdas and pattern matching

\section{Conclusions}
- lambda calculus very close to mathematical notation: denotational / declarative way to think about computation: describe WHAT one wants to compute.
- referential transparency \& evaluation order independence: absence of side-effects and stateful machine (as in TM) removes difficult bugs as well.
- static strong type system: can guarantee absence of huge number of bugs already at compile-time. Still non-terminating functions possible (this would need more sophisticated type-system e.g. Agda, Coq,...).