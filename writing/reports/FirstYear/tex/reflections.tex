\chapter{Reflecting the Literature}
\label{chap:refl}

\section{Conclusion}
the single strength of todays oop is its use as a modelling language. the problem is what is going on under the hood: the sharing of mutable state. this is the real problem of mixing up the concept of object and agent

TODO: put at the end
Thread of argumentation: actor model is nice but for the kind of simulation we want to do we need too much synchronization otherwise we end up non-deterministic \& non-replicable. also actor-model very difficult to reason about because of non-deterministic specifications of message-transmission. what we need is a combination: deterministic, synchronized traversal of agents which are represented as a pure function: immutable messages and no exchange of aliases through which state can be mutated.

process calculi are nice for algebraic reasoning but are too cumbersome and not feasible for real, complex ABS. You do not program a large system in the lambda calculus, as you would not program a real distributed system in a process calculus \footnote{it was shown by TODO: cite that the pi-calculus can encode the lambda calculus, thus it is conceptually on a very low level: too much raw power leads to chaos.}. they may be of use for verification \& validation later on of small, critical parts of the ABS-communication which can be mapped to e.g. the pi-calculus and then apply algebraic reasoning.
As emphasised in the literature-review, not research was found on using process-calculi in the field of ABS. Although we can reason that if the $\pi$-calculus can be used to specifying and reasoning about MAS then it should be possible to do so for ABS or parts of it. There exists also a connection from the actor-model to process calculi \cite{agha_foundation_1997}, which strengthen our argument. 

the concept of homoiconicity of LISP seems to be very interesting and powerful to apply to ABS but an agent-based model where this extremely powerful technique can be applied is lacking. Also it is clear that when using this technique verification and validation becomes immensely more difficult, if not even impossible. Thus we refrain from LISP and its homoiconicity and look for something more structured, static and not as dynamic.

This leads us to pure functional programming with FRP which combines all the benefits: local memory, messages, switching of behaviour, algebraic reasoning, static typesystem

By the literature-review it seems that all the problems of object-oriented programming (as it is done in Java and C++) mentioned in the introduction, can be solved by (pure) functional programming which abandons the concept of global state, Objects and Classes and makes data-flow explicit. This then allows to reason about correctness, termination and other properties of the program e.g. if a given function exhibits side-effects or not. Other benefits are fewer lines of code, easier maintainability and ultimately fewer bugs thus making functional programming the ideal choice for scientific computing and simulation and thus also for ACE.

\section{Actor Model}
upside: extreme huge number of agnets possible due to distributed and parallel technology 
downside: depends on system \& hardware: scheduler, system time, systime resolution (not very nice for scientific computation), much more complicated, debugging difficult due to concurrency, no global notion of time appart from systemtime, thus always runs in real-time, but there is no global notion of time in the actor model anyway, no EDSL full of technical details, no determinism, no reasoning

Agents more a high-level concept, Actors low level, technical concurrency primitives

This makes simulations very difficult and also due to concurrency implementing a sync conversation among agents is very cumbersome. I have already experience with the Actor Model when implementing a small version of my Master-Thesis Simulation in Erlang which uses the Actor Model as well. For a continuous simulation it was actually not that bad but the problem there was that between a round-trip between 2 agents other messages could have already interfered - this was a problem when agents trade with each other, so one has to implement synchronized trading where only messages from the current agent one trades with are allowed otherwise budget constraints could be violated. Thus I think Erlang/Akka/Actor Model is better suited for distributed high-tolerance concurrent/parallel systems instead for simulations. Note: this is definitely a major point I have to argue in my thesis: why I am rejecting the actor model.


AKKA: thus my prediction is: akka/actor model is very well suited to simulations which 1. dont rely on global time 2. dont have multi-step conversations: interactions among agents which are only question-answer. TODO: find some classical simulation model which satisfies these criterias.

how can we simulate global time? how can we implement multistep conversations (by futures)?

The real problem seems to be concurrency but i feel we can simulate concurrency by synchronizing to continuous time. computations are carried out after another but because time is explicitly modelled they happen logically at the same time. these rules hold: an agent cannot be in two conversations at the same time, the agent can be in only one or none conversation at a given time t.

What if time is of no importance and only the continuous dynamics are of interest?

To put it another way: real concurrency (with threads) makes time implicit by connecting it to the real-time of the real-world, which is what one does NOT want in simulation. Maybe FRP is the way to go because it allows to explicitly model continuous and discrete time, but I have to get into FRP first to make a proper judgement about its suitability.

\cite{Bezirgiannis2013} describes in chapter 3.3 a naive clone of NetLogo in the Erlang programming language where each agent was represented as an Erlang process. The author claims the 1:1 mapping between agent and process to "be inherently wrong" because when recursively sending messages (e.g. A to B to A) it will deadlock as A is already awaiting Bs answer. Of course this is one of the problems when adopting Erlang/Scala with Akka/the Actor Model for implementing agents \textit{but it is inherently short-sighted to discharge the actor-model approach just because recursive messaging leads to a deadlock}. It is not a problem of the actor-model but merely a very problem with the communication protocol which needs to be more sophisticated than \cite{Bezirgiannis2013} described. The hypothesis is that the communication protocol will be in fact \textit{very highly application-specific} thus leading to non-reusable agents (across domains, they should but be re-usable within domains e.g. market-simulations) as they only understand the domain-specific protocol. This is definitely NOT a drawback but can't be solved otherwise as in the end (the content of the) communication can be understand to be the very domain of the simulation and is thus not generalizable. Of course specific patterns will show up like "multi-step handshakes" but they are again then specifically applied to the concrete domain.

\section{Pure Functional Programming}
why pure functional programming? what are its strengths? why does it overcome the problems? what are possible problems when doing it pure functional instead of oop?

\subsection{Strengths}
\begin{itemize}
	\item Pure - explicit about effects through monads
	\item Composability through higher-order functions and lazyness
	\item EDSL by its declarative style
	\item Reasoning through EDSL and lack of implicit side-effects 
	\item Static Type System
\end{itemize}

\subsection{Weaknesses}
Here we give an overview of the weaknesses and problems of pure, lazy functional programming in Haskell.

\paragraph{Space-Leaks}
The main issue in a lazy functional programming language is the difficulty of predicting space behaviour, which is very hard even for experienced programmers \cite{hudak_history_2007}. The problem arises from the fact, that Haskell abstracts away from evaluation order and object lifetimes. Programmers have no way to determine which data-structures live for how long - indeed they don't want and should not be bothered to think about these details as this would violate the whole concept behind pure lazyness \cite{hudak_history_2007}.

\paragraph{Debugging}
Due to the lazy evaluation and non-imperative programming style it becomes apparent that debugging needs to be approached completely different than in imperative programming where one can freely set breakpoints to statements and inspect data. This is not possible in Haskell as there are no imperative statements and the data may have not been evaluated yet due to the unpredictable evaluation order as mentioned already in the space-leak problem.

\paragraph{Performance}
Due to the lack of side-effects and aliasing, efficient in-order updates of memory is not as easily possible as in imperative languages like C thus real-time applications like Games which have a big global mutable state run much slower compared to its imperative implementations.
TODO: check if these references really support my argument \cite{mun_hon_functional_2005}, \cite{meisinger_game-engine-architektur_2010} - The works on game-programming in Yampa mention a similar problem (FRP-section).

\paragraph{Monad composition}
Monads do not compose in a nice, modular way and this issue is still open research \cite{hudak_history_2007}. TODO: is this still the case?




