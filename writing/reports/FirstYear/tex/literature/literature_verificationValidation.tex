\section{Verification \& Validation of ABS}
Verification \& Validation are, generally speaking, independent processes to check whether a product meets its requirements and fulfills its intended purpose. Here we focus explicitly on software verification \& validation where we identify \textit{Verification} to be the process of checking whether an implementation matches a given specification without any bugs or missing parts and \textit{Validation} to be the process of checking if the implementation meets high level requirements.
To put short according to Boehm \cite{boehm_software_1989} the difference between verification \& validation is that verification tries to answer "are we building the product right?" and validation "are we building the right product?". For (most of) the software built in the industry in well-defined software-development processes with its own quality control and quality assurance to answer these questions is rather straight-forward. Numerous techniques like checklists, software-tests, integration-tests,... have been developed to deal with Verification \& Validation. 
The question is how the above applies to ABS and from reviewing the literature \footnote{Leigh Tesfatsion has, as part of her internet-presence on ACE, set up a whole site devoted just to this topic where she cites major references \url{http://www2.econ.iastate.edu/tesfatsi/empvalid.htm}. Most of the literature we have investigated  is drawn from there.}, it becomes evident that in the context of ABS it is a rather difficult process as there exist no straight-forward way \cite{sargent_verification_2005} and is still open research \cite{balci_verification_1998} \footnote{This work gives an in-depth discussion and treatment of verification, validation and testing of complex and large-scale simulations from an engineering point of view.}.

\cite{ormerod_epistemological_2009} clarifies on this subject a bit and defines verification to be "the process of determining that the equations are solved correctly [...] does the model do what we think it is supposed to do?" and validation to be "the process of determining that we are using the correct equations" - basically a reformulation of the questions posed by Boehm \cite{boehm_software_1989} above. They add on these definitions by adding the concept of a model: verification is then "the process of determining that a computational software implementation correctly represents a model of a process" and validation is "the process of assessing the degree to which a computer model is an accurate representation of the real world from the perspective of the models intended applications".
A fundamentally observation \cite{ormerod_epistemological_2009} make is that ABS may blur the boundary between verification and validation. This is due to the problem of plausibly deciding how the micro-behaviour influences the macro-behaviour when changing e.g. a variable which drives the micro-behaviour. For this to decide we have to turn to validation, where we check if the model is a plausible image of the real-world process.
\cite{galan_errors_2009} makes the critical point that the dynamics of a model in ABS are almost always so complex, that the creator of the is not able to exactly explain what the deeper reasons are for them and which aspects of the model are responsible for their exhibition. If this were so, one would not need to resort to simulation. This implies that one can not know in advance what exactly to expect and which part of the emergent behaviour of the system connects to which local interaction amongst agents. Also it is very hard to check if the emergent behaviour is not due to some bug in the implementation.

Also an important question raised in \cite{ormerod_epistemological_2009} is the one how the time-stepping of ABS relates to time in reality \footnote{My foundational paper on Iteration-Strategies clarifies on this a bit: although we perceive time in reality as a continuous flow, the way we \textit{implement} time in a simulation is always in steps, be it discrete- or continuous-time.}. Concluding, the authors follow the Popperian position 'that validation can never be proof'. They stress that an important part of validation is a clear description of what is being explained (My question: can this be only natural language e.g. could we formalize this? if yes, we can formalize validation!).

The issue of validation \& verification is also very closely related to the problem of replication. In the paper \cite{wilensky_making_2007} Wilensky discusses the issues with replicating a model and gives advices to modellers what details (e.g. order of events) to publish to support replication of models. They recommend to make pseudo-code publicly available and to converge to a common standard form for model publication in the long run. This is our approach: an EDSL can act both as a pseudo-code specification which is in fact already runnable code and due to its declarative and concise nature can act as a full description.
  
In this thesis we aim for replicating both the Sugarscape \cite{epstein_growing_1996} and Agent\_Zero \cite{epstein_agent_zero:_2014} models. In doing so we also perform verification \& validation on these models, or at least on parts of them. Also we aim to develop new methods of verification and validation of ABS based on theses models as use-cases. TODO: my own first paper goes towards replication and verification

The paper of \cite{windrum_empirical_2007} explores problems with empirical validation of agent-based models. They identify six issues and presents a novel taxonomy to classify the approaches developed by the ABS community so far in tackling these issues. The authors then discussed three approaches to ABS validation and showed how to apply them.

\subsection{Verification}
Axelrods seminal paper \cite{axelrod_advancing_1997} establishes 3 ways of comparing models: same numerical outputs, same distributions of numerical outputs, same correlation of outputs: if x is increased in both models and y increases in the original then it should also be increased in the replicated model. \cite{ormerod_epistemological_2009} claim that the last property is rather too weak and extend it to be 'if input variable x is increased in both models by a given amount, the distribution observed in the changes in output variable y should be statistically indistinguishable'.

In the work of \cite{axtell_aligning_1996} the authors tried to see whether the more complex Sugarscape model can be used to reproduce the results of \cite{axelrod_convergence_1995}. In both models agents have a tag for cultural identification which is comprised of a string of symbols. The question was whether Sugarscape, focusing on generating a complete artifical society which incorporates many more mechanisms like trading, war, ressources can reproduce the results of \cite{axelrod_convergence_1995} which only focuses on transmission of these cultural tags. Although interesting the question if two models are qualitatively equivalent is not what we want to pursue in our thesis as it requires a complete different direction of research. The cooperative work of \cite{axtell_aligning_1996} gives insights into verification of computational models, in a process what they call "alignment". They try to determine if two models deal with the same phenomena. For this they tried to qualtiatively reproduce the same results of \cite{axelrod_convergence_1995} in the Sugarscape model of \cite{epstein_growing_1996}. Both models are of very different nature but try to investigate the qualitatively same phenomenon: that of cultural processes.

\subsection{Validation}
Wilensky \cite{wilensky_making_2007} defines model validation to be the process of determining whether the simulation explains and corresponds to phenomenon of the real world. \cite{galan_errors_2009} define validation of a model to check if it is consistent with the intended application of the model, how well the model captures its empirical referent.

TODO: the next section is too cumbersome and contains unsatisfying definitions
Carley \cite{carley_validating_1996} is a comprehensive reference focused explicitly on validation and provides information and techniques for it. The author distinguishes between six types of validation:
\begin{enumerate}
	\item Conceptual - is the underlying theoretical model adequate to characterize the real world?
	\item Internal - is the computer code correct, free of coding errors \footnote{This may be confused with the established definition of verification. The question is to which kind of correctness this definition refers: a program can be correct in the way that it has no bugs e.g. causing no crashes, or it is correct in the way that it implemented the model as it was intended.}?
	\item External - is the model adequate and accurate in matching the real world data?
	\item Cross-Model - is the output of the model qualitatively the same as another model \footnote{An imporant example for this is the work of \cite{axtell_aligning_1996}, see above}?
	\item Data - is the data adequate and accurate for addressing the problem?
	\item Security - has the model been tampered with \footnote{We don't understand what is really meant here.}?
\end{enumerate}
Carley primarily focuses on external validation and describes four techniques 
\begin{enumerate}
	\item Grounding - establishing the reasonableness of a computational model.
	\item Calibrating - fitting a a model to details of real data.
	\item Verification - comparing the models predictions to a set of real data \footnote{It is a bit confusing that the author sees verification as part of validation as they are defined as distinct processes in software-engineering. But as pointed out above, in ABS the clear boundaries between validation \& verification become blurred.}.
	\item Harmonization - showing that theoretical assumptions of the model are well grounded with the real world.
\end{enumerate}

From the cited literature above, it becomes clear that validation is concerned with establishing the relationship between the simulated model and its real-world counterpart - the system / thing / object being simulated. The question is whether there is a view which connects both of them? So far the established tool has always been the one of numerical and statistical tests which have their strengths and weaknesses. The question is if we can compare the two structurally and qualitatively. 

\subsection{Emergence}
An essential aspect of ABS is the one of emergent properties of a simulation: the global dynamics the system exhibits under simulation emerges from the local interaction of its parts where these local interactions were not designed to force the system towards this emergent property - the whole system exhibits properties the parts don't have, a collection of interacting systems shows collective behaviour \cite{baas_emergence_1997} or simply \textit{the whole is more than the sum of its parts}.

A non-formal view on emergence from the social sciences is given by Gilbert \cite{gilbert_holism_1996}. He defines emergent behaviour to be that which cannot be predicted from knowledge of the properties of the agents except as a result of simulation and conjectures that this definition gives an explanation for the popularity of simulation for researching complex adaptive systems. Gilbert then raises the question if emergence can be formalized.

When doing verification - checking the relationship between the simulated model and its real-world counterpart - we need to check if the emergent properties of both systems match or are qualitatively the same. For this we need \textit{some} formalization of emergence.
Baas \cite{baas_emergence_1994}, \cite{baas_emergence_1997} attempts a highly intriguing formalization of emergence. In \cite{baas_emergence_1997} he approaches emergence from a category theory perspective \footnote{What a wonderful coincidence!} where systems are represented as objects and their interactions as morphisms. Baas introduces the concept of an observer \footnote{Note the importance of an observer for emergence: an essential question is when to believe a model and how to interpret its results and its emergent behaviour - something highly dependent on an observer} and observational mechanisms which can be thought of a functor, mapping from one category to another - emergence is a holistic structure which can not be decomposed into its parts. He then distinguishes between two kinds of emergence: Deducible or computational emergence for which an algorithmic explanation exists and observational emergence for which none such explanation exists. It seems that the work of Baas on emergence could be used to drive our ambitions in using category-theory as underlying foundation for ABS and model-specification and connect it to the foundations of and transfer it to pure functional programming. This could close the second gap which exists between the emergent-property of the model specification and the real-world system through means of functional programming.

\subsection{Conclusion}
It is no coincidence that the formalization of emergency is given by Baas, a mathematician in the field of Algebraic Topology - a field out of which category theory was born and in \cite{baas_emergence_1997} he gives a category-theoretical view on it.
Hypothesis: this may be a hint to attempt a category theory approach to ABS. category-theory may be an approach which allows to abstract from the simulated model and the real-world system and show that the simulated model is indeed an instance of the real-world system. Also it may work in the way that we extract a category-theoretical view of our real-world system and map this then to pure functional concepts in Haskell.

From the cited literature, we can derive that verification of ABS means, to check if our model-specification matches our implementation - this is is the definition we stick to throughout our thesis. The model-specification could be delivered in natural language, mathematical equations, specific formalisms e.g. DEVS, Process Calculi, pseudo-code,... This implies that there exists always a gap between the specification and the implementation. Our hypothesis is that this gap can be closed by implementing a sufficiently powerful EDSL in Haskell which is both specification- and implementation-language at the same time - the specification becomes the implementation. Of course when a model is conceived it will be almost always be written down in a narrative natural-language style as this is the way we think. The next step would then be to translate this natural-language description into the EDSL. So we see that there is obviously always some translation step included so the question is what we really gain. Also we can ask: aren't there already enough formalisms for ABS? TODO: i need to argue very very thoroughly because this is another major attacking point to my thesis (the other one is why bother doing functional programming).
Answer: its the power of the whole package of EDSL = Haskell-code. this let us share the specification and apply all the reasoning-techniques available to Haskell. Question: is this impossible in other specifications like FRABJOUS or NetLogo?

