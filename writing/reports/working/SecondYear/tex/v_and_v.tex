\chapter{Verification \& Validation}
\label{chap:v_and_v}

In this chapter we will have a closer look on the topic of Verification \& Validation. It is of most importance to understand the ideas and concepts behind it, when we are referring to the \textit{correctness of a simulation}, as in our initial hypothesis. 

We first will give a short and concise introduction \ref{sec:vav_introduction} on the topic \footnote{In this short introduction we closely follow the book \cite{robinson_simulation:_2014}} and then briefly discuss verification \& validation in the context of agent-based simulation \ref{sec:vav_abs}.

%TODO: we need to distinguish between various meanings of 'correct'
%1. correctness of software: the implementation is correct up to a model specification
%2. correctness of a simulation: the implementation is correct up to a model specification AND it generates the same dynamics

\section{Introduction}
\label{sec:vav_introduction}
%According to \cite{robinson_simulation:_2014}, when implementing a model one should have the following aims in mind:
%\begin{itemize}
%	\item Speed Of Coding - how quickly can we implement a model.
%	\item Transparency - how easily can the code be understood.
%	\item Flexibility - how easily can the code be changed to model changes.
%	\item Run-Speed - how quickly will the code execute.
%\end{itemize}
%
%Further the author distinguishes three activities 
%\begin{itemize}
%	\item Coding - implementing the model.
%	\item Testing - verifying and white-box validating the model.
%	\item Documenting - recording details of the model.
%\end{itemize}
%
%Also it is important to distinguish between different natures of a simulation: terminating or non-terminating, and its output transient or steady-state (cycle or shifting).

%steward robinson simulation book on implementation
%- meaning of implementation
%	-> 1 implementing the findings: conduct a study which defines and gathers all findings about the model and document them
%	-> 2 implementing the model
%	-> 3 implementing the learning

\textbf{\textit{Validation}} is the process of ensuring that the model is sufficiently accurate for the purpose at hand. \textbf{\textit{Verification}} is the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy. \cite{balci_verification_1998} define verification as "are we building the model right?" and validation as "are we building the right model?".

In this research we will primarily focus on verification, because its there where one ensures that the model is programmed correctly, the algorithms have been implemented properly, and the model does not contain errors, oversights, or bugs. Note that verification has a narrow definition and can be seen as a subset of the wider issue of validation. 

In verification and validation the aim is to ensure, that the model is sufficiently accurate, which always implies its purpose. Therefore the purpose and objectives must be known before it is validated. One distinguishes between:

\begin{itemize}
	\item White-box Validation: detailed, micro check if each part of the model represent the real world with sufficient accuracy. It is therefore intrinsic to model coding. The ways to do it is checking the code, visual checks, inspecting output reports. 
	\item Black-box Validation: overall, macro check whether the model provides a sufficiently accurate representation of the real world system. It can only be performed once model code is complete. Ways to do this is comparison with the real system or with other (simpler) models.

	\item White-box Verification: compares the content of the model to the \textit{conceptual} model. This is different to white-box validation which compares the content of the model to the \textit{real world}
	\item Black-box Verification: treating the functionality to test as a black box with inputs and outputs and comparing controlled inputs to expected outputs.

	%\item Data Validation: determining that the contextual data and the data required for model realisation and validation are sufficiently accurate for the purpose at hand.
\end{itemize}

%good paper \url{http://www2.econ.iastate.edu/tesfatsi/VVAccreditationSimModels.OBalci1998.pdf} : very nice 15 guidelines and life cycles, VERY valuable for background and introduction

So in general one can see verification as a test of the fidelity with which the conceptual model is converted into the computer model. Verification (and validation) is a continuous process and if it is already there in the programming language / supported by then this is much easier to do. This is the fundamental basis of our hypothesis where we claim that by choosing a programming language which supports this continuous verification and validation process, then the result is an implementation of a model which is more likely to be correct.

Unfortunately, there is no such thing as general validity: a model should be built for one purpose as simple as possible and not be too general, otherwise it becomes too bloated and too difficult / impossible to analyse. Also, there may be no real world to compare against: simulations are developed for proposed systems, new production / facilities which don't exist yet. Further, it is questionable which real world one is speaking of: the real world can be interpreted in different ways, therefore a model valid to one person might not be valid to another. Sometimes validation struggles because the real world data are inaccurate or there is not enough time to verify and validate everything.

In general this implies that we can only \textit{raise the confidence} in the correctness of the simulation, not validity: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. Therefore, the process of verification and validation is not the proof that a model is correct but trying to prove that the model is incorrect! The more tests/checks one carries out which show that it is not incorrect, the more confidence we can place on the models validity.

%	- methods of verification and validation
%	-> conceptual model validation: judment based on the documentation
%	-> data validation: analysing data for inconsistencies
%	-> verification and white-box validation
%		-> both conceptually different but often treated together because both occur continuously through model coding
%		-> what should be checked: timings (cycle times, arrival times,...), control of elements (breakdown frequency, shift patterns), control flows (e.g. routing), control logic (e.g. scheduling, stock replenishment), distribution sampling (samples obtained from an empirial distribution)
%	-> verification and whilte-box validation methods
%		-> checking code: reading through code and ensure right data and logic is there. explain to others/discuss together/others should look at your code. 
%		-> Visual checks
%		-> inspecting output reports
%		
%	-> black-box testing: consider overall behaviour of the model without looking into its parts, basically two ways
%		-> comparison with the real system: statistical tests
%		-> comparison with another model (e.g. mathematical equations): could compare exactly or also through statistical tests
%		-> 
%		

%peers slides (inspired by steward robinson book):
%    - Experimentation Validation: Determining that the experimental procedures adopted are providing results that are sufficiently accurate for the purpose at hand.
%    	-> How can we do this?
%    		- Graphical or statistical methods for determining warm-up period, run length and replications (to obtain accurate results)
%			- Sensitivity analysis (to improve the understanding of the model)
%	
%	- Solution Validation: Determining that the results obtained from the model of the proposed solution are sufficiently accurate for the purpose at hand
%		-> How does this differ from Black Box Validation? Solution validation compares the model of the proposed solution to the implemented solution while black-box validation compares the base model to the real world
%		-> How can we do this? Once implemented it should be possible to validate the implemented solution against the model results
%		
%	- Verification: Testing the fidelity with which the conceptual model is converted into the computer model. Verification is done to ensure that the model is programmed correctly, the algorithms have been implemented properly, and the model does not contain errors, oversights, or bugs.
%
%		-> How can we do this? Same methods as for white-box validation (checking the code, visual checks, inspecting output reports) but ... Verification compares the content of the model to the conceptual model while white-box validation compares the content of the model to the real world
%		
%	- Difficulties of verification and validation
%		-> There is no such thing as general validity: a model is only valid with respect to its purpose
%		-> There may be no real world to compare against
%		-> Which real world? Different people have different interpretations of the real world
%		->  Often real world data are inaccurate: If the data are not accurate it is difficult to determine if the model's results are correct. Even if the data is accurate, the real worl  data are only a sample, which in itself creates inaccuracy
%		-> There is not enough time to verify and validate every aspect of a model
%		
%	- Some final remarks:
%		-> V\&V is a continuous and iterative process that is performed throughout the life cycle of a simulation study.
%			Example: If the conceptual model is revised as the project progresses it needs to be re-validated
%		-> V\&V work together by removing barriers and objections to model use and hence establishing credibility.
%		
%- Conclusion: Although, in theory, a model is either valid or not, proving this in practice is a very different matter. It is better to think in terms of confidence that can be placed in a model!

\section{Verification \& Validation in Agent-Based Simulation}
\label{sec:vav_abs}
In our research we focus primarily on the \textit{Verification} aspect of agent-based simulation: ensuring that the implementation reflects the specifications of the \textit{conceptual} model - have we built the model right? Thus we are not interested in our research into making connections to the real world and always see the model specifications as our "last resort", our ground truth beyond nothing else exists. When there are hypotheses formulated, we always treat and interpret them in respect of the conceptual model.
In \cite{ormerod_validation_2006} the authors clarify on verification in ABS. Verification "... is essentially the question: does the model do what we think it is supposed to do? Whenever a model has an analytical solution, a condition which embraces almost all conventional economic theory, verification is a matter of checking the mathematics.". They say about validation that "In an important sense, the current process of building ABMs is a discovery process, of discovering the types of behavioural rules for agents which appear to be consistent with phenomena we observe.". Further they claim that "Because such models are based on simulation, the lack of an analytical solution (in general) means that verification is harder, since there is no single result the model must match. Moreover, testing the range of model outcomes provides a test only in respect to a prior judgment on the plausibility of the potential range of outcomes. In this sense, verification blends into validation."
So the baseline is that either one has an analytical model as the basis of an agent-based model or one does not. In the former case, e.g. the SIR model, one can very easily validate the dynamics generated by the simulation to the one generated by the analytical solution (e.g. through System Dynamics). In the latter case one has basically no idea or description of the emergent behaviour of the system prior to its execution e.g. SugarScape. It is important to have some hypothesis about the emergent property / dynamics. The question is how verification / validation works in this setting as there is no formal description of the expected behaviour: we don't have a ground-truth against which we can compare our simulation dynamics. %(eventuell hilft hier hans vollbrecht weiter: Simulation hat hier den Sinn, die Controller anhand der Roboteraufgabe zu validieren, Bei solchen Simulationen ist man interessiert an allen möglichen Sequenzen, und da das meist zu viele sind, an einer möglichst gut verteilten Stichprobenmenge. Hier geht es weniger um richtige Zeitmodellierung, sondern um den Test aller möglichen Ereignissequenzen.)

General there are the following simulation requirements to ABS \cite{robinson_simulation:_2014}, which all can be addressed in our \textit{pure} functional approach as described in the paper in Appendix \ref{app:pfe}:

\begin{itemize}
	\item Modelling progress of time - achieved using functional reactive programming (FRP)
	\item Modelling variability - achieved using FRP
	\item Fixing random number streams to allow simulations to be repeated under same conditions - ensured by \textit{pure} functional programming and Random Monads
	\item Rely only on past - guaranteed with \textit{Arrowized} FRP
	\item Bugs due to implicitly mutable state - reduced using pure functional programming
	\item Ruling out external sources of non-determinism / randomness - ensured by \textit{pure} functional programming
	\item Deterministic time-delta - ensured by \textit{pure} functional programming
	\item Repeated runs lead to same dynamics - ensured by \textit{pure} functional programming
\end{itemize}

\cite{kleijnen_verification_1995} suggests good programming practice which is extremely important for high quality code and reduces bugs but real world practice and experience shows that this alone is not enough, even the best programmers make mistakes which often can be prevented through a strong static or a dependent type system already at compile time. What we can guarantee already at compile time, doesn't need to be checked at run-time which saves substantial amount of time as at run-time there may be a large number of execution paths through the simulation which is almost always simply not feasible to check (note that we also need to check all combinations). This paper also cites modularity as very important for verification: divide and conquer and test all modules separately. We claim that this is especially easy in functional programming as code composes better than in traditional object-oriented programming due to the lack of interdependence between data and code as in objects and the lack of global mutable state (e.g. class variables or global variables) - this makes code extremely convenient to test. The paper also discusses statistical tests (the t test) to check if the outcome of a simulation is sufficiently close to real-world dynamics - we explicitly omit this as it part of validation and not the focus of this research. % Also the paper suggests using animations to visualise the processes within the simulation for verification purposes (of course they note that animation may be misleading when one focuses on too short simulation runs).

%So we ask whether we can encode phenomena we observe in the types? can we use types for the discovery process as well? can dependent types guide our exploratory approach to ABS?

\cite{polhill_ghost_2005}: "For some time now, Agent Based Modelling has been used to simulate and explore complex systems, which have proved intractable to other modelling approaches such as mathematical modelling. More generally, computer modelling offers a greater flexibility and scope to represent phenomena that do not naturally translate into an analytical framework. Agent Based Models however, by their very nature, require more rigorous programming standards than other computer simulations. This is because researchers are cued to expect the unexpected in the output of their simulations: they are looking for the 'surprise' that shows an interesting emergent effect in the complex system. It is important, then, to be absolutely clear that the model running in the computer is behaving exactly as specified in the design. It is very easy, in the several thousand lines of code that are involved in programming an Agent Based Model, for bugs to creep in. Unlike mathematical models, where the derivations are open to scrutiny in the publication of the work, the code used for an Agent Based Model is not checked as part of the peer-review process, and there may even be Intellectual Property Rights issues with providing the source code in an accompanying web page."

\cite{galan_errors_2009}: "a prerequisite to understanding a simulation is to make sure that there is no significant disparity between what we think the computer code is doing and what is actually doing. One could be tempted to think that, given that the code has been programmed by someone, surely there is always at least one person - the programmer - who knows precisely what the code does. Unfortunately, the truth tends to be quite different, as the leading figures in the field report, including the following: You should assume that, no matter how carefully you have designed and built your simulation, it will contain bugs (code that does something different to what you wanted and expected), "Achieving internal validity is harder than it might seem. The problem is knowing whether an unexpected result is a reflection of a mistake in the programming, or a surprising consequence of the model itself. […] As is often the case, confirming that the model was correctly programmed was substantially more work than programming the model in the first place. This problem is particularly acute in the case of agent-based simulation. The complex and exploratory nature of most agent-based models implies that, before running a model, there is some uncertainty about what the model will produce. Not knowing a priori what to expect makes it difficult to discern whether an unexpected outcome has been generated as a legitimate result of the assumptions embedded in the model or, on the contrary, it is due to an error or an artefact created in the model design, its implementation, or its execution."

\subsection{Testing}
Although (pure) functional programming allows us to have stronger guarantees about the behaviour and absence of bugs of the simulation already at compile-time, we still need to test all the properties of our simulation which we cannot guarantee at compile-time.

We found property-based testing particularly well suited for ABS. Although it is now available in a wide range of programming languages and paradigms, propert-based testing has its origins in Haskell \cite{claessen_quickcheck:_2000, claessen_testing_2002} and we argue that for that reason it really shines in pure functional programming.
Property-based testing allows to formulate \textit{functional specifications} in code which then the property-testing library (e.g. QuickCheck \cite{claessen_quickcheck:_2000}) tries to falsify by automatically generating random test-data covering as much cases as possible. When an input is found for which the property fails, the library then reduces it to the most simple one. It is clear to see that this kind of testing is especially suited to ABS, because we can formulate specifications, meaning we describe \textit{what} to test instead of \textit{how} to test (again the declarative nature of functional programming shines through).

Generally we need to distinguish between two types of testing/verification: 1. testing/verification of models for which we have real-world data or an analytical solution which can act as a ground-truth. examples for such models are the SIR model, stock-market simulations, social simulations of all kind and 2. testing/verification of models which are just exploratory and which are only be inspired by real-world phenomena. examples for such models are Epsteins Sugarscape and Agent\_Zero.

In both cases this leaves us with Black-Box and White-Box Verification:

%\subsection{Comparison of dynamics against existing data}
%- utilise a statistical test with H0 "ABS and comparison is not the same" and H1 "ABS and comparison is the same"
%- how many replications and how do we average?
%- which statistical test do we implement? (steward robinson simulation book, chapter 12.4.4)
%	-> Normalizsed Mean Squared Error (NMSE)
%	-> TODO: implement confidence interval 
%	-> TODO: what about chi-squared?
%	-> TODO: what about paired-t confidence interval
%
%IMPORTANT: this is not what we are after here in this paper, statistical tests are a science on their own and there actually exists quite a large amount of literature for conducting statistical tests on ABS dynamics: Robinson Book (TODO: find additional literature)	

\subsubsection{Black Box Verification}
In Black Box Verification one generally feeds input and compares it to expected output. In the case of ABS we have the following examples of black-box test:
\begin{enumerate}
	\item Isolated Agent Behaviour - test isolated agent behaviour under given inputs using unit- and property-based testing
	\item Interacting Agent Behaviour - test if interaction between agents are correct 
	\item Simulation Dynamics - compare emergent dynamics of the ABS as a whole under given inputs to an analytical solution / real-world dynamics in case there exists some using statistical tests
	\item Hypotheses- test whether hypotheses are valid / invalid using unit- and property-based testing. % TODO: how can we formulate hypotheses in unit- and/or property-based tests?
\end{enumerate}

%- testing of the final dynamics: how close do they match the analytical solution
%- can we express model properties in tests e.g. quickcheck?
%- property-testing shines here
%- isolated tests: how easy can we test parts of an agent / simulation?

Using black-box verification and property-based testing we can apply for the following use cases for testing ABS in FRP:

\paragraph{Finding optimal $\Delta t$}
The selection of the right $\Delta t$ can be quite difficult in FRP because we have to make assumptions about the system a priori. One could just play it safe with a very conservatively selected small $\Delta t < 0.1$ but the smaller $\Delta t$, the lower the performance as it quickly multiplies the number of steps to calculate. Obviously one wants to select the \textit{optimal} $\Delta t$, which in the case of ABS is the largest possible $\Delta t$ for which we still get the correct simulation dynamics.
To find out the \textit{optimal} $\Delta t$ one can make direct use of the Black Box tests: start with a large $\Delta t = 1.0$ and reduce it by half every time the tests fail until no more tests fail - if for $\Delta t = 1.0$ tests already pass, increasing it may be an option. It is important to note that although isolated agent behaviour tests might result in larger $\Delta t$, in the end when they are run in the aggregate system, one needs to sample the whole system with the smallest $\Delta t$ found amongst all tests. Another option would be to apply super-sampling to just the parts which need a very small $\Delta t$ but this is out of scope of this paper.

\paragraph{Agents as signals}
Agents \textit{might} behave as signals in FRP which means that their behaviour is completely determined by the passing of time: they only change when time changes thus if they are a signal they should stay constant if time stays constant. This means that they should not change in case one is sampling the system with $\Delta t = 0$. Of course to prove whether this will \textit{always} be the case is strictly speaking impossible with a Black Box verification but we can gain a good level of confidence with them also because we are staying pure. It is only through white box verification that we can really guarantee and prove this property.

\subsubsection{White-Box Verification}
White-Box verification is necessary when we need to reason about properties like \textit{forever}, \textit{never}, which cannot be guaranteed from black-box tests. Additional help can be coverage tests with which we can show that all code paths have been covered in our tests.

\subsection{Example: Property-Based Testing of SIR}
As an example we discuss the black-box testing for the SIR model using property-testing. We test if the \textit{isolated} behaviour of an agent in all three states Susceptible, Infected and Recovered, corresponds to model specifications. The crucial thing though is that we are dealing with a stochastic system where the agents act \textit{on averages}, which means we need to average our tests as well. We conducted the tests on the implementation found in the paper of Appendix \ref{app:pfe}.

\subsubsection{Black-Box Verification}
The interface of the agent behaviours are defined below. When running the SF with a given $\Delta t$ one has to feed in the state of all the other agents as input and the agent outputs its state it is after this $\Delta t$.

\begin{HaskellCode}
data SIRState 
  = Susceptible 
  | Infected 
  | Recovered
  
type SIRAgent = SF [SIRState] SIRState

susceptibleAgent :: RandomGen g => g -> SIRAgent
infectedAgent :: RandomGen g => g -> SIRAgent
recoveredAgent :: SIRAgent
\end{HaskellCode}

\paragraph{Susceptible Behaviour}
A susceptible agent \textit{may} become infected, depending on the number of infected agents in relation to non-infected the susceptible agent has contact to. To make this property testable we run a susceptible agent for 1.0 time-unit (note that we are sampling the system with a smaller $\Delta t = 0.1$) and then check if it is infected - that is it returns infected as its current state.

Obviously we need to pay attention to the fact that we are dealing with a stochastic system thus we can only talk about averages and thus it does not suffice to only run a single agent but we are repeating this for e.g. $N = 10.000$ agents (all with different RNGs). We then need a formula for the required fraction of the N agents which should have become infected on average. Per 1.0 time-unit, a susceptible agent makes \textit{on average} contact with $\beta$ other agents where in the case of a contact with an infected agent the susceptible agent becomes infected with a given probability $\gamma$. In this description there is another probability hidden, which is the probability of making contact with an infected agent which is simply the ratio of number of infected agents to number not infected agents. The formula for the target fraction of agents which become infected is then: $\beta * \gamma * \frac{number of infected}{number of non-infected}$. To check whether this test has passed we compare the required amount of agents which on average should become infected to the one from our tests (simply count the agents which got infected and divide by N) and if the value lies within some small $\epsilon$ then we accept the test as passed.

Obviously the input to the susceptible agents which we can vary is the set of agents with which the susceptible agents make contact with. To save us from constructing all possible edge-cases and combinations and testing them with unit-tests we use property-testing with QuickCheck which creates them randomly for us and reduces them also to all relevant edge-cases. This is an example for how to use property-based testing in ABS where QuickCheck can be of immense help generating random test-data to cover all cases.

%TODO: derive the target-fraction formula from the differential equations
%TODO: can we encode this somehow on a type level using dependent types? then we don't need to test this property any more

\paragraph{Infected Behaviour}
An infected agent \textit{will always} recover after a finite time, which is \textit{on average} after $\delta$ time-units. Note that this property involves stochastics too, so to test this property we run a large number of infected agents e.g. $N = 10.000$ (all with different RNGs) until they recover, record the time of each agents recovery and then average over all recovery times. To check whether this test has passed we compare the average recovery times to $\delta$ and if they lie within some small $\epsilon$ then we accept the test as passed.

We use property-testing with QuickCheck in this case as well to generate the set of other agents as input for the infected agents. Strictly speaking this would not be necessary as an infected agent never makes contact with other agents and simply ignores them - we could as well just feed in an empty list. We opted for using QuickCheck for the following reasons:

\begin{itemize}
	\item We wanted to stick to the interface specification of the agent-implementation as close as possible which asks to pass the states of all agents as input.
	\item We shouldn't make any assumptions about the actual implementation and if it REALLY ignores the other agents, so we strictly stick to the interface which requires us to input the states of all the other agents.
	\item The set of other agents is ignored when determining whether the test has failed or not which indicates by construction that the behaviour of an infected agent does not depend on other agents.
	\item We are not just running a single replication over 10.000 agents but 100 of them which should give black-box verification more strength.
\end{itemize}

%TODO: derive the average formula from the differential equations
%TODO: can we encode this somehow on a type level using dependent types? then we don't need to test this property any more

\paragraph{Recovered Behaviour}
A recovered agent will stay in the recovered state \textit{forever}. Obviously we cannot write a black-box test that truly verifies that because it had to run in fact forever. In this case we need to resort to White-Box Verification (see below).

Because we use multiple replications in combination with QuickCheck obviously results in longer test-runs (about 5 minutes on my machine)
In our implementation we utilized the FRP paradigm. It seems that functional programming and FRP allow extremely easy testing of individual agent behaviour because FP and FRP compose extremely well which in turn means that there are no global dependencies as e.g. in OOP where we have to be very careful to clean up the system after each test - this is not an issue at all in our \textit{pure} approach to ABS.

\paragraph{Simulation Dynamics}
We won't go into the details of comparing the dynamics of an ABS to an analytical solution, that has been done already by \cite{macal_agent-based_2010}. What is important is to note that population-size matters: different population-size results in slightly different dynamics in SD => need same population size in ABS (probably...?). Note that it is utterly difficult to compare the dynamics of an ABS to the one of a SD approach as ABS dynamics are stochastic which explore a much wider spectrum of dynamics e.g. it could be the case, that the infected agent recovers without having infected any other agent, which would lead to an extreme mismatch to the SD approach but is absolutely a valid dynamic in the case of an ABS. The question is then rather if and how far those two are \textit{really} comparable as it seems that the ABS is a more powerful system which presents many more paths through the dynamics.
%TODO: i really want to solve this for the SIR approach
%	-> confidence intervals?
%	-> NMSE?
%	-> does it even make sense?

\paragraph{Finding optimal $\Delta t$}
Obviously the \textit{optimal} $\Delta t$ of the SIR model depends heavily on the model parameters: contact rate $\beta$ and illness duration $\delta$. We fixed them in our tests to be $\beta = 5$ and $\delta = 15$. By using the isolated behaviour tests we found an optimal $\Delta t = 0.125$ for the susceptible behaviour and $\Delta t = 0.25$ for the infected behaviour. %TODO: dynamics comparison?

\paragraph{Agents as signals}
Our SIR agents \textit{are} signals due to the underlying continuous nature of the analytical SIR model and to some extent we can guarantee this through black box testing. For this we write tests for each individual behaviour as previously but instead of checking whether agents got infected or have recovered we assume that they stay constant: they will output always the same state when sampling the system with $\Delta t = 0$. The tests are conceptual the complementary tests of the previous behaviour tests so in conjunction with them we can assume to some extent that agents are signals. To prove it, we need to look into white box verification as we cannot make guarantees about properties which should hold \textit{forever} in a computational setting.

\subsubsection{White-Box Verification}
%TODO: the implementation below has a SEVERE bug, all stochastic functions are correlated because they use the same RNG. this leads to different distributions of the dynamics, which can be shown using the test-code which generates the dynamics. The random monad version seems to perform much better where the mean is very close to the SD solution.

In the case of the SIR model we have the following invariants: 
\begin{itemize}
	\item A susceptible agent will \textit{never} make the transition to recovered.
	\item An infected agent will \textit{never} make the transition to susceptible.
	\item A recovered agent will \textit{forever} stay recovered.
\end{itemize}

All these invariants can be guaranteed when reasoning about the code. An additional help will be then coverage testing with which we can show that an infected agent never returns susceptible, and a susceptible agent never returned infected given all of their functionality was covered which has to imply that it can never occur!

%Lets start with looking at the recovered behaviour as it is the simplest one. We then continue with the infected behaviour and end with the susceptible behaviour as it is the most complex one.

We will only look at the recovered behaviour as it is the simplest one. We leave the susceptible and infected behaviours for further research / the final thesis because the conceptual idea becomes clear from looking at the recovered agent.

\paragraph{Recovered Behaviour}
The implementation of the recovered behaviour is as follows:

\begin{HaskellCode}
recoveredAgent :: SIRAgent
recoveredAgent = arr (const Recovered)
\end{HaskellCode}

Just by looking at the type we can guarantee the following:
\begin{itemize}
	\item it is pure, no side-effects of any kind can occur
	\item no stochasticity possible because no RNG is fed in / we don't run in the random monad
\end{itemize}

The implementation is as concise as it can get and we can reason that it is indeed a correct implementation of the recovered specification: we lift the constant function which returns the Recovered state into an arrow. Per definition and by looking at the implementation, the constant function ignores its input and returns always the same value. This is exactly the behaviour which we need for the recovered agent. Thus we can reason that the recovered agent will return Recovered \textit{forever} which means our implementation is indeed correct.