\chapter{Introduction}
The most important property of a general purpose programming language is being \textit{Turing Complete}. This term says roughly that a programming language which has this property can compute anything which is \textit{computable}. We will not go into the precise meaning of computability but it suffices to say that a problem is computable if we can formulate a program which, after a finite number of steps, produces an output for a given input to the problem. This also implies that there are non-computable problems e.g. comparing whether two infinite sets are extensionally equal is uncomputable. Turing completeness also says something about the theoretical limits of a programming language, termed the \textit{halting-problem}: it is not possible for a program written in such a programming language to decide \textit{in general} whether another program of a turing complete language terminates or not. From this follows, that all general purpose programming languages which are turing complete are equal in power: none can compute something another can't and all suffer the same limitations of the halting-problem.
These fundamental insights come from the foundations and theory of computation and research on computability, which originated in the 1930s by the pioneering work of Turing, Church and Gödel. All of them have developed different models of computation but all were shown to be equal in power: they are all turing complete which means each of them can simulate the others \footnote{This implies that a language which guarantees that its programs always halt, is not powerful enough to interpret itself.}.
Today's general purpose programming languages can be generally categorized into two categories: they either build on the model of the Turing Machine as introduced by Alan Turing or on the Lambda Calculus as introduced by Alonzo Church.
Thus assuming that all general purpose programming languages are turing complete, one could ask the following questions:

\begin{enumerate}
	\item Why don't we implement our problems directly in a Turing Machine or the Lambda Calculus?
		
	\item Why are all these programming languages necessary in the first place if all are equal in power?

	\item What are the implications for a programming language when choosing the one or the other model of computation as their foundation?
\end{enumerate}

The reason why we do not program directly in these models of computation is because the raw power quickly becomes unmanageable and too complex which is also due to the lack of a type system. The interesting thing though is that compared to the Turing Machine, the Lambda Calculus is much more manageable in terms of complexity. When one programs in a TM, the solution gets extremely complicated very quickly whereas one can program quite a while in LC because of its fundamental different nature \footnote{Actually when programming in Haskell (or ML / similar class of families) one programs basically in a slightly 'sugarized' version of the Lambda Calculus.}.

Implementing simple arithmetic operations on natural numbers can be already quite challenging with surprisingly substantial amount of complexity \footnote{Note that these seemingly 'trivial' problems already require substantial amount of work, namely the encoding of natural numbers in either the Turing Machine or the Lambda Calculus, which is not trivial. This for a good reason: operations on natural numbers were used by Gödel to study the basics of computation.} In the subsequent chapters on functional programming and object-oriented programming we will give implementations of such simple arithmetic operations in the Lambda Calculus and the Turing Machine respectively - this will also show by example that the raw power of the Lambda Calculus is more manageable than the one of the Turing Machine.
Still we can conclude that the TM and LC are basically only useful as theoretical foundations and to study fundamental and foundational problems but not to solve real problems. This is because we think problems different than a TM or LC works. So both do not allow us directly to express in the way we think, we need to build  more mechanisms on top of this raw complexity. To control complexity, the best solution has always been to add layers of abstraction. Thus we build up more and more levels of abstractions where each depends on preceding ones. Some programming languages stop at some point, where other languages go further in abstractions - the point here is that not all programming languages are equal in their abstractions of complexity.

In the end a programming language is a tool to solve a problem by expressing the solution in this language. Depending on the underlying computational model some problems may be easier to solve in different languages \textit{because the language supports expressing a given solution more natural for the given problem}.
When looking closer at the nature of the Turing Machine and Lambda Calculus we can attribute the Turing Machine to be an operational model and the Lambda Calculus to be a denotational one. Roughly speaking, an operational model describes \textit{how} to compute something whereas a denotational model describes \textit{what} to compute.
This fundamental difference of operational vs. denotational is directly reflected in programming languages which build on these two different models of computation: imperative languages follow the operational model of the TM and functional languages basically the denotational model of the LC.
Although today's computers are basically highly efficient Turing Machines and follow thus the operational model \footnote{TODO: why this is so is probably due to history reasons and because probably also a philosophical problem: when facing the real world we need simply operational models as this is the way to manifest something in the real-world, declarative model resorts more to magic which is not as reliable.} this means that also declarative, functional languages are ultimately translated into an operational model, which is possible because both models are of the same power. This but does not make functional languages operational, what matters here is that they allow to approach a problem from a very different perspective - how it is ultimately executed does not matter and is not visible to the programmer, nor should he or she think about it.
The conclusion is that because of the different approach of operational vs. denotational, one thinks problems very different in either models which in turn also implies that different paradigms and languages are differently well suited to formulate solutions to problems.

All this ultimately leads to the question of how well the pure functional and object-oriented paradigms are suited to formulate ABS in comparison to each other. Also we are interested in the strengths of both paradigms in general and ask if they apply directly when formulating ABS. In this text we want to give an extensive answer to these questions by first looking closer into the abstractions used in pure FP and OOP to formulate an ABS and what the challenges one encounters in doing so.

\section{Challenges}
The challenges one faces when implementing an Agent-Based Simulation (ABS) plain, without support from a library (e.g. Repast) are manifold. In the paper on update-strategies (TODO: cite) we've discussed already in a very general, programming language agnostic way, the fundamental things to consider. Here we will look at the problem in a much more technical way by precisely defining what problems need to be solved and what approaches are from a programming paradigm view-point - where we focus on the pure functional (FP) and imperative object-oriented (OO) paradigms.

Generally one faces the following challenges:

\begin{enumerate}
	\item Agent Representation - How is an Agent represented in the paradigm?
	\item Agent Updating - How is the set of Agents organized and how are all of them updated?
	\item Agent-Agent Interactions 
	\item Environment Representation
	\item Environment Updating
	\item Agent-Environment Interactions
	\item Replications
\end{enumerate}

It is important to note that we are facing a non-trivial software-engineering problem which implies that there are no binary correct \ wrong approaches - whatever works good enough is OK. This implies that the challenges as discussed below, can be also approached in different ways but we tried to stick as close as possible to the \textit{best practices} of the respective paradigm.

%TODO: there is already a low-level haskell/java comparison there: in the code of the update-strategies paper. Also make direct use of this paper as it discusses some of the fundamental challenges implementing ABS in an language- and paradigm-agnostic way