\section{Background}
\label{sec:background}

TODO: what shall we write here? about dependent types in general?

\subsection{Sugarscape}
TODO

Sugarscape is an exploratory model inspired by real-world phenomenon which means it has lots of hypotheses implicit in the model but there does not exist real-world data / dynamics against which one could validate the simulated dynamics. Still we can conduct black-box verification because we have an informal model specification but we cannot do any statistical testing of simulated dynamics as we don't have data acting as ground-truth. But what we can do and what we will explore extensively in this section is how we can encode hypotheses about the dynamics (prior to running the simulation) in unit- and property-based tests and check them. Obviously white-box verification applies as well because we can reason about the code whether it matches the informal model specification or not.

\subsection{Verification \& Validation in ABS}
TODO

Validation \& Verification in ABS
\url{http://www2.econ.iastate.edu/tesfatsi/VVAccreditationSimModels.OBalci1998.pdf} : verification = are we building the model right? validation = are we building the right model?

good paper \url{http://www2.econ.iastate.edu/tesfatsi/VVAccreditationSimModels.OBalci1998.pdf} : very nice 15 guidelines and life cycles, VERY valuable for background and introduction

\url{http://www2.econ.iastate.edu/tesfatsi/VVSimulationModels.JKleijnen1995.pdf} : suggests good programming practice which is extremely important for high quality code and reduces bugs but real world practice and experience shows that this alone is not enough, even the best programmers make mistakes which often can be prevented through a strong static or a dependent type system already at compile time. What we can guarantee already at compile time, doesn't need to be checked at run-time which saves substantial amount of time as at run-time there may be a huge number of execution paths through the simulation which is almost always simply not feasible to check (note that we also need to check all combinations). This paper also cites modularity as very important for verification: divide and conquer and test all modules separately. this is especially easy in functional programming as composability is much better than with traditional oop due to the lack of interdependence between data and code as in objects and the lack of global mutable state (e.g. class variables or global variables) - this makes code extremely convenient to test. The paper also discusses statistical tests (the t test) to check if the outcome of a simulation is sufficiently close to real-world dynamics. Also the paper suggests using animations to visualise the processes within the simulation for verification purposes (of course they note that animation may be misleading when one focuses on too short simulation runs).

good paper:\url{ https://link.springer.com/chapter/10.1007/978-3-642-01109-2_10}
	-> verification. "This is essentially the question: does the model do what we think it is supposed to do? Whenever a model has an analytical solution, a condition which embraces almost all conventional economic theory, verification is a matter of checking the mathematics."
	-> validation: "In an important sense, the current process of building ABMs is a discovery process, of discovering the types of behavioural rules for agents which appear to be consistent with phenomena we observe."
		=> can we encode phenomena we observe in the types? can we use types for the discovery process as well? can dependent types guide our exploratory approach to ABS?
	-> "Because such models are based on simulation, the lack of an analytical solution (in
general) means that verification is harder, since there is no single result the model
must match. Moreover, testing the range of model outcomes provides a test only in
respect to a prior judgment on the plausibility of the potential range of outcomes.
In this sense, verification blends into validation."

either one has an analytical model as the basis of an agent-based model (ABM) or one does not.
In the former case, e.g. the SIR model, one can very easily validate the dynamcis generated by the ABM to the one generated by the analytical solution (e.g. through System Dynamics). Of course the dynamics wont be exactly the same as ABS discretisizes the approach and introduces stochastics which means, one must validate averaged dynamics.
In the latter case one has basically no idea or description of the emergent behaviour of the system prior to its execution. It is important to have some hypothesis about the emergent property / dynamics. The question is how verification / validation works in this setting as there is no formal description of the expected behaviour: we don't have a ground-truth against which we can compare our simulation dynamics. (eventuell hilft hier hans vollbrecht weiter: Simulation hat hier den Sinn, die Controller anhand der Roboteraufgabe zu validieren, Bei solchen Simulationen ist man interessiert an allen möglichen Sequenzen, und da das meist zu viele sind, an einer möglichst gut verteilten Stichprobenmenge. Hier geht es weniger um richtige Zeitmodellierung, sondern um den Test aller möglichen Ereignissequenzen.)

look into DEVS

TODO: the implementation phase is just one stage in a longer process \url{http://jasss.soc.surrey.ac.uk/12/1/1.html}

WE FOCUS ON VERIFICATION
important: we are not concerned here with validating a model with the real world system it simulates. this is an entirely different problem and focuses on the questions if we have built the right model.
we are interested here in extremely strong verification: have we built the model right? we are especially interested in to which extend purely and dependently-typed functional programming can support us in this task.

\url{http://jasss.soc.surrey.ac.uk/8/1/5.html}: "For some time now, Agent Based Modelling has been used to simulate and explore complex systems, which have proved intractable to other modelling approaches such as mathematical modelling. More generally, computer modelling offers a greater flexibility and scope to represent phenomena that do not naturally translate into an analytical framework. Agent Based Models however, by their very nature, require more rigorous programming standards than other computer simulations. This is because researchers are cued to expect the unexpected in the output of their simulations: they are looking for the 'surprise' that shows an interesting emergent effect in the complex system. It is important, then, to be absolutely clear that the model running in the computer is behaving exactly as specified in the design. It is very easy, in the several thousand lines of code that are involved in programming an Agent Based Model, for bugs to creep in. Unlike mathematical models, where the derivations are open to scrutiny in the publication of the work, the code used for an Agent Based Model is not checked as part of the peer-review process, and there may even be Intellectual Property Rights issues with providing the source code in an accompanying web page."

\url{http://jasss.soc.surrey.ac.uk/12/1/1.html}: "a prerequisite to understanding a simulation is to make sure that there is no significant disparity between what we think the computer code is doing and what is actually doing. One could be tempted to think that, given that the code has been programmed by someone, surely there is always at least one person - the programmer - who knows precisely what the code does. Unfortunately, the truth tends to be quite different, as the leading figures in the field report, including the following: You should assume that, no matter how carefully you have designed and built your simulation, it will contain bugs (code that does something different to what you wanted and expected), "Achieving internal validity is harder than it might seem. The problem is knowing whether an unexpected result is a reflection of a mistake in the programming, or a surprising consequence of the model itself. […] As is often the case, confirming that the model was correctly programmed was substantially more work than programming the model in the first place. This problem is particularly acute in the case of agent-based simulation. The complex and exploratory nature of most agent-based models implies that, before running a model, there is some uncertainty about what the model will produce. Not knowing a priori what to expect makes it difficult to discern whether an unexpected outcome has been generated as a legitimate result of the assumptions embedded in the model or, on the contrary, it is due to an error or an artefact created in the model design, its implementation, or its execution."


general requirements to ABS
- modelling progress of time (steward robinson simulation book, chapter 2)
- modelling variability (steward robinson simulation book, chapter 2)
- fixing random number streams to allow simulations to be repeated under same conditions (steward robinson simulation book, chapter 1.3.2 and chapter 2)
- only rely on past
	-> solved with Arrowized FRP
- bugs due to implicitly mutable state
	-> can be ensured by pure functional programming
- ruling out external sources of non-determinism / randomness
	-> can be ensured by pure functional programming
- correct interaction protocols
	-> can be ensured by dependent state machines
- deterministic time-delta
	-> TODO: can we ensure it through dependent-types at type-level?
- repeated runs lead to same dynamics
	-> can be ensured by pure functional programming

steward robinson simulation book bulletpoints
- chapter 8.2: speed of coding, transparency, flexibility, run-speed
- chapter 8.3: three activities - 1 coding, 2 testing verification and white-box validating, 3 documenting
- chapter 9.7: nature of simulation: terminating vs. non-terminating
- chapter 9.7: nature of simulation output: transient or steady-state (steady-state cycle, shifting steady-state)

steward robinson simulation book on implementation
- meaning of implementation
	-> 1 implementing the findings: conduct a study which defines and gathers all findings about the model and document them
	-> 2 implementing the model
	-> 3 implementing the learning

steward robinson simulation book on verification, validation and confidence
- Verification is the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy (Davis 1992)
- Validation is the process of ensuring that the model is sufficiently accurate for the purpose at hand (Carson 1986).
- Verification has a narrow definition and can be seen as a subset of the wider issue of validation
- In Verification and validation the aim is to ensure, that the model is sufficiently accurate, which always implies its purpose.
- => the purpose / objectives mus be known BEFORE it is validated 
- white-box validation: detailed, micro check if each part of the model represent the real world with sufficient accuracy 
	-> intrinsic to model coding
- black-box validation: overall, macro check whether the model provides a sufficiently accurate representation of the real world system
	-> can only be performed once model code is complete
- other definition of verification: it is a test of the fidelity with which the conceptual model is converted into the computer model
- verification (and validation) is a continuous process => if it is already there in the programming language / supported by it e.g. through types,... then this is much easier to do
- difficulties of verification and validation
	-> there is no such thing as general validity: a model should be built for one purpose as simple as possible and not be too general, otherwise it becomes too bloated and too difficult / impossible to analyse
	-> there may be no real world to compare against: simulations are developed for proposed systems, new production / facilities which dont exist yet. 
	-> which real world?: the real world can be interpreted in different ways => a model valid to one person may not be valid to another
	-> often the real world data are inaccurate
	-> there is not enough time to verify and validate everything
	-> confidence, not validity: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. 
		=> verification and validation is thus not the proof that a model is correct but trying to prove that the model is incorrect, the more tests/checks one carries out which show that it is NOT incorrect, the more confidence we can place on the models validity
	- methods of verification and validation
	-> conceptual model validation: judment based on the documentation
	-> data validation: analysing data for inconsistencies
	-> verification and white-box validation
		-> both conceptually different but often treated together because both occur continuously through model coding
		-> what should be checked: timings (cycle times, arrival times,...), control of elements (breakdown frequency, shift patterns), control flows (e.g. routing), control logic (e.g. scheduling, stock replenishment), distribution sampling (samples obtained from an empirial distribution)
	-> verification and whilte-box validation methods
		-> checking code: reading through code and ensure right data and logic is there. explain to others/discuss together/others should look at your code. 
		-> Visual checks
		-> inspecting output reports
		
	-> black-box testing: consider overall behaviour of the model without looking into its parts, basically two ways
		-> comparison with the real system: statistical tests
		-> comparison with another model (e.g. mathematical equations): could compare exactly or also through statistical tests
		-> 
		
peers slides:
	- Model testing (verification and validation)
		-> Required to place confidence in a study's results
		-> Model testing is not a process of trying to demonstrate that the model is correct but a process of trying to prove that the model is incorrect!
		
	- Model verification: The process of ensuring that the model design has been transformed into a computer model with sufficient accuracy
	- Model validation: The process of ensuring that the model is sufficiently accurate for the purpose at hand
		-> models are not meant to be completely accurate
		-> models are supposed to be build for a specific purpose
		
	- Data Validation: Determining that the contextual data and the data required for model realisation and validation are sufficiently accurate for the purpose at hand.

    - white-Box Validation: Determining that the constituent parts of the computer model represent the corresponding real world elements with sufficient accuracy for the purpose at hand (micro check)
    	-> how: Checking the code, visual checks, inspecting output reports
    	
    - Black-Box Validation: Determining that the overall model represents the real world with sufficient accuracy for the purpose at hand (macro check)
    	-> comparison with the real system
    	-> comparison with other (simpler) models
    	
    - Experimentation Validation: Determining that the experimental procedures adopted are providing results that are sufficiently accurate for the purpose at hand.
    	-> How can we do this?
    		- Graphical or statistical methods for determining warm-up period, run length and replications (to obtain accurate results)
			- Sensitivity analysis (to improve the understanding of the model)
	
	- Solution Validation: Determining that the results obtained from the model of the proposed solution are sufficiently accurate for the purpose at hand
		-> How does this differ from Black Box Validation? Solution validation compares the model of the proposed solution to the implemented solution while black-box validation compares the base model to the real world
		-> How can we do this? Once implemented it should be possible to validate the implemented solution against the model results
		
	- Verification: Testing the fidelity with which the conceptual model is converted into the computer model. Verification is done to ensure that the model is programmed correctly, the algorithms have been implemented properly, and the model does not contain errors, oversights, or bugs.

		-> How can we do this? Same methods as for white-box validation (checking the code, visual checks, inspecting output reports) but ... Verification compares the content of the model to the conceptual model while white-box validation compares the content of the model to the real world
		
	- Difficulties of verification and validation
		-> There is no such thing as general validity: a model is only valid with respect to its purpose
		-> There may be no real world to compare against
		-> Which real world? Different people have different interpretations of the real world
		->  Often real world data are inaccurate: If the data are not accurate it is difficult to determine if the model's results are correct. Even if the data is accurate, the real world data are only a sample, which in itself creates inaccuracy
		-> There is not enough time to verify and validate every aspect of a model
		
	- Some final remarks:
		-> V\&V is a continuous and iterative process that is performed throughout the life cycle of a simulation study.
			Example: If the conceptual model is revised as the project progresses it needs to be re-validated
		-> V\&V work together by removing barriers and objections to model use and hence establishing credibility.
		
	- Conclusion: Although, in theory, a model is either valid or not, proving this in practice is a very different matter. It is better to think in terms of confidence that can be placed in a model!
	
TODO: explore ABS testing in pure functional Haskell
- we need to distinguish between two types of testing/verification
	-> 1. testing/verification of models for which we have real-world data or an analytical solution which can act as a ground-truth. examples for such models are the SIR model, stock-market simulations, social simulations of all kind
	-> 2. testing/verification of models which are just exploratory and which are only be inspired by real-world phenomena. examples for such models are Epsteins Sugarscape and Agent\_Zero
