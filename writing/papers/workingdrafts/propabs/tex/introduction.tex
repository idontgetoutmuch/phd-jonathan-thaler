\section{Introduction}
% what is the problem
When implementing an agent-based simulation (ABS) it is of utmost importance that the implementation is correct up to a specification, which is the model. To ensure that an implementation matches a specification, one uses verification, which ensures that \textit{"we are building the model right"} (TODO: cite). With the established approaches in the field of ABS, which are primarily the object-oriented programming languages Java, Python and C++, it is very difficult, or might even be impossible to \textit{formally} prove that an implementation is correct up to a specification. Also one can say in general that formal proofs of correctness are highly complex and take a lot of effort and are almost always beyond the scope of a project and just not feasible. Still, not checking the correctness of the implementation in \textit{some} way would be highly irresponsible and thus software engineers have developed the concept of unit-testing and test-driven development (todo: cite). Put shortly, in test-driven development one implements unit-tests for each feature to implement before actually implementing the feature. Then the features is implemented and the tests for it should pass. This cycle is repeated until the implementation of all requirements has finished. Of course it is important to cover the whole functionality with tests to be sure that all cases are checked which can be supported by code coverage tools to ensure that all code-paths have been tested.
Thus we can say that test-driven development in general and unit-testing together with code-coverage in particular, allow to guarantee the correctness of an implementation to some informal degree which has been proven to be sufficiently enough through years of practice in the software industry all over the world. Also a fundamental strength of such tests is that programmers gain much more confidence when making changes to code - without such tests all bets are off and there is no reliable way to know whether the changes have broken something or not.

% establishing V&V
The book \cite{robinson_simulation:_2014} discusses various aspects of testing and making sure a simulation is correct and distinguishes in this process between \textit{validation} and \textit{verification}. It defines validation to be the process of ensuring that a model or specification is sufficiently accurate for the purpose at hand whereas verification to be the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy. In other words, \cite{balci_verification_1998} define validation as \textit{"are we building the right model?"} and verification as \textit{"are we building the model right?"}.

In this paper we will only focus on verification, because it is there where one ensures that the model is programmed correctly, the algorithms have been implemented properly, and the model does not contain errors, oversights, or bugs. Note that verification has a narrow definition and can be seen as a subset of the wider issue of validation. One distinguishes between:

\begin{itemize}
	\item White-box Verification: compares the content of the model to the \textit{conceptual} model by detailed, micro check if each part of the implementation represent the conceptual model with sufficient accuracy. 
	\item Black-box Verification: treating the functionality to test as a black-box with inputs and outputs and comparing controlled inputs to expected outputs.
\end{itemize}

So in general one can see verification as a test of the fidelity with which the conceptual model is converted into the computer model. Verification is a continuous process and if it is already there in the programming language / supported by then this is much easier to do. A fundamental hypothesis of this paper is that by choosing a programming language which supports this continuous verification and validation process, then the result is an implementation of a model which is more likely to be correct.

TODO: this is not true: we do validation as well when comparing the output of SIR to the formal specification!
In our research we focus primarily on the \textit{Verification} aspect of agent-based simulation: ensuring that the implementation reflects the specifications of the \textit{conceptual} model - have we built the model right? Thus we are not interested in our research into making connections to the real world and always see the model specifications as our "last resort", our ground truth beyond nothing else exists. When there are hypotheses formulated, we always treat and interpret them in respect of the conceptual model.

% establishing TDD
Test-Driven Development (TDD) was conceived in the late 90s by Kent Beck (TODO: cite) as an way to a more agile approach to software-engineering where instead of doing each step (requirements, implementation, testing,...) as separated from each other, all of them are combined in shorter cycles. TDD approaches software construction in a way that one writes first unit-tests for the functionality one wants to test and then iteratively implements this functionality until all tests succeed. This is then repeated until the whole software package is finished. The important difference to e.g. the waterfall model where the steps are done in separation from each other, is that the customer receives a working software package at the end of each short cycle, allowing to change requirements which in turn allows the software-development team to react quickly to changing requirements.

It is important to understand that the unit-tests act both as documentation / specification of what the code / interface which is tested should do and as an insurance against future changes which might break existing code. If the tests cover all possible code paths - there exist tools to measure the test-coverage and visualising the missing code-paths / tests - of the software, then if the tests also succeed after future changes one has very high confidence that these future changes didn't break existing functionality. If though tests break then either the changes are erroneous or the tests are an incomplete specification and need to be adapted to the new features.

TODO: read all the papers peer has sent me
TODO: https://www.atlassian.com/continuous-delivery/different-types-of-software-testing


The authors \cite{ormerod_validation_2006} make the important point that because the current process of building ABS is a discovery process, often models of an ABS lack an analytical solution (in general) which makes verification much harder if there is no such solution.

So the baseline is that either one has an analytical model as the foundation of an agent-based model or one does not. In the former case, e.g. the SIR model, one can very easily validate the dynamics generated by the simulation to the one generated by the analytical solution through System Dynamics. In the latter case one has basically no idea or description of the emergent behaviour of the system prior to its execution e.g. SugarScape. In this case it is important to have some hypothesis about the emergent property / dynamics. The question is how verification / validation works in this setting as there is no formal description of the expected behaviour: we don't have a ground-truth against which we can compare our simulation dynamics.

The paper \cite{polhill_ghost_2005} makes the very important point that ABS should require more rigorous programming standards than other computer simulations. Because researchers in ABS look for an emergent behaviour in the dynamics of the simulation, they are always tempted to look for some surprising behaviour and expect something unexpected from their simulation. Thus it is of utmost importance to ensure that the implementation is matching the specification as closely as possible and nothing is left to chance. The paper \cite{galan_errors_2009} supports this as well and emphasises that the fundamental problem of ABS is that due to its mostly exploratory nature, there exists some amount of uncertainty about the dynamics the simulation will produce before running it. Thus it is often very difficult to judge whether an unexpected outcome can be attributed to the model or has in fact its roots in a subtle programming error.

The work of \cite{kleijnen_verification_1995} suggests good programming practice which is extremely important for high quality code and reduces bugs but real world practice and experience show that this alone is not enough, even the best programmers make mistakes which often can be prevented through a strong static or a dependent type system already at compile-time. What we can guarantee already at compile-time, doesn't need to be checked at run-time which saves substantial amount of time as at run-time there may be a large number of execution paths through the simulation which is almost always simply not feasible to check (note that we also need to check all combinations). This paper also cites modularity as very important for verification: divide and conquer and test all modules separately. We claim that this is especially easy in functional programming as code composes better than in traditional object-oriented programming due to the lack of interdependence between data and code as in objects and the lack of global mutable state (e.g. class variables or global variables) - this makes code extremely convenient to test. The paper also discusses statistical tests (the t test) to check if the outcome of a simulation is sufficiently close to real-world dynamics - we explicitly omit this as it part of validation and not the focus of this research.

Unfortunately, there is no such thing as general validity: a model should be built for one purpose as simple as possible and not be too general, otherwise it becomes too bloated and too difficult or impossible to analyse. Also, there may be no real world to compare against: simulations are developed for proposed systems, new production or facilities which don't exist yet. Further, it is questionable which real world one is speaking of: the real world can be interpreted in different ways, therefore a model valid to one person might not be valid to another. Sometimes validation struggles because the real world data are inaccurate or there is not enough time to verify and validate everything.

In general this implies that we can only \textit{raise the confidence} in the correctness of the simulation: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. Therefore, the process of verification and validation is not the proof that a model is correct but trying to prove that the model is incorrect! The more tests/checks one carries out which show that it is not incorrect, the more confidence we can place on the models validity.

% related works
%% TDD in ABS
Research on TDD of ABS is quite new and thus there exists relative few publications. The work of \cite{collier_test-driven_2013} is the first to discusses how to apply the TDD approach to ABS, using unit-testing to verify the correctness of the implementation up to a certain level. They showed how to implement unit-tests within the RePast Framework and make the important point that such a software need to be designed to be sufficiently modular otherwise testing becomes too cumbersome and involves too many parts. The paper \cite{asta_investigation_2014} discusses a similar approach to DES in the AnyLogic software toolkit. 

The paper \cite{onggo_test-driven_2016} proposes Test Driven Simulation Modelling (TDSM) which combines techniques from TDD to simulation modelling. They present a case study for maritime search-operations where they employ ABS. They emphasise that simulation modelling is an iterative process where changes are made to existing parts thus a TDD approach to simulation modelling seems to be a good match. They present how to validate their model against analytical solutions from theory using unit-tests by running the whole simulation within a unit-test and then perform a statistical comparison against a formal specification. This approach will become of importance later on in our SIR case study.

The paper \cite{brambilla_property-driven_2012} propose property-driven design of robot swarms. They propose a top-down approach by specifying properties a swarm of robots should have from which a prescriptive model is created, which properties are verified using model checking. Then a simulation is implemented following this prescriptive and verified model after then the physical robots are implemented. The authors identify the main difficulty of implementing such a system that the engineer must \textit{"think at the collective-level, but develop at the individual-level}. It is arguably true that this also applies to implementing agent-based models and simulations where the same collective-individual separation exists from which emergent system behaviour of simulations emerges - this is the very foundation of the ABS methodology.

%% TDD in MAS
Although the work on TDD is scarce in ABS, there exists quite some research on applying TDD and unit-testing to MAS. Although MAS is a different discipline than ABS, the latter one has derived many technical concepts from the former one thus testing concepts applied to MAS might be also applicable to ABS. 

The paper \cite{nguyen_testing_2011} is a survey of testing in multi-agent systems (MAS). It distinguishes between unit tests which tests units that make up an agent, agent tests which test the combined functionality of units that make up an agent, integration tests which test the interaction of agents within an environment and observe emergent behaviour, system test which test the MAS as a system running at the target environment and acceptance test in which stakeholders verify that the software meets their goal. The paper enumerates existing research and shows that some research is working on generating automated test input for agent level tests.

The paper \cite{tiryaki_sunit:_2007} discusses Test Driven Development in MAS and puts much emphasis on proposing agile processes to develop MAS software to handle complexity and continuously changing nature of requirements. The authors develop the SUNIT testing framework to implement unit-testing in an MAS environment.

% the gap 
In this paper we discuss a complementary method of testing the implementation of an ABS, called \textit{property-based} testing, which allows to directly express model-specifications in code and test them through \textit{automated} test-generation. We see it as an addition to TDD where it works in combination with unit-testing to verify and validate a simulation to increase the confidence in its correctness. Property-based testing has its origins \cite{claessen_quickcheck:_2000, claessen_testing_2002, runciman_smallcheck_2008} in the pure functional programming language Haskell \cite{hudak_history_2007} where it was first conceived and implemented and thus we discuss it from that perspective. It has been successfully used for testing Haskell code for years an also been proven valuable and successful in the industry \cite{hughes_quickcheck_2007}, thus we investigate its potential for ABS. To to our best knowledge property-based testing has not been discussed in the field of ABS yet. 

Property-based testing has a close connection to model-checking \cite{mcmillan_symbolic_1993}, where properties of a system are proved in a formal way. The important difference is that the checking happens directly on code and not on the abstract, formal model, thus one can say that it combines model-checking and unit-testing, embedding it directly in the software-development and TDD process without an intermediary step.

We hypothesise that adding it to the already existing testing methods in the field of ABS is of substantial value as it allows to cover a much wider range of test-cases due to automatic generation. This can be used in two ways: to verify an implementation against a formal specification or to test hypotheses about an implemented simulation. The work of \cite{onggo_test-driven_2016} explicitly mention the problem of test coverage which would often require to write a large number of tests manually to cover the parameter ranges sufficiently enough - property-based testing addresses exactly this problem by \textit{automating} the test generation.
To substantiate and test our claims, we present two models as case-studies. First, the agent-based SIR model \cite{macal_agent-based_2010}, which is of explanatory nature, where we show how to express formal model-specifications in property-tests. Second, the SugarScape model \cite{epstein_growing_1996}, which is of exploratory nature, where we show how to express hypotheses in property-tests. 

% aim & contribution
The aim and contribution of this paper is the investigation of the potential of pure functional property-based testing for ABS using Haskell as programming language. Further we will show that by simply using a pure functional programming language removes a large class of run-time errors and allows much stronger guarantees of correctness already at compile time, making the implementation to \textit{very likely} be correct.

% structure
The structure of the paper is as follows. First we discuss the background in which we conceptually introduce property-based testing and pure functional programming in Haskell to make this paper sufficiently self-contained. Then we shortly present existing research on \textit{how} to implement ABS in Haskell. Then we present both case-studies in Section \ref{sec:case_SIR} and Section \ref{sec:case_sug}. Finally we conclude in Section \ref{sec:conclusions} and give further research in Section \ref{sec:further}. 