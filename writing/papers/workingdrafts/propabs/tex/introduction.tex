\section{Introduction}
% setting the scene: what is the problem
When implementing an Agent-Based Simulation (ABS) it is of fundamental importance that the implementation is correct up to some specification and that this specification matches the real world in some way. This process is called verification and validation (V\&V), where \textit{validation} is the process of ensuring that a model or specification is sufficiently accurate for the purpose at hand whereas \textit{verification} is the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy \cite{robinson_simulation:_2014}. In other words, validation determines if we are we building the \textit{right model} and verification if we are we building the \textit{model right} \cite{balci_verification_1998}.

% there is no general validity, an approach is TDD: V&V particularly difficult in ABS
One can argue that ABS should require more rigorous programming standards than other computer simulations \cite{polhill_ghost_2005}. Because researchers in ABS look for an emergent behaviour in the dynamics of the simulation, they are always tempted to look for some surprising behaviour and expect something unexpected from their simulation. %Thus it is of utmost importance to ensure that the implementation is matching the specification as closely as possible and nothing is left to chance.
Also, due to ABS mostly exploratory nature, there exists some amount of uncertainty about the dynamics the simulation will produce before running it. Thus it is often very difficult to judge whether an unexpected outcome can be attributed to the model or has in fact its roots in a subtle programming error \cite{galan_errors_2009}.

In general this implies that we can only \textit{raise the confidence} in the correctness of the simulation: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. Therefore, the process of V\&V is not the proof that a model is correct but trying to prove that the model is incorrect. The more tests/checks one carries out which show that it is not incorrect, the more confidence we can place on the models validity. To tackle such a problem in software, software engineers have developed the concept of test-driven development (TDD) (todo: cite).

% establishing TDD
Test-Driven Development (TDD) was conceived in the late 90s by Kent Beck (TODO: cite) as a way to a more agile approach to software-engineering where instead of doing each step (requirements, implementation, testing,...) as separated from each other, all of them are combined in shorter cycles. TDD approaches software construction in a way that one writes first unit-tests for the functionality one wants to test and then iteratively implements this functionality until all tests succeed. This is then repeated until the whole software package is finished. The important difference to traditional models, where the steps are done in separation from each other, is that the customer receives a working software package at the end of each short cycle, allowing to change requirements which in turn allows the software-development team to react quickly to changing requirements.

It is important to understand that the unit-tests act both as documentation / specification of what the code / interface which is tested should do and as an insurance against future changes which might break existing code. If the tests cover all possible code paths - there exist tools to measure the test-coverage and visualising the missing code-paths / tests - of the software, then if the tests also succeed after future changes one has very high confidence that these future changes didn't break existing functionality. If though tests break then either the changes are erroneous or the tests are an incomplete specification and need to be adapted to the new features.

%The authors \cite{ormerod_validation_2006} make the important point that because the current process of building ABS is a discovery process, often models of an ABS lack an analytical solution (in general) which makes verification much harder if there is no such solution.

%So the baseline is that either one has an analytical model as the foundation of an agent-based model or one does not. In the former case, e.g. the SIR model, one can very easily validate the dynamics generated by the simulation to the one generated by the analytical solution through System Dynamics. In the latter case one has basically no idea or description of the emergent behaviour of the system prior to its execution e.g. SugarScape. In this case it is important to have some hypothesis about the emergent property / dynamics. The question is how verification / validation works in this setting as there is no formal description of the expected behaviour: we don't have a ground-truth against which we can compare our simulation dynamics.

%Put shortly, in TDD one implements unit-tests for each feature to implement before actually implementing the feature. Then the features is implemented and the tests for it should pass. This cycle is repeated until the implementation of all requirements has finished. Of course it is important to cover the whole functionality with tests to be sure that all cases are checked which can be supported by code coverage tools to ensure that all code-paths have been tested.

Thus we can say that test-driven development in general and unit-testing together with code-coverage in particular, allow to guarantee the correctness of an implementation to some informal degree, which has been proven to be sufficiently enough through years of practice in the software industry all over the world. Also a fundamental strength of such tests is that programmers gain much more confidence when making changes to code - without such tests all bets are off and there is no reliable way to know whether the changes have broken something or not.

% the gap 
In this paper we discuss a complementary method of testing the implementation of an ABS, called \textit{property-based} testing, which allows to directly express model-specifications in code and test them through \textit{automated} test-data generation. We see it as an addition to TDD where it works in combination with unit-testing to verify and validate a simulation to increase the confidence in its correctness.

Property-based testing has its origins \cite{claessen_quickcheck:_2000, claessen_testing_2002, runciman_smallcheck_2008} in the pure functional programming language Haskell \cite{hudak_history_2007} where it was first conceived and implemented and thus we discuss it from that perspective. It has been successfully used for testing Haskell code for years and also been proven to be useful in the industry \cite{hughes_quickcheck_2007}, thus we investigate its potential for ABS, which to to our best knowledge has not been done yet. 

Property-based testing has a close connection to model-checking \cite{mcmillan_symbolic_1993}, where properties of a system are proved in a formal way. The important difference is that the checking happens directly on code and not on the abstract, formal model, thus one can say that it combines model-checking and unit-testing, embedding it directly in the software-development and TDD process without an intermediary step. We hypothesise that adding it to the already existing testing methods in the field of ABS is of substantial value as it allows to cover a much wider range of test-cases due to automatic data generation. This can be used in two ways: to verify an implementation against a formal specification or to test hypotheses about an implemented simulation. This puts property-based testing on the same level as agent- and system testing, where not technical implementation details of e.g. agents are checked like in unit-tests but their individual complete behaviour and the system behaviour as a whole.

The work \cite{onggo_test-driven_2016} explicitly mentions the problem of test coverage which would often require to write a large number of tests manually to cover the parameter ranges sufficiently enough - property-based testing addresses exactly this problem by \textit{automating} the test-data generation. Note that this is closely related to data-generators \cite{gurcan_generic_2013} or load generators \cite{burnstein_practical_2010} but property-based testing goes one step beyond by integrating them into a specification language directly into code, emphasising a declarative approach and pushing the generators behind the scenes, making them transparent and focusing on the specification rather than on the data-generation. 

To substantiate and test our claims, we present two case-studies. First, the agent-based SIR model \cite{macal_agent-based_2010}, which is of explanatory nature, where we show how to express formal model-specifications in property-tests. Second, the SugarScape model \cite{epstein_growing_1996}, which is of exploratory nature, where we show how to express hypotheses in property-tests. 

% aim & contribution
The aim and contribution of this paper is the investigation of the potential of pure functional property-based testing for ABS using Haskell as programming language. Further we will show that by simply using a pure functional programming language removes a large class of run-time errors and allows much stronger guarantees of correctness already at compile time, increasing the confidence in the correctness of the simulation up to a new level.

% structure
The structure of the paper is as follows: First we present related work in Section \ref{sec:related}. To make this paper sufficiently self-contained, we introduce pure functional programming in Haskell on a conceptual level in Section \ref{sec:fp}. Then we give a more in-depth explanation of property-based testing in Section \ref{sec:proptesting}. Next we shortly present existing research on \textit{how} to implement ABS in Haskell and conceptually apply property-based testing in Section \ref{sec:pfABS}. The heart of the paper are the two case-studies, which we present in Section \ref{sec:case_SIR} and \ref{sec:case_sug}. Finally we conclude in Section \ref{sec:conclusions} and discuss further research in Section \ref{sec:further}. 