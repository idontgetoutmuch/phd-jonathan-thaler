\section{Background}
The book \cite{robinson_simulation:_2014} discusses various aspects of testing and making sure a simulation is correct and distinguishes in this process between \textit{validation} and \textit{verification}. It defines validation to be the process of ensuring that a model or specification is sufficiently accurate for the purpose at hand whereas verification to be the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy. In other words, \cite{balci_verification_1998} define validation as \textit{"are we building the right model?"} and verification as \textit{"are we building the model right?"}.

In this paper we will only focus on verification, because it is there where one ensures that the model is programmed correctly, the algorithms have been implemented properly, and the model does not contain errors, oversights, or bugs. Note that verification has a narrow definition and can be seen as a subset of the wider issue of validation. One distinguishes between:

\begin{itemize}
	\item White-box Verification: compares the content of the model to the \textit{conceptual} model by detailed, micro check if each part of the implementation represent the conceptual model with sufficient accuracy. 
	\item Black-box Verification: treating the functionality to test as a black-box with inputs and outputs and comparing controlled inputs to expected outputs.
\end{itemize}

So in general one can see verification as a test of the fidelity with which the conceptual model is converted into the computer model. Verification is a continuous process and if it is already there in the programming language / supported by then this is much easier to do. A fundamental hypothesis of this paper is that by choosing a programming language which supports this continuous verification and validation process, then the result is an implementation of a model which is more likely to be correct.

Unfortunately, there is no such thing as general validity: a model should be built for one purpose as simple as possible and not be too general, otherwise it becomes too bloated and too difficult or impossible to analyse. Also, there may be no real world to compare against: simulations are developed for proposed systems, new production or facilities which don't exist yet. Further, it is questionable which real world one is speaking of: the real world can be interpreted in different ways, therefore a model valid to one person might not be valid to another. Sometimes validation struggles because the real world data are inaccurate or there is not enough time to verify and validate everything.

In general this implies that we can only \textit{raise the confidence} in the correctness of the simulation: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. Therefore, the process of verification and validation is not the proof that a model is correct but trying to prove that the model is incorrect! The more tests/checks one carries out which show that it is not incorrect, the more confidence we can place on the models validity.

In our research we focus primarily on the \textit{Verification} aspect of agent-based simulation: ensuring that the implementation reflects the specifications of the \textit{conceptual} model - have we built the model right? Thus we are not interested in our research into making connections to the real world and always see the model specifications as our "last resort", our ground truth beyond nothing else exists. When there are hypotheses formulated, we always treat and interpret them in respect of the conceptual model.

The authors \cite{ormerod_validation_2006} make the important point that because the current process of building ABS is a discovery process, often models of an ABS lack an analytical solution (in general) which makes verification much harder if there is no such solution.

So the baseline is that either one has an analytical model as the foundation of an agent-based model or one does not. In the former case, e.g. the SIR model, one can very easily validate the dynamics generated by the simulation to the one generated by the analytical solution through System Dynamics. In the latter case one has basically no idea or description of the emergent behaviour of the system prior to its execution e.g. SugarScape. In this case it is important to have some hypothesis about the emergent property / dynamics. The question is how verification / validation works in this setting as there is no formal description of the expected behaviour: we don't have a ground-truth against which we can compare our simulation dynamics.

The paper \cite{polhill_ghost_2005} makes the very important point that ABS should require more rigorous programming standards than other computer simulations. Because researchers in ABS look for an emergent behaviour in the dynamics of the simulation, they are always tempted to look for some surprising behaviour and expect something unexpected from their simulation. Thus it is of utmost importance to ensure that the implementation is matching the specification as closely as possible and nothing is left to chance. The paper \cite{galan_errors_2009} supports this as well and emphasises that the fundamental problem of ABS is that due to its mostly exploratory nature, there exists some amount of uncertainty about the dynamics the simulation will produce before running it. Thus it is often very difficult to judge whether an unexpected outcome can be attributed to the model or has in fact its roots in a subtle programming error.

The work of \cite{kleijnen_verification_1995} suggests good programming practice which is extremely important for high quality code and reduces bugs but real world practice and experience show that this alone is not enough, even the best programmers make mistakes which often can be prevented through a strong static or a dependent type system already at compile-time. What we can guarantee already at compile-time, doesn't need to be checked at run-time which saves substantial amount of time as at run-time there may be a large number of execution paths through the simulation which is almost always simply not feasible to check (note that we also need to check all combinations). This paper also cites modularity as very important for verification: divide and conquer and test all modules separately. We claim that this is especially easy in functional programming as code composes better than in traditional object-oriented programming due to the lack of interdependence between data and code as in objects and the lack of global mutable state (e.g. class variables or global variables) - this makes code extremely convenient to test. The paper also discusses statistical tests (the t test) to check if the outcome of a simulation is sufficiently close to real-world dynamics - we explicitly omit this as it part of validation and not the focus of this research.

\subsection{Test-Driven Development}
While there exists quite some work on validating an ABS, there doesn't exist much work on verification which discusses the problem from an implementation perspective with program code. The only work we could find is \cite{collier_test-driven_2013} which briefly discusses how to apply the test-driven development approach to ABS, using unit-testing to check the correctness of the implementation.

\subsection{Property-Based Testing}
TODO: explain pure functional programming first

TODO: QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs \cite{claessen_quickcheck:_2000}
TODO: Testing Monadic Code with QuickCheck \cite{claessen_testing_2002}
