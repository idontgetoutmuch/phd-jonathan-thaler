\section{Introduction}
% setting the scene: what is the problem
When implementing an Agent-Based Simulation (ABS) it is of fundamental importance that the implementation is correct up to some specification and that this specification matches the real world in some way. This process is called verification and validation (V\&V), where \textit{validation} is the process of ensuring that a model or specification is sufficiently accurate for the purpose at hand whereas \textit{verification} is the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy \cite{robinson_simulation:_2014}. In other words, validation determines if we are we building the \textit{right model} and verification if we are we building the \textit{model right} \cite{balci_verification_1998}.

% there is no general validity, an approach is TDD: V&V particularly difficult in ABS
One can argue that ABS should require more rigorous programming standards than other computer simulations \cite{polhill_ghost_2005}. Because researchers in ABS look for an emergent behaviour in the dynamics of the simulation, they are always tempted to look for some surprising behaviour and expect something unexpected from their simulation. 
Also, due to ABS mostly exploratory nature, there exists some amount of uncertainty about the dynamics the simulation will produce before running it. The authors \cite{ormerod_validation_2006} see the current process of building ABS as a discovery process where often models of an ABS lack an analytical solution (in general) which makes verification much harder if there is no such solution. Thus it is often very difficult to judge whether an unexpected outcome can be attributed to the model or has in fact its roots in a subtle programming error \cite{galan_errors_2009}.

In general this implies that we can only \textit{raise the confidence} in the correctness of the simulation: it is not possible to prove that a model is valid, instead one should think of confidence in its validity. Therefore, the process of V\&V is not the proof that a model is correct but the process of trying to prove that the model is incorrect. The more checks one carries out which show that it is not incorrect, the more confidence we can place on the models validity. To tackle such a problem in software, software engineers have developed the concept of test-driven development (TDD).

Put shortly, in TDD one writes to called tests for each feature before actually implementing it. One should implement as many tests as necessary to fully test the functionality of that feature. Then the feature is implemented and the tests for it should pass. This cycle is repeated until the implementation of all requirements has finished. Of course it is important to cover the whole functionality with tests to be sure that all cases are checked which can be supported by code coverage tools to ensure that all code-paths have been tested. Traditionally TDD relies on so called unit-tests which can be understood as a piece of code which when run, tests some functionality of an implementation. Each unit-test should only test a small, clearly distinguishable part of the implementation and should not depend on other tests e.g. which need to run before or after - it should be able to run isolated.

% establishing TDD
%Test-Driven Development (TDD) was conceived in the late 90s by Kent Beck (TODO: cite) as a way to a more agile approach to software-engineering where instead of doing each step (requirements, implementation, testing,...) as separated from each other, all of them are combined in shorter cycles. TDD approaches software construction in a way that one writes first unit-tests for the functionality one wants to test and then iteratively implements this functionality until all tests succeed. This is then repeated until the whole software package is finished. The important difference to traditional models, where the steps are done in separation from each other, is that the customer receives a working software package at the end of each short cycle, allowing to change requirements which in turn allows the software-development team to react quickly to changing requirements.

%It is important to understand that the unit-tests act both as documentation / specification of what the code / interface which is tested should do and as an insurance against future changes which might break existing code. If the tests cover all possible code paths - there exist tools to measure the test-coverage and visualising the missing code-paths / tests - of the software, then if the tests also succeed after future changes one has very high confidence that these future changes didn't break existing functionality. If though tests break then either the changes are erroneous or the tests are an incomplete specification and need to be adapted to the new features.Â´

%Thus we can say that test-driven development in general and unit-testing together with code-coverage in particular, allow to guarantee the correctness of an implementation to some informal degree, which has been proven to be sufficiently enough through years of practice in the software industry all over the world. Also a fundamental strength of such tests is that programmers gain much more confidence when making changes to code - without such tests all bets are off and there is no reliable way to know whether the changes have broken something or not.

% the gap 
In this paper we discuss \textit{property-based} testing, a complementary method of testing the implementation of an ABS, which allows to directly express model-specifications and laws in code and test them through \textit{automated} test-data generation. We see it as an addition to TDD where it works in combination with unit-testing to verify and validate a simulation to increase the confidence in its correctness.

Property-based testing has its origins \cite{claessen_quickcheck_2000,claessen_testing_2002,runciman_smallcheck_2008} in the pure functional programming language Haskell \cite{hudak_history_2007} where it was first conceived and implemented and thus we discuss it from that perspective. It has been successfully used for testing Haskell code for years and also been proven to be useful in the industry \cite{hughes_quickcheck_2007}.

To substantiate and test our claims, we present two case-studies. First, the agent-based SIR model \cite{macal_agent-based_2010}, which is of explanatory nature, where we show how to express formal model-specifications in property-tests. Second, the SugarScape model \cite{epstein_growing_1996}, which is of exploratory nature, where we show how to express hypotheses in property-tests and how to property-test agent functionality. 

% aim & contribution
The aim and contribution of this paper is the introduction of property-based testing to ABS. To our best knowledge property-based testing has never been looked at in the context of ABS and this paper is the first one to do so.

% structure
The structure of the paper is as follows: First we present related work in Section \ref{sec:related}. Then we give a more in-depth explanation of property-based testing in Section \ref{sec:proptesting}. Next we shortly discuss how to conceptually apply property-based testing to ABS in Section \ref{sec:testingABS}. The heart of the paper are the two case-studies, which we present in Section \ref{sec:case_SIR} and \ref{sec:case_sug}. Finally we conclude and discuss further research in Section \ref{sec:conclusions}.