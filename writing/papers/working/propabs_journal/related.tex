\section{Related Work}
\label{sec:related}

Research on code testing of ABS is quite new with few publications so far. Our own work \cite{thaler_show_2019} is the first paper to introduce property-based testing to ABS. In it we show on a conceptual level that property-based testing allows to do both verification and validation of an implementation. However, in this work we do not go into technical details of actual implementations nor how to use property-based testing on a technical level.

The work of Collier et al. \cite{collier_test-driven_2013} is the first to discusses how to apply TDD to ABS, using unit testing \cite{beck_test_2002} to verify the correctness of the implementation up to a certain level. They show how to implement unit tests within the RePast Framework  and make the important point that such a software needs to be designed to be sufficiently modular otherwise testing becomes too cumbersome and involves too many parts. The authors of \cite{asta_investigation_2014} discuss a similar approach to DES in the AnyLogic software toolkit. 

In \cite{onggo_test-driven_2016} the authors propose Test Driven Simulation Modeling (TDSM) which combines techniques from TDD to simulation modeling. The authors present a case study for maritime search operations where they employ ABS. They emphasize that simulation modeling is an iterative process, where changes are made to existing parts, making a TDD approach to simulation modeling a good match. They present how to validate their model against analytical solutions from theory using unit tests by running the whole simulation within a unit test and then perform a statistical comparison against a formal specification. This approach is important for our SIR and Sugarscape case studies.

The paper \cite{brambilla_property-driven_2012} proposes property-driven design of robot swarms. The authors propose a top-down approach by specifying properties a swarm of robots should have from which a prescriptive model is created, which properties are verified using model checking. Then a simulation is implemented following this prescriptive and verified model after then the physical robots are implemented. The authors identify the main difficulty of implementing such a system that the engineer must \textit{"think at the collective-level, but develop at the individual-level}. It is arguably true that this also applies to implementing agent-based models and simulations where the same collective-individual separation exists from which emergent system behaviour of simulations emerges - this is the very foundation of the ABS methodology.

The authors of \cite{gurcan_generic_2013} give an in-depth and detailed overview over verification, validation and testing of agent-based models and simulations and proposes a generic framework for it. The authors present a generic UML class-model for their framework which they then implement in the two ABS frameworks RePast and MASON. Both of them are implemented in Java and the authors provide a detailed description how their generic testing framework architecture works and how it utilizes JUnit to run automated tests. To demonstrate their framework they provide also a case study of an agent-base simulation of synaptic connectivity where they provide an in-depth explanation of their levels of test together with code.

% TDD in MAS
Although the work on TDD is scarce in ABS, there exists quite some research on applying TDD and unit testing to Multi-Agent Systems (MAS). Although MAS is a different discipline than ABS, the latter one has derived many technical concepts from the former one, thus testing concepts applied to MAS might also be applicable to ABS. \cite{nguyen_testing_2011} performed a survey of testing in MAS. It distinguishes between unit tests of parts that make up an agent, agent tests which test the combined functionality of parts that make up an agent, integration tests which test the interaction of agents within an environment and observe emergent behaviour, system tests which test the MAS as a system running at the target environment and acceptance test in which stakeholders verify that the software meets their goal. Although not all ABS simulations need acceptance and system tests, still this classification gives a good direction and can be directly transferred to ABS. 

Property-based testing has a close connection to model-checking \cite{mcmillan_symbolic_1993}, where properties of a system are proved in a formal way. The important difference is that the checking happens directly on code and not on the abstract, formal model, thus one can say that it combines model-checking and unit testing, embedding it directly in the software-development and TDD process without an intermediary step. We hypothesise that adding it to the already existing testing methods in the field of ABS is of substantial value as it allows to cover a much wider range of test-cases due to automatic data generation. This can be used in two ways: to verify an implementation against a formal specification and to test hypotheses about an implemented simulation. This puts property-based testing on the same level as agent- and system testing, where not technical implementation details of e.g. agents are checked like in unit tests but their individual complete behaviour and the system behaviour as a whole.

The work of \cite{onggo_test-driven_2016} explicitly mention the problem of test coverage, which would often require to write a large number of tests manually to cover the parameter ranges sufficiently enough - property-based testing addresses exactly this problem by \textit{automating} the test-data generation. Note that this is closely related to data-generators \cite{gurcan_generic_2013} and load generators and random testing \cite{burnstein_practical_2010} but property-based testing goes one step further by integrating this into a specification language directly into code, emphasizing a declarative approach and pushing the generators behind the scenes, making them transparent and focusing on the specification rather than on the data-generation. 