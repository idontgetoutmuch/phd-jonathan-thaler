\section{Case-Study: Pure Functional SugarScape}
\label{sec:case_study}

To explore how to approach ABS based on pure functional programming concepts as introduced before, we did a \textit{full and verified} implementation of the seminal Sugarscape model \cite{epstein_growing_1996}. We chose the model because it is quite well known in the ABS community, it was highly influential in sparking the interest in ABS, it is quite complex with non-trivial agent-interactions and it used object-oriented techniques and explicitly advocates them as a good fit to ABS. 

Our goal was first to develop techniques and concepts to show \textit{how} to engineer a clean, maintainable and robust ABS in Haskell. The second step was then to identify benefits and drawbacks to identify \textit{why} one would follow such an approach. In a third step we pushed the benefits of pure functional programming further and tried to find a remedy for the drawbacks. Absolutely paramount in our research was, that we are being \textit{pure}, which avoids the IO effect type under all circumstances because we would practically lose all strong compile time guarantees \footnote{The code is freely accessible from \url{https://github.com/thalerjonathan/phd/tree/master/public/towards/SugarScape}}.

TODO: \cite{macal_everything_2016} sugarscape is level 4, chapter 2 is level 3 or 2?

TODO: page 28, footnote 16: we can guarantee that in haskell at compile time

\subsection{A Functional View}
Due to the fundamentally different approaches of functional programming (FP) an ABS needs to be implemented fundamentally different as well compared to established object-oriented (OO) approaches. We face the following challenges:

\begin{enumerate}
	\item How can we represent an Agent, its local state and its interface?
	\item How can we implement direct agent-to-agent interactions?
	\item How can we implement an environment and agent-to-environment interactions? 
\end{enumerate}

The fundamental building blocks to solve these problems are \textit{recursion} and \textit{continuations}. In recursion a function is defined in terms of itself: in the process of computing the output it \textit{might} call itself with changed input data. Continuations in turn allow to encapsulate the execution state of a program including local variables and pick up computation from that point later on.

This allows us to define an agent as a function, the question is what its input and output are. Event-driven approach \cite{meyer_event-driven_2014} Agent-agent interactions are trivial in object-orientation: one either makes a direct method call or send an event, mutating the internal state of the receiving agent. In functional programming we need to come up with alternatives because neither method-calls nor globally mutable state is available.
TODO: derive the agent-interface, which is driven by agent-interactions

As output the agent returns a data-structure which holds all \textit{observable} information which the agent wants to share with the outside world. Together with the continuation this guarantees that the agent is in full control over its local state, which no one can mutate or access from outside. This also implies that one can only get information out of the agent by running its function. It also means that the output type of the function has to cover all possible input cases - it cannot change or depend on the input. 

The alternative are \textit{synchronous} interactions which are necessary when an arbitrary number of interactions between two agents need to happen instantaneously without any time-steps in between. The use-case for this are price negotiations between multiple agents where each pair of agents needs to come to an agreement in the same time-step \cite{epstein_growing_1996}. In object-oriented programming, the concept of synchronous communication between agents is trivially implemented directly with method calls but it can get tricky to get right in an functional programming setting. The only option one has, is to dynamically find the target agents signal function and run it within the source agent. This would imply some effectful context which allows read/write to all signal functions in the system: we need to read it to find the target and write it to put the continuation back in because it has locally encapsulated state. This is active research we conduct at the moment and we leave this for further research as it is out of the scope of this paper.

\begin{HaskellCode}
TODO: give a short example of continuation agent
\end{HaskellCode}

From this example it becomes apparent that we can encapsulate local state which which is not accessible and mutable from outside but only through explicit inputs and outputs to the continuation.

Obviously the agents in the Sugarscape are located in a discrete 2d environment where they move around and harvest resources, which means the need to read and write data of environment. This is conveniently implemented by adding a State side-effect type to the agent continuation function. Further we also add a Random effect type because dynamics in most ABS in general and Sugarscapes in particular are driven by random number streams, so our agent needs to have access to one as well.

\subsection{Code metrics}
We used the command line tool \textit{cloc} to count the lines of Haskell code we have written (ignoring comments, reporting only the 'code' values)

TODO: cite the book / paper  (?) which report the metrics of the sugarscape implementation.

Count LoC of NetLogo (4.0.4, as 5.1 seemed to have bugs in some of their functionality): 2128 LoC in a single (!) file (Sugarscape.nlogo)
Count LoC of Java implementation (http://sugarscape.sourceforge.net/): 6525 in 5 files
Count LoC of Python (https://github.com/citizen-erased/sugarscape): 1109 in 9 files

Count LoC of my implementation
- complete project: ~4300 in 38 files
- complete project without test-code ~3660 in 27 files
- test code: ~635 in 11 files
- simulation-core and infrastructure (no rendering): ~1550 in 9 files
- data-export: ~70 in 1 file
- visualisation: ~200 in 2 files
- agent-behaviour only: ~1700 in 14 files

Big difference in our implementation
- lots of lines are type-, import- and export (module) declarations. We conjecture that roughly 40\% of the whole code consists of such declarations.

- several hundred lines are the scenario-definitions
- what we provide in addition (netlogo does not need): simulation kernel, infrastructure, utilities, exporting of data, low-level rendering

\subsection{Memory and Performance}
Haskell is notorious for its space-leaks due to laziness. Even for simple programs one can be hit by a serious space-leak where unevaluated code pieces (thunks) builds up in memory until they are needed, leading to dramatically increased memory usage for a problem which should be solved using a fraction.

It is no surprise that our highly complex sugarscape implementation (TODO: what about our SIR implementation) suffered severely from space-leaks. In Simulation this is a big issue, threatening the value of the whole implementation despite its other benefits: because simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays constant.

Exactly this was violated in our sugarscape implementation where the memory usage increased linearly with about 40MByte per second! 
Haskell allows to add so-called Strict pargmas to code-modules which forces strict evaluation of all data even if it is not used. Carefully adding this conservatively file-by file and checking for changes in memory-leaks reduced the memory consumption considerably and also led to a substantial performance increase. Now only the environment data-structure left leaking.

\subsection{Concurrency and parallelism}
To see how difficult it was to build a concurrent implementation we took the existing sequential implementation and added concurrency to it using Software Transactional Memory (STM). The main idea behind STM is that instead of locking and synchronising access to shared data, STM executes code-blocks as atomic transactions which either commit successfully in case no dirty-read happened or retries in case the value was changed since its last read. Although STM exists in other languages as well, Haskells type-system guarantees that retries have no persisting side-effects, which is crucial for the retry-semantics of STM implementations. We have written a separate paper about using STM to implement concurrent ABS TODO cite my paper in TOMACS, thus we will not go into more detail here but refer to that paper instead. 

\subsection{Testing}
We implemented a number of tests for agent functions which don't cover a whole sub-part of an agents behaviour: checks whether an agent has died of age, check whether an agent has starved to death, the metabolism, immunisation step, check if an agent is a potential borrower, check for fertility, lookout, trading transaction. What all these functions have in common is that they are not pure computations like utility functions but are already running within an agent-context which means they have access to the agent state, environment, simulation context and random-number stream. This makes testing harder because one needs to construct more complex simulation state and needs to run the agent-context with the provided states.

TODO: shortly describe property-based testing
Property-Based works surprisingly well in this context because properties seem to be quite abound here. We simply implement data-generators for our agent state and environment and its cells and then let QuickCheck generate the random data and us running the agent with the provided data, checking for the properties. An example for such a property is that an agent has starved to death in case its sugar (or spice) level has dropped to 0. The corresponding property-test generates a random agent state and also a random sugar level which we set in the agent state. We then run the function which returns True in case the agent has starved to death. We can then check that this flag is true only iff the initial random sugar level was less then or equal 0. TODO: maybe explain fertility check or borrower check

This might not sound too exciting but this concept has tremendous potential with reaching consequences: it reliefs one from covering a myriad number of edge cases but shifts it towards writing data-generators and the reliance on QuickCheck to find them (which it does, unless the data is too complex). Also the nature of a property-test has more a specification character, shifting the testing nature more towards a declarative nature, where we test what something is or is not instead of a more operational approach in unit-testing were we test a known fixed input against an a priori known fixed output. 

Due to the way Haskell deals with side-effects and separation of data and code in functional programming (which is both strength and weakness in oop / fp respectively), testing is quite straightforward because there are no implicit dependencies, everything is explicit. What is particularly powerful is that one has complete control and insight over the changed state before and after e.g. a function was called on an agent: thus it is very easy to check if the function just tested has changed the agent-state itself or the environment or other data provided to the agent through a Monad: the new environment is returned after running the agent and can be checked for equality of the initial one - if the environments are not the same, one simply lets the test fail. This behaviour is very hard to emulate in OOP because one can not exclude side-effect at compile time, which means that some implicit data-change might slip away unnoticed. In FP we get this for free.

One drawback though is that because the agents monad stack contains the random-number generator we also need to execute the Random Monad runner even if the respective function never makes use of the Random Number functionality - this is simply not possible to detect at compile time. In such a case it is no problem to simply pass a default random number generator always initialised by a fixed seed. This might look more serious than it is, some functions only make use of the agent state, which they declare in their type: the monad they run in is only a state monad with the AgentState as state-type - this makes it easy to run using the state runner and also guarantees at compile time that no other effects can and will happen.