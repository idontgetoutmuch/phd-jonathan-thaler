\section{Case-Study: Pure Functional SugarScape}
TODO

why sugarscape
- original sugarscape sparked ABS and use of OOP, therefore 
- quite complex model, will challenge implementation techniques

\footnote{The code is freely accessible from \url{https://github.com/thalerjonathan/phd/tree/master/public/towards/SugarScape}}

\cite{weaver_replicating_nodate}

page 28, footnote 16: we can guarantee that in haskell at compile time

TODO: investigate where data-parallelisation is possible. concurrency has been dealt with in the STM paper already.

\section{Chapter II}
each agent is a Signal Function with no input and outputs an AgentOut which contains a list of agents it wants to spawn, a flag if the agent is to be removed from the simulation (e.g. starved to death) and observable properties the agent exhibits to the outside world. All the agents properties are encapsulated in the SF continuation and there is no way to access and manipulate the data from outside without running the SF itself which will produce an AgentOut.

An agent has access to the shared environment state, a random-number generator and a shared ABS-system state which contains the next agent-id when birthing a new agent. All this is implemented by sharing the data-structures amongst all agents which can read/write it - this is possible in functional programming using Monadic Programming which can simulate a global state, accessible from within a function which can then read/write this state. The fundamental difference to imperative oop-programming is that all reads / writes are explicit using functions (no assignment).

Updating the agents is straight-forward because in this chapter, the agents interact with each other indirectly through the environment. In each step the agents are shuffled and updated one after another, where agents can see actions of agents updated before. 

Our approach of sharing the environment globally and the agent-state locally works but immediately creates potential problems like: ordering of updates matter - in the end we are implementing a kind of an imperative approach but embedded in a functional language. The benefits are that we have much stronger type-safety and that the access and modification of the states is much more explicit than in imperative approaches - also we dont have mutable references.

We implemented a different approach to iterating: instead of running the agents one after another and interacting through a globally shared environment all agents are now run \textit{conceptually} at the same time and receive the current environment as additional input and have to provide it in the output. This has the following implications: we end up with n copies of the environment where n is the number of agents, agents are not able to see the actions of others until the next step, there can be conflicts where multiple agents end up on the same position. 
Obviously, positional conflicts need to be solved as the sugarscape specification clearly states that only one agent stays on a site at a time. Functional programming makes solving such conflicts easy: we pick a winning agent and rollback the other agents by re-running them with their SF at the beginning of the step - this will undo all changes within the encapsulation. Obviously it would be possible to have conflicts again thus one needs to recursively run the conflict-resolving process until no more conflicts are present.
Although this solution is much slower and more complex to implement and thus not feasible to use in practice but we wanted to explore it for the following reasons:
- it is "closer" to functional programming in spirit because programming with globally mutable state (even if its restricted, explicit and only simulated) should be avoided as far as possible.
- we can exploit data-parallelism (but in this case its not possible anyway because of monadic computations: need mapM which can by definition not be parallel because ordering matters)
- it serves more as a study to what different approaches are possible and how difficult / easy it is to implement them in FP, in this case, "rolling back" the actions of an agent is trivial in FP as long as the underlying monadic context is immune to rollbacks, in our case we argue that it is: incrementing agentids in ABSState does not matter, as it doesnt matter that we have a changed random-number stream. It would be a different matter if there is a global shared state which was modified by the agent.
- in the extreme case this degenerates to a (much more expensive) sequential update 

\section{Chapter III}
This chapter reveals the fundamental difference and difficulty in pure functional programming over established OOP approaches in the field: direct agent-interaction e.g. in mating where 2 agents interact synchronously with each other and might updated their internal state. These interactions \textit{must} happen synchronously because there are resource constraints in place which could be violated if an agent interacts with multiple agents virtually at the same time.

In established OOP approaches this is nearly trivial and straight forward: the agent which initiates the direct interaction holds or looks up (e.g. through a central simulation management object) a reference to the other agent (e.g. by neighbourhood) and then makes direct method calls to the other agent where internal agent-states of both agents may be mutated.
This approach is not possible in pure functional programming because: 1. there are no objects which encapsulate state and behaviour and 2. there are not side-effects possible which would allow such a mutation of local state \footnote{Relaxing our constraint by also allowing \textit{impure} functional features so we can workaround the limitation of not being able to locally mutate state but this is not what we are interested in in this paper because we lose all relevant guarantees which make FP relevant and of benefit.}. 

This makes implementation of direct agent-interactions utterly difficult.
If we build on the approach we used for Chapter II (and which worked very well there!) we quickly run into painful problems:
\begin{itemize}
	\item To mutate local agent state or to generate an output / seeing local properties requires to run the SF. 
	\item Running the SF is intrinsically linked in stepping the simulation forward from t to t+1. Currently the agent has no means to distinguish between different reasons why the SF is being run.
	\item The agents are run after another (after being shuffled) and cannot make invokations of other agents SF during being executed due to pure functional programming.
\end{itemize}

A solution is to change to an event-driven approach: SF now have an input, which indicates an EventType and Agents need some way of initiating a multi-step interaction where a reply can lead to a new event and so on. In case of a simple time-advancement the SF is run with a "TimeStep" event, if an agent requests mating, then it sends "MatingRequest" to the other SF. This requires a completely different approach to iterating the agents.

Stateful programming (or programming that \textit{feels} stateful) comes inherently with difficulties where one can forget to update a state or mutate state where not appropriate. A pure functional approach to that is no exception and shows the same problems. In our case we ran into a bug where the trading agent saw an outdated MRS value of the trading-partner resulting into two different trading-prices which obviously must be prevented under all circumstances because it would destroy / create wealth. The origin of the bug was that MRS depends on the wealth (sugar and spice) of the agent and we simply forgot to update the MRS in the environment from which the offering agent can read it when the trading agents wealth changed (e.g. through harvesting, inheritance,...).

explain continuation, explain monads = replacement of ; operator, runs custom (depending on monad) Code between evaluations

\subsection{Performance}
Haskell is notorious for its space-leaks due to laziness. Even for simple programs one can be hit by a serious space-leak where unevaluated code pieces (thunks) builds up in memory until they are needed, leading to dramatically increased memory usage for a problem which should be solved using a fraction.

It is no surprise that our highly complex sugarscape implementation (TODO: what about our SIR implementation) suffered severeyl by space-leaks. In Simulation this is a severe issue, threatening the value of the whole implementation despite its other benefits: because simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays constant.

Exactly this was violated in our sugarscape implementation where the memory usage increased linearly with about 40MByte per second! 
Haskell allows to add so-called Strict pargmas to code-modules which forces strict evaluation of all data even if it is not used. Carefully adding this conservatively file-by file and checking for changes in memory-leaks reduced the memory consumption considerably and also led to a substantial performance increase. Now only the environment data-structure left leaking. This 

We found that the crucial files / modules were: initialisation, environment data-structure handling, simulation model data-structure, simulation core. What was particularly interesting was that when we added it to our initialisation module where the whole sugarscape model is constructed (agents and environment) it led to a huge improvement of memory-leaks and performance, so it seems to be necessary and quite beneficial to force strictness / evaluation for initialisation for a smooth running simulation.

Init.hs       -> Major
Common.hs	  -> Major
Discrete.hs	  -> Minor
Model.hs	  -> Minor
Simulation.hs -> Minor

After fixing the memory-leaks we get a very low level memory consumption - depending on number of agents is around 3 MB in case of 250 Agents in Animation III-1. What is interesting is that the concurrent implementation consistently uses less memory than the sequential one with the Animation III-1 using up around only 2 MB.

TODO: performance comparison with netlogo implementation
TODO: laziness can save Performance: laziness vs strictness

\subsection{Concurrency}
Although concurrent programming in general is hard, Haskell takes much of the difficulties out through its functional nature and its strong static type system. Because of its referential transparency it is easy to guarantee that no concurrent modification of state will happen (unless running in IO). Also through the type system it is possible to indicate that concurrent computations might or might not happen: also being clear about difference between parallelism and concurrency in types is possible: parallel computations run in parallel and do NOT interfere with each other e.g. through synchronisation or data-dependencies / data-mutation. Concurrent computations run in parallel but might interfere with each other through synchronisation primitives and shared data. Haskell allows to distinguish between these two types of computations in its type-system: a parallel computation is always deterministic and thus pure / referential transparent. Concurrency is indicated using IO or STM.

\subsubsection{Getting it right}
There were a few subtle bugs in my implementation as getting a concurrent implementation right is still hard even when using Haskell. Still Haskells type system and lack of effects helps a lot when reasoning about concurrent behaviour and also the run-time provides amazing help. For example will the program terminate with an exception when a thread blocks on a synchronisation primitive (e.g. MVar) which no other thread references - this is an example for a classic deadlock which cannot be recovered. It is highly beneficial that Haskell actually detects such deadlocks which would be quite difficult to detected without such facilities and in many other languages one would simply end up with infinitely hanging threads.

https://www.fpcomplete.com/blog/2018/05/pinpointing-deadlocks-in-haskell

\subsection{Code}
We used the command line tool \textit{cloc} to count the lines of Haskell code we have written (ignoring comments, reporting only the 'code' values)

Count LoC of NetLogo (4.0.4, as 5.1 seemed to have bugs in some of their functionality): 2128 LoC in a single (!) file (Sugarscape.nlogo)
Count LoC of Java implementation (http://sugarscape.sourceforge.net/): 6525 in 5 files
Count LoC of Python (https://github.com/citizen-erased/sugarscape): 1109 in 9 files

Count LoC of my implementation
- complete project: ~4300 in 38 files
- complete project without test-code ~3660 in 27 files
- test code: ~635 in 11 files
- simulation-core and infrastructure (no rendering): ~1550 in 9 files
- data-export: ~70 in 1 file
- visualisation: ~200 in 2 files
- agent-behaviour only: ~1700 in 14 files

Big difference in our implementation
- lots of lines are type-, import- and export (module) declarations. We conjecture that roughly 40\% of the whole code consists of such declarations.

- several hundred lines are the scenario-definitions
- what we provide in addition (netlogo does not need): simulation kernel, infrastructure, utilities, exporting of data, low-level rendering

\subsection{Testing}
To see how well pure functional programming is suited for code-testing we implemented tests on 4 different levels. Note that we only implemented a few tests on each level to develop an insight in their usefulness and how well FP is suited for each level. We didn't cover the whole functionality because of lack of time. In a proper, high-quality implementation the whole functionality needs to be covered. 

\subsubsection{Testing utility functions}
We implemented a number of tests for utility functions which we implemented as simple unit-tests and a few also as property-based tests (see next section). These utility functions are pure computations like calculating the MRS, exchange rates of a trade, genetic crossover, neighbourhood distance, wrapping coordinates. Due to their nature they are very easy to test because the have no side-effects and don't need any construction of complex simulation state. Often it is not really necessary to test these functions because they are sufficiently short and one can reason about its correctness directly in code - a key feature of functional programming due to its different notion of computation: we can reason equationally about pure computations as they were simple math equations.

TODO: discrete and bestcell

\subsubsection{Testing individual agent functions}
We implemented a number of tests for agent functions which don't cover a whole sub-part of an agents behaviour: checks whether an agent has died of age, check whether an agent has starved to death, the metabolism, immunisation step, check if an agent is a potential borrower, check for fertility, lookout, trading transaction. What all these functions have in common is that they are not pure computations like utility functions but are already running within an agent-context which means they have access to the agent state, environment, simulation context and random-number stream. This makes testing harder because one needs to construct more complex simulation state and needs to run the agent-context with the provided states.

TODO: shortly describe property-based testing
Property-Based works surprisingly well in this context because properties seem to be quite abound here. We simply implement data-generators for our agent state and environment and its cells and then let QuickCheck generate the random data and us running the agent with the provided data, checking for the properties. An example for such a property is that an agent has starved to death in case its sugar (or spice) level has dropped to 0. The corresponding property-test generates a random agent state and also a random sugar level which we set in the agent state. We then run the function which returns True in case the agent has starved to death. We can then check that this flag is true only iff the initial random sugar level was less then or equal 0. TODO: maybe explain fertility check or borrower check

This might not sound too exciting but this concept has tremendous potential with reaching consequences: it reliefs one from covering a myriad number of edge cases but shifts it towards writing data-generators and the reliance on QuickCheck to find them (which it does, unless the data is too complex). Also the nature of a property-test has more a specification character, shifting the testing nature more towards a declarative nature, where we test what something is or is not instead of a more operational approach in unit-testing were we test a known fixed input against an a priori known fixed output. 

TODO: implement
- fertility
- potential lender

Due to the way Haskell deals with side-effects and separation of data and code in functional programming (which is both strength and weakness in oop / fp respectively), testing is quite straightforward because there are no implicit dependencies, everything is explicit. What is particularly powerful is that one has complete control and insight over the changed state before and after e.g. a function was called on an agent: thus it is very easy to check if the function just tested has changed the agent-state itself or the environment or other data provided to the agent through a Monad: the new environment is returned after running the agent and can be checked for equality of the initial one - if the environments are not the same, one simply lets the test fail. This behaviour is very hard to emulate in OOP because one can not exclude side-effect at compile time, which means that some implicit data-change might slip away unnoticed. In FP we get this for free.

One drawback though is that because the agents monad stack contains the random-number generator we also need to execute the Random Monad runner even if the respective function never makes use of the Random Number functionality - this is simply not possible to detect at compile time. In such a case it is no problem to simply pass a default random number generator always initialised by a fixed seed. This might look more serious than it is, some functions only make use of the agent state, which they declare in their type: the monad they run in is only a state monad with the AgentState as state-type - this makes it easy to run using the state runner and also guarantees at compile time that no other effects can and will happen.

\subsubsection{Testing agent behaviour}
These tests have the purpose of testing whole parts of agent behaviour. Due to the very different approach to ABS in FP this is also easier to test because agents have less dependencies between each other as they interact with each other through messages. This decouples them to a very high level. Also the sending of messages happens through passing the messages as output which will then be handled by the simulation kernel. Examples for such tests are all the handlers for incoming mating, trading or lending messages. More difficult to test is the behaviour of the Tick event which is scheduled to each agent in each time-step: depending on the scenario this can have multiple different paths and is quite involved.

TODO: implement lending
´
TODO: can we find the lending with inheritance bug with that?

Note that we are lacking testing of interaction between agents which we had to leave due to lack of time. This should follow a similar approach to testing agent behaviour but we leave this for further research.

\subsubsection{Testing of the whole simulation}
Conceptually, on this level we are testing the model for emergent properties shown and hypotheses expressed in the book. Technically speaking we have implemented that with unit-tests where in general we run the whole simulation with a fixed scenario and test the output for statistical properties which, in some cases is straight forward e.g. in case of Trading the authors of the Sugarscape model explicitly state that the standard deviation is below 0.05 after 1000 ticks.
Obviously one needs to run multiple replications of the same simulation, each with a different random-number generator and perform a statistical test depending on what one is checking: in case of an expected mean one utilises a t-test and in case of standard-deviations a chi-squared test. We discuss some of tests we wrote in the appendix TODO. 

Running multiple replications of the same simulation in parallel is extremely easy in functional programming: because each simulation is independent from each other, it is a case of data-parallelism which means that each can run independently in parallel, without the need to change the types. To run e.g. 100 replications just requires to replace a single function call by a different function which runs them all in parallel. Also the testing library we used (Tasty) supports running tests in parallel out of the box without danger of any side-effects interfering with each other. 
Of course both parallelisms are possible in traditional OOP approaches and if the programmer has done his or her job right there should be no problem but the important message here is that: 1. haskell can guarantee that no interference will occur already at compile time and 2. it does support the parallelisation on a language level without the pain of low level thread management or locks.