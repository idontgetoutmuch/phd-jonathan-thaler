\section{Discussion}
\label{sec:discussion}
Probably the biggest strength is that we can guarantee reproducibility at compile time: given identical initial conditions, repeated runs of the simulation will lead to same outputs. This is of fundamental importance in simulation and addressed in the Sugarscape model: \textit{"... when the sequence of random numbers is specified ex ante the model is deterministic. Stated yet another way, model output is invariant from run to run when all aspects of the model are kept constant including the stream of random numbers."} (page 28, footnote 16) - we can guarantee that in our pure functional approach already \textit{at compile time}.

Refactoring is very convenient and quickly becomes the norm: guided by types (change / refine them) and relying on the compiler to point out problems, results in very effective and quick changes without danger of bugs showing up at run-time. This is not possible in Python because of its lack of compiler and types, and much less effective in Java due to its dynamic type-system which is only remedied through strong IDE support.

Adding data-parallelism is easy and often requires simply swapping out a data-structure or library function against its parallel version. Concurrency, although still hard, is less painful to address and add in a pure functional setting due to immutable data and explicit side-effects. Further, the benefits of implementing concurrent ABS based on Software Transactional Memory (STM) has been shown \cite{thaler_tale_2018} which underlines the strength of Haskell for concurrent ABS due to its strong guarantees about retry-semantics.

Testing in general allows much more control and checking of invariants due to the explicit handling of effects - together with the strong static type system, the testing-code is in full, explicit control over the functionality it tests. Property-based testing in particular is a perfect match to testing ABS due to the random nature in both and because it supports convenient expressing of specifications. Thus we can conclude that in a pure functional setting, testing is very expressive and powerful and supports working towards an implementation which is very likely to be correct.

\subsection{Issues}
Haskell is notorious for its memory-leaks due to lazy evaluation: data is only evaluated when required. Even for simple programs one can be hit hard by a serious space-leak where unevaluated code pieces (thunks) build up in memory until they are needed, leading to dramatically increased memory usage. It is no surprise that our highly complex Sugarscape implementation initially suffered severely from space-leaks, piling up about 40 MByte / second. In simulation this is a big issue, threatening the value of the whole implementation despite its other benefits: because simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays somewhat constant. As a remedy, Haskell allows to add so-called strictness pragmas to code-modules which forces strict evaluation of all data even if it is not used. Carefully adding this conservatively file-by file applying other techniques of forcing evaluation removed most of the memory leaks.

% Another memory leak was caused by selecting the wrong data-structure for our environment, for which we initially used an immutable array. The problem is that in the case of an update the whole array is copied, causing memory leaks AND a performance problem. We replaced it by an IntMap which uses integers as key (mapping 2d coordinates to unique integers is trivial) and is internally implemented as a radix-tree which allows for very fast lookups and inserts because whole sub-trees can be re-used.

The main drawback of our approach is performance, which at the moment does not come close to OO implementations. There are two main reasons for it: first, FP is known for being slower due to higher level of abstractions, which are bought by slower code in general and second, updates are the main bottleneck due to immutable data requiring to copy the whole (or subparts) of a data structure in cases of a change. The first one is easily addressable through the use of data-parallelism and concurrency as we have done in our paper on STM \cite{thaler_tale_2018}. The second reason can be addressed by the use of linear types \cite{bernardy_linear_2017}, which allow to annotate a variable with how often it is used within a function. From this a compiler can derive aggressive optimisations, resulting in imperative-style performance but retaining the declarative nature of the code.