\chapter{Parallel ABS}
\label{ch:parallel_abs}
The future of scientific computing in general and Agent-Based Simulation (ABS) in particular is parallelism: Moore's law is declining as we are reaching the physical limits of CPU clocks. The only option is to go massively parallel due the to availability of cheap parallel local hardware with many cores, or cloud services like Amazon EC. This trend has already been recognised in the field of ABS as a research challenge for \textit{Large-scale ABMS} \cite{macal_everything_2016} was called out and a substantial body of research for parallel ABS shows \cite{abar_agent_2017, cicirelli_efficient_2015,  gasser_mace3j:_2002,  gorur_repast_2016, hay_experiments_2015, himmelspach_plugn_2007, lees_using_2008, logan_distributed_2001, minson_distributing_2008, riley_next_2003, suryanarayanan_synchronised_2013, suryanarayanan_pdes-mas_2013}. 

In this body of work, it has been established that parallelisation of autonomous agents, situated in some spacial, metric environment can be particularly challenging. The reason for this is that the environment constitutes a key medium for the agents' interactions, represented as a \textit{passive} data structure, recording attributes of the environment and the agents \cite{lees_using_2008}. Thus, the problem of parallelising ABS boils down to the problem of how to synchronise access to shared state, without violating the causality principle and resource constraints \cite{logan_distributed_2001, suryanarayanan_pdes-mas_2013}. Various researchers have developed different techniques and most of them are based on the concept of Parallel Discrete-Event Simulation (PDES). The idea behind PDES is to partition the shared space into logical processes, which run at their own speed, processing events coming from themselves and other logical processes. To deal with inconsistencies a conservative approach and an optimistic approach exists. The former one does not allow for processing events with a lower timestamp than the current time of the logical process, the latter one deals with inconsistencies through rolling back changes to state.

Adopting PDES to ABS is challenging, as agents are autonomous, which means that the topology can change in every step. This erratic set of changes make it hard to predict the topology of logical processes in advance \cite{lees_using_2008}, posing a difficult problem for parallelisation in general \cite{cicirelli_efficient_2015}. The work \cite{suryanarayanan_synchronised_2013, suryanarayanan_pdes-mas_2013} discusses this challenge by giving a detailed and in-depth discussion of the internals and implementation of their powerful and highly complex PDES-MAS system. The rather conceptual work \cite{logan_distributed_2001} proposes a general, distributed simulation framework for multi-agent systems and addresses a number of key problems: decomposition of the environment, load balancing, modelling, communication and shared state variables, which the authors mention as the central problem of parallelisation.

In addition, various distributed simulation environments for ABS have been developed and their internals published in research papers: the SPADES system \cite{riley_next_2003} manages agents through UNIX pipes using a parallel sense-think-act cycle employing a conservative PDES approach. Mace3J \cite{gasser_mace3j:_2002} is a Java based system running on single or multi-core workstations. It implements a message passing approach to parallelism. James II \cite{himmelspach_plugn_2007} is also a Java based system and focuses on a PDEVS simulation with a plugin architecture to facilitate the reuse of models. The well known RePast-HPC \cite{gorur_repast_2016, minson_distributing_2008} framework uses a PDES engine under the hood. 

The baseline of this body of research is that parallelisation is possible and we know how to do it. However, the complexity of these parallel and distributed simulation concepts and toolkits is high and the model development effort is difficult \cite{abar_agent_2017}. Furthermore, this sophisticated and powerful machinery is not always required as ABS does not always need to be run in a distributed way. However, the implementers 'simply' want to parallelise their models locally. Although these existing distributed ABS frameworks could be used for this, they are an excess and more straightforward concepts for parallelising ABS would be appropriate. That being said, in this case not very much research exists and implementers either resort to the distributed ABS frameworks, implement their own low-level, complex concurrency plumbing, or simply refrain from using parallelism due to the complexity and accept a longer execution time. What makes it worse is that parallelism always comes with the danger of very subtle bugs, which might lie dormant, potentially invalidating significant scientific results of the model. Therefore, something simpler is needed for local parallelism. Unfortunately, the established imperative languages in the ABS field, Python, Java, C++, don't make adding parallelism easy, due to their inherent use of unrestricted side effects. What is more, they mostly follow a lock-based approach to concurrency which is error prone and does not compose.

\medskip

In the introduction in Chapter \ref{ch:motivation}, this thesis hypothesised that functional programming should allow the easy application of parallel computation to ABS. The subsequent two chapters test this hypothesis by performing a deeper investigation of the potential of parallel programming offered by pure functional programming to apply to ABS. An additional motivation for this undertaking is the notorious performance problems of our time- and event-driven implementations. The work in these chapters can be seen as a direction to mitigate the notorious performance problems of functional programming.

\medskip

It is reasonable to say that pure functional programming as in Haskell is well known and accepted as a remedy against the difficulties and problems of parallel computation \cite{hudak_history_2007}. The reason for it is that immutable data and explicit control of side effects removes a large class of bugs due to data conflicts and data races. A fundamental benefit and strength of Haskell is that it clearly distinguishes between parallelism and concurrency \textit{in its types} \cite{jones_tackling_2002}. It is very important for us to do so in this thesis as well:

\begin{itemize}
	\item \textbf{Parallelism} - in parallelism, code runs in parallel solely for the purpose of doing more work within the same time frame, without interfering with other code through shared data (references, mutexes, semaphores,...). An example is the function \texttt{map :: (a $\rightarrow$ b) $\rightarrow$ [a] $\rightarrow$ [b]}, which maps each element of type \texttt{a} to \texttt{b} using the function \texttt{(a $\rightarrow$ b)}. It is a pure function and thus no sharing of data either through some monadic context or through the function \texttt{(a $\rightarrow$ b)} is possible. This opens the potential to run it in parallel as each function evaluation \texttt{(a $\rightarrow$ b)} could theoretically be executed at the same time, if we had enough CPU cores. Whether it actually runs in parallel or not has no influence on the outcome as it is not subject to any non-deterministic influences. Thus, we identify parallelism with pure and deterministic execution of data transformations in parallel (data parallelism).

	\item \textbf{Concurrency} - concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units \cite{lamport_time_1978}. Those parts \textit{can} be run in parallel, which as a consequence, \textit{might} give rise to asynchronous, non-deterministic events \footnote{The functional \textit{concurrent} programming language Erlang \cite{armstrong_erlang_2010}, which uses the actor model for its concurrency model, was single threaded from its conception in 1986 until around 2008. This fact might be surprising, but it underlines the fact that concurrency per se has nothing to do with parallel execution.}.

	An example is two threads, running in parallel, which share data through a reference. Depending on the scheduling and the code, which is run in each thread, they may give rise to very different access patterns - the events - to the shared data, with the potential for race conditions and dirty reads. In concurrency, per definition ordering is important. The challenge of implementing parallel, concurrent programs is to write the program in a way that, despite these non-deterministic events, the program still works correctly. Thus, we identify concurrency with a parallel, impure, non-deterministic execution of imperative-style and ordered monadic evaluation.
\end{itemize}

In the next two chapters we investigate the application of both parallelism and concurrency to our pure functional ABS approach. In general, we want to see if and how parallel and concurrent programming in Haskell is transferable to pure functional ABS and what the benefits are. In particular, we are interested in speeding up the existing implementations by generally developing techniques that allow us to  \textit{run agents in parallel \footnote{We use the term \textit{parallel} to identify both \textit{parallelism} and \textit{concurrency} and we distinguish between them whenever necessary using their respective terms.}}. 

The focus here is primarily on the conceptual nature of how to apply parallelism and concurrency to pure functional ABS. Therefore, we refrain from doing more in-depth performance analysis, than we did already in the discussion sections \ref{sub:timedriven_performance} and \ref{sub:eventdriven_performance}, as it is beyond the scope of this work. Still, we are very well aware that mindlessly trying to apply parallel computation can actually result in loss of performance as a problem can only be sped up in so far as we can partition it and run those partitions in parallel. Furthermore, parallel computation comes with an overhead. If the partitioning is too fine-grained, this overhead might eat up the speedup, or make it even worse. Thus, in real-world problems, performance measurements have to come first. Then one can investigate where and why the performance is lost. Only if this is understood properly can one then decide whether parallelism or concurrency is applicable, or if neither is applicable because the problem is actually completely sequential. %As D. Knuth famously put it: \textit{"Premature optimisation is the root of all evil"}, thus, when we see adding parallel computation as one way of optimising a problem, we need hard facts instead of wild guesses.

Besides performance improvement, we are generally interested in the implications of the way Haskell deals with parallelism and concurrency in its types. In particular, we ask about the ability of keeping deterministic guarantees about the reproducibility of our simulations. We hypothesise that parallelism will allow us to retain \textit{all} static guarantees about reproducibility \textit{and} gives us noticeable speedup. What is more, we hypothesise that in concurrency we might see a bigger speedup, but sacrifice the guarantee about reproducibility. However, we assume that by using Haskell's unique approach to Software Transactional Memory, we don't lose this guarantee completely, but still retain that the non-deterministic influence is through concurrency only \textit{and nothing else}.

\input{./tex/research/parallelabs/parallelism.tex}

\input{./tex/research/parallelabs/concurrent.tex}