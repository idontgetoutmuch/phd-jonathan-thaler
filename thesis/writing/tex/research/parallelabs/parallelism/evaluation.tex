\section{Evaluation Parallelism}
Evaluation parallelism introduces so called strategies to evaluate lazy data structures in parallel. Example strategies would be to evaluate a list, or tuples in parallel where a spark is created for each element. The fundamental concept Haskell uses to achieve evaluation parallelism is its own non-strictness nature. Non-strictness means that expressions are not eagerly evaluated when defined, like in imperative programming languages but only evaluated when their result is actually needed. This is implemented internally using thunks, which are pointers to expressions. When the value of an expression is needed, this thunk is accessed and the expression is reduced until the next data constructor or lambda expression is encountered. This is called Weak Head Normal Form (WHNF) evaluation because it only reduces the 'head' of the expression, which could consist of sub expressions. This indirection, the separation of data creation from consumption and evaluation, enables evaluation parallelism and Haskell provides two additional functions to support this:

\begin{itemize}
	\item \texttt{par :: a $\rightarrow$ b $\rightarrow$ b} returns the second argument \texttt{b} but evaluates the first argument \texttt{a} in parallel. It is used when the result of evaluating \texttt{a} is required later.
	
	\item \texttt{seq :: a $\rightarrow$ b $\rightarrow$ b} returns the second argument \texttt{b} but is strict in its first argument, which means it forces its evaluation to WHNF. It is used when the result of evaluating \texttt{a} is required now.
\end{itemize}

Internally, evaluation parallelism is handled through so-called \textit{sparks}, which are thunks evaluated in parallel. The Haskell runtime system manages sparks and distributes them to threads where they get executed. Due to their extremely light-weight nature, it is easy to create tens of thousands of sparks. One has to bear in mind that, although evaluating in parallel through sparks is extremely cheap, it still has some overhead. Thus, if the work load of each element in a list might be too low for a spark, then one can split a list into chunks and distribute them onto a single spark.
All this works without side effects and the strategy combinators are all pure functions building on \texttt{par} and \texttt{seq}. This allows us to add parallelism to an algorithm by applying a parallel evaluation strategy to its result which could be a lazy list. This is made possible by the non-strictness nature of Haskell, which separates the construction of data from its consumption.

\subsection{Evaluation Parallelism in ABS}
Using \textit{compositional} parallelism is exactly what we used to aim at adding evaluation parallelism for agent execution in the non-monadic SIR implementation in Chapter \ref{sec:timedriven_firststep}. We know that the whole simulation is a completely pure computation because Yampa is non-monadic. Consequently it is guaranteed that there are no side effects. Moreover, agents are then run conceptually in parallel using \texttt{map}, which should enable us to add parallelism without needing to reimplement \texttt{dpSwitch} (the function running the agents in parallel). %(also re-implementing switch functions would not get us very far because of WHNF evaluation it is the wrong end to start parallel evaluation: probably only the arguments would be evaluated but not the agent behaviour.)

The solution is to add evaluation parallelism in the agent-output collection phase, where the recursive switch into the \texttt{stepSimulation} function happens. It is there where we use an evaluation strategy to evaluate the outputs of all agents in parallel. The agents will then be evaluated in parallel due to compositional parallelism, when we force the output of each in parallel. We provide more details on the topic in a short case study in section \ref{parallel_nonmonadic_sir}.