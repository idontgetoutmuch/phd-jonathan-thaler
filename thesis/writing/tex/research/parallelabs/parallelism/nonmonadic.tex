\subsection{Non-monadic SIR}
\label{parallel_nonmonadic_sir}

\paragraph{Evaluation parallelism}
As outlined above, we want to apply parallelism to agent evaluation by composing the output with parallel evaluation through slightly changing the function \texttt{switchingEvt}. This function receives the output of all agents from the current simulation step and generates an event to recursively switch back into \texttt{stepSimulation} to compute the next simulation step. The code is as follows:

\begin{HaskellCode}
switchingEvt :: SF ((), [SIRState]) (Event [SIRState])
switchingEvt = arr (\ (_, newAs) -> parEvalAgents newAs)
  where
    -- NOTE: need a seq here otherwise would lead to GC'd sparks because
    -- the main thread consumes the output already when aggregating, so using seq 
    -- will force parallel evaluation at that point 
    parEvalAgents :: [SIRState] -> Event [SIRState]
    parEvalAgents newAs = newAs' `seq` Event newAs' 
      where
        -- NOTE: chunks of 200 agents seem to deliver the best performance
        -- when we are purely CPU bound and don't have any IO
        newAs' = withStrategy (parListChunk 200 rseq) newAs
        -- NOTE: alternative is to run every agent in parallel
        -- only use when IO of simulation output is required
        -- newAs' = withStrategy (parList rseq) newAs
\end{HaskellCode}

Which evaluation strategy results in the best performance depends on how we observe the results of the simulation. Due to Haskell's non-strict nature, as long as no output is \textit{observed}, nothing would get computed ever. We have developed four different ways to observe the output of this simulation and thus we measured the timings for all of them:

\begin{enumerate}
	% Parallel : 3.86, 3.77, 3.87, 3.83, 4.13, 3.77, 3.88, 4.15 = 3.9 (0.15 std)
	% Sequential: 16.74, 16.54, 16.69, 16.33, 16.68, 16.42, 16.57, 16.38 = 16.54 (0.15 std)
	% Factor = 4.24
	
	\item Printing the output of the last simulation step. This requires running the simulation for the whole 150 time steps because each step depends on the output of the previous one. Because the simulation is completely CPU bound, the best performance increase turned out to run agents in batches, where for this model 200 seems to deliver the best performance. If each agent is run in parallel, we still achieve a substantial performance increase but not as high as the batched version. An analysis shows that around 1.5 million (!) sparks were created but most of them were never evaluated. There is a limit in the spark pool and we have obviously hit that.
	
	% Parallel: 9.37, 9.18, 9.2, 9.2, 9.3, 9.7, 9.95, 9.44 = 9.4175 (0.27541 std)
	% Sequential: 10.13, 10.42, 10.2, 10.12, 10.0, 10.2, 10.1, 10.2 = 10.171 (0.12135 std)
	% Factor = 1.08
	\item Writing the aggregated output of the whole simulation to an export file. In principle this requires running the simulation until the last time step. Due to non-strictness, the writing to the export file begins straight away. This writing interferes with parallelism due to system calls which get interleaved with parallelism, leading to less of a performance increase than the previous one. It turned out that, in this case, running each agent in parallel didn't lead to reduced performance, because we are IO bound.
	
	% Parallel: 9.24, 9.44, 9.35, 9.61, 10.16, 10.45, 10.25, 9.4 =  9.7375 (0.47286 std)
	% Sequential: 10.07, 10.05, 10.0, 10.03, 10.04, 9.95, 10.06, 10.144 = 10.043 (0.055990 std)
	% Factor = 1.03
	\item Appending the aggregated output of the current step to an export file. This is necessary when we have a very long running simulation for which we want to write each step to the file as soon as it is computed. The function which runs this simulation is tail recursive and can thus run forever, which is not possible in the previous case where the function is not necessarily tail recursive and aggregates the outputs. Here we use a strategy which evaluates each agent in parallel as well.
	
	% Parallel: 5.1, 4.9, 4.88, 4.99, 5.08, 4.89, 4.94, 5.38 = 5.0200 (0.16810 std)
	% Sequential: 20.11, 19.55, 19.53, 19.46, 19.61, 19.46, 19.58, 20.18 = 19.685 (0.28928 std)
	% Factor = 3.92
	\item A combined approach of 1 and 2 where the output of the last simulation step is printed and then the aggregate is written to a file.
\end{enumerate}

The timings are reported in Table \ref{tab:parallel_nonmonadic_sir_timings}. All timings were measured with 1000 agents running for 150 time steps, and $\Delta t = 0.1$. We performed 8 runs and report the average timing in seconds. The parallel version used all 8 available cores. For the sequential implementation the evaluation strategies were removed, to leave no chance that it is purely sequential code. All experiments were carried out on the same machine \footnote{Dell XPS 13 (9370) with Intel Core i7-8550U (8 cores), 16 GB, 512 GByte SDD, plugged in.}

\begin{table}
	\centering
	\begin{tabular}{ c || c | c | c }
		Output type                   & Parallel & Sequential & Factor \\ \hline
		Print of last step (1)        & 3.9      & 16.38      & 4.24 \\ \hline
		Writing simulation output (2) & 9.41     & 10.17      & 1.08 \\ \hline
		Appending current step (3)    & 9.73     & 10.04      & 1.03 \\ \hline
		(1) and (2) combined	          & 5.02     & 19.68      & 3.92 \\ \hline
	\end{tabular}
	
	\caption[Timings of parallel vs. sequential non-monadic SIR]{Timings of parallel vs. sequential non-monadic SIR. Values in the Parallel and Sequential column indicate seconds (lower is better). Values in the Factor column indicate the ratio between the Sequential and Parallel timings (higher is better).}
	\label{tab:parallel_nonmonadic_sir_timings}
\end{table}

The table clearly indicates that because we are purely CPU bound we get quite an impressive speedup of 4.24 on 8 cores. Parallelism clearly pays off here, especially because it is so easy to add. On the other hand, it seems that as soon as we are IO bound, the parallelism performance benefit is completely wasted. This does not come as a surprise and it is well established that generally as soon as IO is involved, performance benefits from parallelism will suffer. This point will be addressed by the use of concurrency, where due to concurrent evaluation, IO is decoupled from the computation making the latter one completely CPU bound and resulting in an impressive speedup in this case as well.

What comes as a bit of a surprise is that, in the case of the sequential implementation, the CPU bound implementation (1), that performs no IO is actually slower than the ones which do perform IO. This result can be attributed to lazy evaluation, which seems to increase performance, because IO can actually be performed while the simulation computes the next step, interleaving the evaluation and IO. Thus, when comparing the parallel CPU bound approach (1) to the IO bound sequential ones (2), and (3) results in a lower speedup factor of roughly 2.6.
The combined approach (4) then shows that we can actually have the substantial speedup of CPU bound (1), but still write the result to the file like as in (2). This is of fundamental importance in simulation, because after all simulations almost always produce large amounts of data that needs to be stored somewhere for later analysis.

\paragraph{Data-flow parallelism} The book \cite{marlow_parallel_2013} mentions that the \texttt{Par} Monad and evaluation strategies roughly result in the same performance in most of the benchmarks. Without going into much detail, we also applied the \texttt{Par} Monad here to run the agents in parallel by evaluating their output. Indeed, in cases (1) and (4) above we reached approximately the same speedup. The IO bound cases (2) and (3) performed slower, where (2) is nearly 50\% slower than its evaluation strategy pendant and (3) is about 25\% slower. It is interesting that running all agents in their own task seems to be fine with data-flow parallelism whereas it was slower with the evaluation strategy in the CPU bound case:

\begin{HaskellCode}
-- NOTE: with the Par monad, splitting the list into chunks seems not 
-- to be necessary - we get the same speedup as in evaluation strategies
parMonadAgents :: [SIRState] -> Event [SIRState]
parMonadAgents newAs = Event (runPar (do
  -- simply return the value of the agent, resulting in a deepseq due to
  -- NFData instance of put in IVar
  ivs <- mapM (spawn . return) newAs
  mapM get ivs))
\end{HaskellCode}

% NOTE: THESE ARE OLD COMMENTS, MADE OBSOLETE BECAUSE I MADE IT ACTUALLY WORK
%Inspired by the work of \cite{perez_60_2014}, which shows the potential of speeding up real-world Haskell programs using Yampa We conducted a comparison of an implementation which makes use of evaluation parallelism to run agents in parallel.
%
%OK, rephrase: compare performance of non-parallel implementation WITH threaded an -N option to non-parallel implementation without threaded and / or N1 to make sure that no performance improvement happens automatically by using threaded e.g. GCs or something else...
%I observed the behaviour in the following code: \url{https://github.com/thalerjonathan/phd/tree/master/public/purefunctionalepidemics/code/SIR_Yampa}
%
%I analysed a bit more using the threadscope tool. I ran the same program twice with different ghc-options:
%1. -O2 -Wall -Werror -eventlog 
%2. -O2 -Wall -Werror -eventlog -threaded -with-rtsopts=-N
%
%When looking at the event logs with threadscope it becomes appartent, that parallel garbage collection is the cause of the CPU usage above 100%:
%-  In the single-threaded case 0 sparks are created and everything runs indeed only on one core. There are two Garbage Collectors (Gen0 and Gen1) but nothing runs in parallel (Par collections are 0 for both).
%- In the multi-threaded case also 0 sparks are created but now 8 cores are used: all 'running' activity happens on only 1 core as expected but garbage collection happens on all 8 cores: the diagrams and the number of Par collections clearly indicates that. The time spent on parallel GC work is 10.76% (0 is completely serial and 100% is completely parallel).
%
%Now when we compare the timing between both runs we see the following: 
%- single-threaded: 11.68s total, 7.35s mutator, 4.34s GC,
%- multi-threaded: 10.70s total, 7.03s mutator, 3.68s GC
%
%This adds up: the ~ 10\% of parallel GC work done in multi-threaded are also the ~ 10\% it is faster over the single-threaded one. Of course I only did a single run in each case but I think the analysis is still valid and the point was made: when running a Haskell program which does not use any parallel features, running it with the -threaded option can lead to an increase in performance due to parallel GC.
%
%% https://www.reddit.com/r/haskell/comments/2jbl78/from_60_frames_per_second_to_500_in_haskell/\\
%
%Our use case: NOTE THIS IS OUTDATED ! I COULD GET IT TO WORK!!
%Unfortunately in our non-monadic Yampa implementation we see a negligible speedup of less than 10\% between running it on 1 or 8 cores and this difference is probably due to garbage collection. When analysing the problem more in-depth it becomes clear that 50\% of the parallel evaluation sparks (todo explain) are duplications and get never evaluated, which is due to the thunk being already evaluated before thus no need to run it actually in parallel. Unfortunately this seems reasonable in this example: the way the agent-behaviour is implemented forces the values, including the output, due to lots of comparisons, which results basically in a strict behaviour with the output already evaluated for many agents. It seems that it depends on the current state the agent is in otherwise we could not explain why some sparks are duplications and others not. Further it seems, that although work happens in parallel, the overhead eats up the benefit and thus we arrive at roughly the same performance of the non-parallel version. This might be completely different for much more computational intensive agent behaviour with a more complex agent-output data-structure - but we leave this for further research.
