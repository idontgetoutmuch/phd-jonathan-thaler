\section{Discussion}
In this chapter we explored how to apply parallelism to our pure functional ABS approach. We ran case studies on our existing models to get a rough estimate of what performance increase we can expect. In general, we aimed at running agents in parallel, employing both the techniques of evaluation and data-flow parallelism. Because of the very sequential nature of the agent behaviour itself, there is much less potential for parallelism \textit{within} an agent, and so the idea was to run them all in parallel. This should create enough workload as an agent is an obvious unit of partitioning which can indeed be run in parallel under certain circumstances.

Although we showed how to apply the techniques, unfortunately, the case studies showed that performance improvement was only possible in the case of the non-monadic SIR as introduced in Chapter \ref{sec:timedriven_firststep}. The speedup stemmed from the fact that the agents ran in parallel as was our original goal, consequently resulting in a significant speedup factor of over 4 on 8 cores. 

Regretfully, all attempts to parallelise the monadic SIR and the Sugarscape implementations failed, which was expected. As soon as we switch to monadic agents, evaluation parallelism is impossible, as agents can't be run in parallel anymore, because side effects require imposing a sequential ordering, which is exactly the idea behind a Monad.

We further showed how to apply parallelism \textit{within} a SIR agent and for updating the environment of the Sugarscape in parallel using the \texttt{Par} Monad. It did not show any speedup either, but this was not the primary objective, as we instead explored a conceptual demonstration of how it can be used. Other models might benefit massively from such an approach as they may contain much more potential for data-flow parallelism.

We did not discuss data parallelism on large array structure or parallelism on GPU as they are used in massively large numerical computation. These techniques achieve tremendous speedups but are not applicable to ABS in general. These speedups can be achieved only in very model specific cases, where for example each agent needs to crunch through arrays of numbers to perform numerical computations. We refer to \cite{marlow_parallel_2013} for a more in-depth discussion of both topics in Haskell and leave the application to pure functional ABS for further research.

In conclusion, we see a direct consequence of the fact that types reflect the semantics of our model. When the agents are pure they can be run in parallel and independently from each other. But, if they are monadic then they are not applicable to parallelism. In the next chapter, we show how to approach this problem and come up with a solution where we can run monadic agents in parallel. This is only possible within a concurrent context, where we utilise Software Transactional Memory (amongst others). Consequently we have to sacrifice determinism in our solution. Still, by favouring Software Transactional Memory using the \texttt{STM} Monad instead of resorting to \texttt{IO}, we get the guarantee that the only source of non-determinism is due to the concurrency of \texttt{STM} \textit{and nothing else}. Furthermore, we show that an additional benefit of using \texttt{STM} over \texttt{IO} is that the \texttt{STM} approach reaches a considerably higher speedup compared to a lock-based approach based on \texttt{IO}. 