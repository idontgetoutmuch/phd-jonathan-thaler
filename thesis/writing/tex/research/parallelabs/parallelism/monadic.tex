\subsection{Monadic SIR}
We now try to apply the same techniques of parallelising the agents as we did in the previous section in the monadic version of the SIR model. There is, however, a fundamental problem in this case, which we have already outlined in the section on data-flow parallelism: we are running the simulation in the monadic context of a \texttt{ReaderT} and \texttt{Rand} Monad stack. In monadic execution, depending on the Monad stack we deal with side effects, which immediately necessitates the ordering of execution. Because, whether an effectful expression is evaluated before another one can have very fundamental differences indeed, and in general we have to assume that it does.
Indeed, the way the agents are evaluated is through the \texttt{mapM} function, which evaluates them sequentially applying their side effects in sequence. It does not matter that the agents behave as if they are run in parallel without the possibility of interfering with each other. The simple fact that the agents are run within the \texttt{ReaderT (Rand g)} Transformer stack requires sequencing. It is not the \texttt{ReaderT} which causes this delicate issue, it is instead the \texttt{Rand} Monad, that basically behaves like a \texttt{State} Monad with the random number generator as its internal state, that gets updated with each draw.
Due to this sequential evaluation, we can hypothesise that our approach is bound to fail from the beginning and that we will not see any speedup  when we apply parallelism. On the contrary, we can expect the performance to be worse, due to the overhead caused by applying parallelism.

Indeed, when we put our hypotheses to the test, using the same experiment setup as in the non-monadic implementation, we see exactly that behaviour. The sequential implementation, which does not use any parallelism and is not compiled with the '-threaded' option takes, on average 41.76 seconds to finish. When adding parallelism with evaluation strategies in the same way as we did in non-monadic SIR, we end up with 49.63 seconds on average to finish, a clear performance \textit{decrease}! For the \texttt{Par} Monad approach, the results are even worse as it averages 52.98 seconds to finish. These timings clearly show that 1) agents which are run in a monadic context with \texttt{mapM} are not applicable to parallelism, 2) the parallelism mechanisms add a substantial overhead, which is in accordance with the reports in \cite{marlow_parallel_2013}.

Still, we do not give up yet and we want to see if continuing to run the agents sequentially while parallelising code \textit{within} them using the \texttt{Par} Monad could gain us some speedup. The function we target is the neighbourhood querying function, which looks up the 8 (Moore) surrounding neighbours of an agent. It is a pure function and uses \texttt{map} and is thus perfectly suitable to parallelism. We simply extend the Transformer stack by adding the \texttt{Par} Monad as the innermost Monad and run the \texttt{neighbours} function within the \texttt{Par} Monad:

\begin{HaskellCode}
-- type simplified for explanatory reasons
neighbours :: Disc2dCoord -> SIREnv -> Par [SIRState]
neighbours (x, y) e = do
    ivs <- mapM (\c -> spawn (return (e ! c))) nCoords
    mapM get ivs
  where
    nCoords = ... -- create neighbours coordinates
\end{HaskellCode}

Unfortunately the performance is even worse averaging 66.68 seconds to finish. The workload seems to be too low for parallelism to pay off. Furthermore, when keeping the \texttt{Par} Monad as the innermost Monad while using the original pure \texttt{neighbours} function without \texttt{Par}, we arrive at an average of 55.9 seconds to finish when running multithreaded on 8 cores and 45.56 seconds when compiled with threading enabled and running on a single core. These measurements demonstrate that using the \texttt{Par} Monad and parallelism in general can lead to substantially \textit{reduced} performance, due to overhead and parallelism that is too fine-grained.

This leaves us without any options for parallelism of the monadic SIR model. Still, we will come back to this use case in the chapter on concurrency, where we will show that by using concurrency it is possible to achieve a substantial speedup even in monadic computations.