\section{Data-Flow Parallelism}
When relying on a lazy data structure to apply parallelism is not an option, evaluation strategies as presented before are not applicable. Furthermore, although lazy evaluation brings compositional parallelism, it makes it hard to reason about performance. Data-flow parallelism offers an alternative over evaluation strategies, where the programmer has to give more details, but gains more control as data dependencies are made explicit and reliance on lazy evaluation is avoided \cite{marlow_monad_2011}.
Data-flow parallelism is implemented through the \texttt{Par} Monad, which provides combinators for expressing data flows. In this Monad it is possible to \texttt{fork} parallel tasks which communicate with each other through shared locations, so called \texttt{IVar}s. Internally these tasks are scheduled by a work-stealing scheduler, which distributes the work evenly on available processors at runtime. \texttt{IVars} behave like futures or promises which are initially empty and can be written once. Reading from an empty \texttt{IVar} will cause the calling task to wait until it is filled. An example is a parallel evaluation of two fibonacci numbers:

\begin{HaskellCode}
runPar :: Par (Integer, Integer)
runPar (do
  i <- new             -- create new IVar
  j <- new             -- create new IVar
  fork (put i (fib n)) -- fork new task compute fib n and put result into IVar i
  fork (put j (fib m)) -- fork new task compute fib m and put result into IVar j
  a <- get i           -- wait for the result from IVar i and collect it
  b <- get j           -- wait for the result from IVar j and collect it
  return (a,b)         -- return the results
\end{HaskellCode}

Note that data-flow parallelism makes it possible to express parallel evaluation of a list or a tuple, as with evaluation strategies. The difference though is, that data-flow parallelism does avoid lazy evaluation. More importantly, putting a value into an \texttt{IVar} requires the type of the value to have an instance of the \texttt{NFData} type class. This simply means that a value of this type can be fully evaluated, not just to WHNF but to evaluate the full expression to the value it represents.

\subsection{Data-Flow Parallelism in ABS}
\label{sec:dataflow_abs}
The \texttt{Par} Monad seems to be a very suitable mechanism to enable agents to express data-flow parallelism within their behaviour. This is only possible with the monadic ABS approach of the time-driven SIR of Chapter \ref{sec:adding_env} and event-driven ABS of Chapter \ref{ch:eventdriven}. An important fact is that if the \texttt{Par} Monad is used, it has to the be one and only Monad because no general \texttt{ParT} transformer exists. Therefore, \textit{in general} it seems that running monadic code in parallel is not possible due to the sequential nature of \textit{bind} and side effects. When thinking of Monads in terms of sequencing of effectful computations with the bind operator, it seems not to be clear what is running in parallel because there is no general structure over which can be parallelised. However, Monads are able to capture the concept of parallel computation and the way to understand this is by looking at a different definition.

%An important fact is that if the \texttt{Par} Monad is used, it has to be the innermost Monad because it cannot be a transformer. This is emphasised by the fact that no \texttt{ParT} transformer instance exists as for other Monads. Making the \texttt{Par} Monad a transformer would have the same semantics as running the monadic \textit{bind} in parallel. It is quite clear that this simply makes no sense, as \textit{bind} is a function for composing and sequencing monadic actions, which generally involves side effects of some kind. Side effects inherently impose some sequencing where evaluation of different sequences has different meanings in general, resulting in the sequential nature of \textit{bind}. It is quite clear that this simply makes no sense, as \textit{bind} is a function for composing and sequencing monadic actions, which generally involves side effects of some kind. Therefore, it seems that running monadic code in parallel is simply not possible in general due to side effects and thus there is no (meaningful) way to put the \texttt{Par} Monad into a Transformer stack. The reason for it is that when thinking of monads in terms of sequencing of effectful computations with the \textit{bind} operator, it would not be clear what we are running in parallel within the \textit{bind} operator as there is nothing to parallelise in general as here is no structure over which we can parallelise in general.

Monads can also be defined in term of \textit{unit} and \textit{join}. This definition is primarily used in Mathematics, but also Haskell provides implementations for them, where \textit{unit} is the already familiar \texttt{return}. \textit{Join} is defined as \texttt{join :: Monad m => m (m a) -> m a} and can be understood as \textit{flattening} two effectful computations or monadic structures into one. The reason why Monads in Haskell are defined in terms of \texttt{return} and \texttt{bind} is that it requires fewer laws to be satisfied, which is more convenient in programming. Also \texttt{bind} appeals more to the sequential thinking of side effects. Howerver, both definitions are equally powerful and can be expressed in terms of each other.

What \textit{flattening} of \texttt{join} means depends on the respective Monad. For example in the \texttt{List} Monad it means flattening the list of lists, allowing for non-deterministic computations:

\begin{HaskellCode}
listMonadExample :: [Int] -> [Int] -> [Int]
listMonadExample xs ys = do
  x <- xs
  y <- ys
  
  let p = x * y

  if odd p
    then []
    else return p
\end{HaskellCode}

The \texttt{<-} operator extracts one element \texttt{x} from the list \texttt{xs} at a time and runs the subsequent computations for each \texttt{x}. The result of the computation is itself a list, therefore resulting in a list of lists, which is flattened using the \texttt{join} implementation of the \texttt{List} Monad. When we run this example we get the following behaviour:

\begin{HaskellCode}
> listMonadExample [1..5] [1..3]
[2,2,4,6,6,4,8,12,10]
\end{HaskellCode}

When looking at the \texttt{List} Monad, it becomes clear that when thinking of Monads in terms of \texttt{join} gives them a rather parallel computation twist. The computation for each element of the list \texttt{xs} could be carried out in parallel and flattened into a common result using \texttt{join}. This interpretation was used by \cite{botta2007relation} for specifying parallelism in bulk synchronous parallel programs.

The consequence with this interpretation is that each effectful action through \texttt{<-} could then be executed in parallel. However, \texttt{join} has different meaning for different Monads and therefore the semantics of such a potentially parallel computation depend on the specific Monad. For the \texttt{List} Monad this is quite clear as explained above. The semantics of a \texttt{Reader} Monad are also clear because the environment is read-only. A \texttt{Writer} Monad is also clear, due to the underlying data being a monoid, therefore the flattening is trivial. However, for the \texttt{State} Monad it strongly depends whether the state can be somehow split up and then be flattened. Therefore we see that providing \textit{general} parallel Monads is indeed not possible, as it strongly depends on the specific Monad instance.

Indeed, there exist variants of the \texttt{Par} Monad \cite{par_extras_hackage}, for example one is combining the \texttt{Par} and \texttt{State} Monad into a parallel \texttt{State} Monad, which supports parallel computation with state. The respective state has to be splittable, which allows to split it on a fork and pass it to the forked computation. Another \texttt{Par} Monad variant supports parallel computations with random number generators, which builds on the parallel \texttt{State} Monad, with the random number generator as splittable state, resulting in a new generator for each forked computation. \texttt{IO} is also possible, as a \texttt{ParIO} Monad shows, however this leads to non-deterministic behaviour due to race conditions.