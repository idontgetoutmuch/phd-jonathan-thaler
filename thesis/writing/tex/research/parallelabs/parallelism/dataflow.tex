\section{Data-Flow Parallelism}
When relying on a lazy data structure to apply parallelism is not an option, evaluation strategies as presented before are not applicable. Furthermore, although lazy evaluation brings compositional parallelism, it makes it hard to reason about performance. Data-flow parallelism offers an alternative over evaluation strategies, where the programmer has to give more details, but gains more control as data dependencies are made explicit and reliance on lazy evaluation is avoided \cite{marlow_monad_2011}.
Data-flow parallelism is implemented through the \texttt{Par} Monad, which provides combinators for expressing data flows. In this Monad it is possible to \texttt{fork} parallel tasks which communicate with each other through shared locations, so called \texttt{IVar}s. Internally these tasks are scheduled by a work-stealing scheduler, which distributes the work evenly on available processors at runtime. \texttt{IVars} behave like futures or promises which are initially empty and can be written once. Reading from an empty \texttt{IVar} will cause the calling task to wait until it is filled. An example is a parallel evaluation of two fibonacci numbers:

\begin{HaskellCode}
runPar :: Par (Integer, Integer)
runPar (do
  i <- new             -- create new IVar
  j <- new             -- create new IVar
  fork (put i (fib n)) -- fork new task compute fib n and put result into IVar i
  fork (put j (fib m)) -- fork new task compute fib m and put result into IVar j
  a <- get i           -- wait for the result from IVar i and collect it
  b <- get j           -- wait for the result from IVar j and collect it
  return (a,b)         -- return the results
\end{HaskellCode}

Note that data-flow parallelism makes it possible to express parallel evaluation of a list or a tuple, as with evaluation strategies. The difference though is, that data-flow parallelism does avoid lazy evaluation. More importantly, putting a value into an \texttt{IVar} requires the type of the value to have an instance of the \texttt{NFData} type class. This simply means that a value of this type can be fully evaluated, not just to WHNF but to evaluate the full expression to the value it represents.

\subsection{Data-Flow Parallelism in ABS}
The \texttt{Par} Monad seems to be a very suitable mechanism to enable agents to express data-flow parallelism within their behaviour. This is only possible with the monadic ABS approach of the time-driven SIR implementation of Chapter \ref{sec:adding_env} and event-driven ABS of Chapter \ref{ch:eventdriven}. An important fact is that if the \texttt{Par} Monad is used, it has to be the innermost Monad because no general \texttt{ParT} transformer exists. Therefore, it seems that running monadic code in parallel is not possible in general due to the sequential nature of \textit{bind} and side effects. It seems that the reason for it is that when thinking of monads in terms of sequencing of effectful computations with the bind operator, it would not be clear what we are running in parallel because there is no general structure over which we can parallelise in general. However, monads are indeed able to capture the concept of parallel computation. The way to understand this is by looking at a different definition.

%An important fact is that if the \texttt{Par} Monad is used, it has to be the innermost Monad because it cannot be a transformer. This is emphasised by the fact that no \texttt{ParT} transformer instance exists as for other Monads. Making the \texttt{Par} Monad a transformer would have the same semantics as running the monadic \textit{bind} in parallel. It is quite clear that this simply makes no sense, as \textit{bind} is a function for composing and sequencing monadic actions, which generally involves side effects of some kind. Side effects inherently impose some sequencing where evaluation of different sequences has different meanings in general, resulting in the sequential nature of \textit{bind}. It is quite clear that this simply makes no sense, as \textit{bind} is a function for composing and sequencing monadic actions, which generally involves side effects of some kind. Therefore, it seems that running monadic code in parallel is simply not possible in general due to side effects and thus there is no (meaningful) way to put the \texttt{Par} Monad into a Transformer stack. The reason for it is that when thinking of monads in terms of sequencing of effectful computations with the \textit{bind} operator, it would not be clear what we are running in parallel within the \textit{bind} operator as there is nothing to parallelise in general as here is no structure over which we can parallelise in general.

Monads can also be defined in term of \textit{unit} and \textit{join}, which is the standard way of defining them in mathematics \footnote{The reason why monads in Haskell are defined in terms of \textit{unit} and \textit{bind} is that in Haskells case fewer laws need to be satisfied, which is more convenient in programming. Also \textit{bind} appeals more to the sequential thinking of side effects. Howerver, both definitions are equally powerful and can be transformed into each other.}. Join removes one level of monadic structure and is defined in Haskell as \texttt{join :: Monad m => m (m a) -> m a}. It can be understood as \textit{flattening} two effectful computations or monadic structures into one. What this flattening means depends on the respective monad. For example for the List Monad it means flattening the list of lists, allowing for non-deterministic computations:

\begin{HaskellCode}
listMonadExample :: [Int] -> [Int] -> [Int]
listMonadExample xs ys = do
  x <- xs
  y <- ys
  
  let p = x * y

  if odd p
    then []
    else return p
\end{HaskellCode}

The \texttt{<-} operator extracts one element \texttt{x} from the list \texttt{xs} at a time and runs the subsequent computations for each \texttt{x}. The result of the computation is itself a list, therefore resulting in a list of lists, which is flattened using the \texttt{join} implementation of the List Monad. When we run this example we get the following, expected behaviour:

\begin{HaskellCode}
> listMonadExample [1..5] [1..3]
[2,2,4,6,6,4,8,12,10]
\end{HaskellCode}

When looking at the List Monad, it becomes clear that when thinking of monads in terms of \textit{join} gives them a rather parallel computation twist. The computation for each element of the list \texttt{xs} could be carried out in parallel and flattened into a common result using \texttt{join}. This interpretation was used by \cite{botta2007relation} for specifying parallelism in bulk synchronous parallel programs.

The consequence with this interpretation is that each effectful action through \texttt{<-} could then be executed in parallel. However, \texttt{join} has different meaning for different monads and therefore the semantics of such a potential parallel computation depend on the specific monad. For the List Monad this is quite clear as explained above. The semantics of a Reader Monad are also clear because the environment is read-only. A Writer Monad is also clear, due to the underlying data being a monoid, therefore the flattening is trivial. However, for the State Monad it strongly depends whether the state can be somehow split up and then be flattened. Therefore we see that providing \textit{general} parallel monads is indeed not possible, as it strongly depends on the specific monad instance.

Indeed, there exist various instances of the \texttt{Par} Monad amongst which are an instance for a parallel State Monad, which supports parallel computation with state, however the state has to be splittable. This allows to split the state on a fork and pass it to the forked computation. Another instance supports parallel computations with random number generators, which builds on the parallel State Monad, with the random number generator as splittable state, resulting in a new generator for each forked computation. IO is also possible, as a \texttt{ParIO} Monad shows, however this leads to non-deterministic behaviour due to race conditions.