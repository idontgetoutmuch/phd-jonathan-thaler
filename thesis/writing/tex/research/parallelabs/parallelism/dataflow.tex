\section{Data-Flow Parallelism}
When relying on a lazy data structure to apply parallelism is not an option, evaluation strategies as presented before are not applicable. Furthermore, although lazy evaluation brings compositional parallelism, it makes it hard to reason about performance. Data-flow parallelism offers an alternative over evaluation strategies, where the programmer has to give more details, but gains more control as data dependencies are made explicit and reliance on lazy evaluation is avoided \cite{marlow_monad_2011}.
Data-flow parallelism is implemented through the \texttt{Par} Monad, which provides combinators for expressing data flows. In this Monad it is possible to \texttt{fork} parallel tasks which communicate with each other through shared locations, so called \texttt{IVar}s. Internally these tasks are scheduled by a work-stealing scheduler, which distributes the work evenly on available processors at runtime. \texttt{IVars} behave like futures or promises which are initially empty and can be written once. Reading from an empty \texttt{IVar} will cause the calling task to wait until it is filled. An example is a parallel evaluation of two fibonacci numbers:

\begin{HaskellCode}
runPar :: Par (Integer, Integer)
runPar (do
  i <- new             -- create new IVar
  j <- new             -- create new IVar
  fork (put i (fib n)) -- fork new task compute fib n and put result into IVar i
  fork (put j (fib m)) -- fork new task compute fib m and put result into IVar j
  a <- get i           -- wait for the result from IVar i and collect it
  b <- get j           -- wait for the result from IVar j and collect it
  return (a,b)         -- return the results
\end{HaskellCode}

Note that data-flow parallelism makes it possible to express parallel evaluation of a list or a tuple, as with evaluation strategies. The difference though is, that data-flow parallelism does avoid lazy evaluation. More importantly, putting a value into an \texttt{IVar} requires the type of the value to have an instance of the \texttt{NFData} type class. This simply means that a value of this type can be fully evaluated, not just to WHNF but to evaluate the full expression to the value it represents.

\subsection{Data-Flow Parallelism in ABS}
The \texttt{Par} Monad seems to be a very suitable mechanism to enable agents to express data-flow parallelism within their behaviour. This is only possible with the monadic ABS approach as in the time-driven SIR implementation of Chapter \ref{sec:adding_env} and event-driven ABS of Chapter \ref{ch:eventdriven} in general. An important fact is that if the \texttt{Par} Monad is used, it has to be the innermost Monad because it cannot be a transformer. This is emphasised by the fact that no \texttt{ParT} transformer instance exists as for other Monads. Making the \texttt{Par} Monad a transformer would have the same semantics as running the monadic \textit{bind} in parallel. It is quite clear that this simply makes no sense, as \textit{bind} is a function for composing and sequencing monadic actions, which generally involves side effects of some kind. Side effects inherently impose some sequencing where evaluation of different sequences has different meanings in general, resulting in the sequential nature of \textit{bind}. Therefore, running monadic code in parallel is simply not possible in general due to side effects \footnote{Additionally, it would not be very clear what we are running in parallel within the \textit{bind} operator as there is nothing to parallelise in general. For example there is no structure over which we can parallelise in general.} and thus there is no (meaningful) way to put the \texttt{Par} Monad into a Transformer stack.