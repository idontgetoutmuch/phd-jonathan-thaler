\section{Parallel runs}
Often, one needs to perform a large number of runs of the same simulation. The most prominent use cases for this are:

\begin{itemize}
	\item Parameter sweeps and variations - to explore the parameter space and the dynamics under varying parameter configurations, the same simulation is run with varying parameters and the results are recorded for statistical analysis.
	
	\item Stochastic replications - due to ABS stochastic nature, running a simulation only once does not allow for generalising or predicting general behaviour as one might have just hit an (un)fortunate special case. To counter this problem, multiple replications of the simulation are run with same initial model parameters, but with different random number streams. All the results are collected and analysed stochastically (averaged, median,...) from which more general properties can then be derived.
\end{itemize}

In each case potentially thousands of runs of the same simulation with different model parameters and varying random number streams are needed, requiring a considerable amount of computing power.

Parallelism is a remedy to this problem, because in each of these cases individual runs do not interfere with each other and thus can be seen as isolated from each other. Our approaches shown in Chapters \ref{ch:timedriven} and \ref{ch:eventdriven} make this very explicit. The top level functions can always be made pure computations because we are ruling out \texttt{IO}. Consequently, even though Monads are employed in many cases, they are still pure. One benefit of our approach is the guarantee at compile time, that individual runs do not interfere with each other and thus there is no danger that parallel runs influence each other. 

Parallelism allows for implementing parameter sweeps and stochastic replications both through evaluation and data-flow parallelism, making the most convincing use case for the use of parallelism in ABS. We hypothesise that data-flow parallelism is better suited for this task because it makes parallelism more explicit. The reason for it is that it is indeed a data-flow problem because we pass parameters to single replications which are run and their results collected. To apply this, we simply run the top level replication logic in the \texttt{Par} Monad, where replications are run in parallel by forking tasks and results are handed back through \texttt{IVars}. If we want the convenience of having a monadic random number generator within the \texttt{Par} Monad, one can use the combined \texttt{ParRand} Monad which provides both.