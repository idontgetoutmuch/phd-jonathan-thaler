\section{Discussion}
In this chapter we have shown at length how to implement event-driven ABS pure functionally. We built on the concepts developed in the previous Chapter \ref{ch:timedriven} on time-driven ABS and substantially extended them by various new concepts, necessary for an event-driven approach. Throughout this chapter it became clear that an event-driven approach relies much more on side-effect than a time-driven one and thus requires the sequential update strategy, as side-effects always impose an ordering of execution.

Transforming a time-driven into an event-driven approach should always be possible because the ability to schedule events with timestamps allows to map all features of time-driven ABS to an event-driven one. The process of turning the time-driven SIR into an event-driven one should give a good direction of how this works. Still for some models one can argue that the time-driven approach is much more expressive than an event-driven one, and we think this is certainly the case for the SIR model. The event-driven approach leads to a quite fragmented logical flow and agent behaviour especially in the case of synchronous agent interactions. Still, we have shown a possible direction of reducing the fragmentation using the \textit{tagless final} approach.

\subsection{A related approach}
The work of \cite{botta_time_2010} tries to solve a similar problem as we do in this chapter. The authors also use Haskell to implement ABS and more specifically look into the use of messages and the problem of when to advance time in models with an arbitrary number of synchronised agent interactions.
The biggest difference is that we approach our agents fundamentally different through the use of Monads and FRP. In our approach an agent is only a single \texttt{MSF} and thus can not be directly queried for its internal state, its id or outgoing messages. Instead of taking a list of messages, our agents take a single event and can produce an arbitrary number of outgoing events together with an observable state. This would allow to query the agent for its id and its state as well by simply sending a corresponding event to the agents MSF and let the agent handle it. Also the state of our agents is \textit{completely} localised and there is no means of accessing the state from outside the agent, they are thus 'fully encapsulated agents' \cite{botta_time_2010}. The authors define their agents with a polymorphic agent-state type \texttt{s}, which implies that without knowledge of the specific type of \texttt{s} there would be no way of accessing the state, rendering it in fact also fully encapsulated.
The problem of advancing time in our approach is conceptually very similar. After sending a \texttt{Tick} message to each agent in random order, we process all agents until they are idle and there are no more enqueued events in the list. The similarities in both approaches might hint at that this seems to be indeed the 'right' way to go. % TODO: can we really say this in a thesis?

\subsection{Layered architecture}
Our approach is designed as a three layered architecture, see Figure \ref{fig:3layer_system}:
\begin{enumerate}
	\item \textit{Pure Functions} are the working horses, which do the actual computations of the simulation. They are mostly used to build up the 2nd layer. Also layer 1 might access them to achieve pure computations when there is no need for effects.
	\item \textit{Monad Transformer Stack} (global and local) does the dirty work of effectful computation: sending messages, mutating the environment, reading model configuration, drawing random numbers, mutating agent state. This layer uses the pure functions to build up its functionality and also propagates between the 1st and 3rd layer.
	\item \textit{MSF} are the backbones of the architecture and define the dynamical structure of the system. This layer builds heavily on the 2nd layer and can also be seen as a high-level delegation mechanism. %Note that MSFs blur the distinctions between the monadic and the arrowized layer.
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=.4\textwidth, angle=0]{./fig/eventdriven/3layers.png}
	\caption{The three layered architecture.}
	\label{fig:3layer_system}
\end{figure}

Separating those three concerns from each other makes the code more robust, easier to refactor and maintain. Further it makes code \textit{much} easier to test as will be shown in Part \ref{ch:property}. 

% maybe refactor sugarscape: write more pure functions which make the distinction between read only and write only of the agent state clear e.g. is agent fertile can be a pure function with state passed explicitly. also always make clear that we can write functions which are monadic but access e.g. only the agent state or the Environment state Without the full monad Transformer stack: try to refactor more in this direction and make thus very clear in the discussion

\subsection{Imperative nature}
Both event-driven use cases make heavy use of the \texttt{State} Monad, thus one might ask what the benefits are of our pure functional approach, after all we seem to fall back into stateful, imperative-style of programming. %We agree that our approach is just one way of implementing ABS in FP but we think we have come a long way thus making our approach quite valuable even if there might be other approaches like shallow EDSLs or freer monads (see Chapter \ref{sec:alternatives}).
The crucial point is that it is highly restricted to very specific types and operations. In the Monad stack we control the operations possible to the respective layers, for example sending events is a write-only operation, accessing the unique agent id and the model configuration is read only. All this is guaranteed at compile time, which makes it much more manageable, maintainable, robust, composable and testable.
To quote John Carmack \footnote{\url{http://www.gamasutra.com/view/news/169296/Indepth_Functional_programming_in_C.php}}: \emph{"A large fraction of the flaws in software development are due to programmers not fully understanding all the possible states their code may execute in."}. We claim that despite using an imperative style, the static guarantees of the types we operate on and the operations provided, make it easier to fully understand the possible states of the simulation code.
This is directly related to the power of polymorphism in Haskell, which goes far beyond the polymorphism of existing object-oriented \footnote{Polymorphism is \textit{not} unique to object-oriented programming.} languages.
We see a particular instance of that in the polymorphism we developed in the concepts behind Sugarscape where we can compose effects depending on the model and we can easily swap out environment and events with very few changes with the benefit that the compiler will inform us about breaking changes. This is directly related to refactoring, which is very convenient and quickly becomes the standard in the development process. Guided by defining types first and relying on the compiler to point out problems, results in very effective and quick changes without danger of bugs showing up at run time. This is not possible in dynamic object-oriented languages like Python because of its lack of a compiler and types, and is also much less effective in Java due to its rather dynamic type system which is only remedied through strong IDE support.

\subsection{Handling IO}
This thesis directly capitalises on the fact that most ABS models are primarily of computational nature, thus CPU bound, not involving IO \textit{inside the agents} while running the simulation. The concurrent approach with Software Transactional Memory in Chapter \ref{ch:concurrent_abs} is an exception but at least we retain the guarantees that the non-determinism within the agent behaviour originates from the concurrency using Software Transactional Memory and nothing else. Even if some IO is required, like rendering the simulation as we did in Sugarscape, due to the loose coupling and compositional qualities of pure functional programming it is straightforward to separate these concerns and keep the impure rendering parts from the pure agent behaviour.

If there arises the use case where agents absolutely need to perform some impure IO within their behaviour then there exist three options. The first one is to let agents construct \texttt{IO} actions and pass them to the simulation kernel for execution, requiring the simulation kernel to run now in \texttt{IO} instead of being pure. This is especially suitable for one-way \texttt{IO} actions, where an agent does not need to synchronously wait for a result. If a synchronous \texttt{IO} action is required with the agent waiting for a result, it could be communicated back from the simulation kernel. This keeps the agent behaviour still pure but with the consequence of indirection and higher complexity. The second option is to use a \textit{tagless final} approach as discussed in Chapter \ref{sec:tagless_final_basics}, where the actions requiring IO are abstracted behind methods of the given type class, for which then an interpreter running in \texttt{IO} has to be written. The benefit is that this allows for a direct synchronous IO behaviour while still restricting the available operations to only the required ones instead of running fully in the \texttt{IO} Monad as would be the third option. In all cases everything becomes possible and all bets are off regarding static guarantees and reproducibility, whereas the tagless final approach provides most control.

%\subsection{Lines of code}
%TODO: report LoC and compare it with other implementations we found on the internet

\subsection{Multiple types of agents}
In the Sugarscape example we have only considered one type of agents, thus the whole population is a homogeneous one in regards of the \textit{type} of the agent. It is quite straightforward to have heterogeneous agent types as well, which is accomplished through adding additional data declarations to the observable output and the agent state. A consequence is that all agent types have to speak the same event language because in regards of types the agents are treated the same way. This is also true for the monadic stack where different agent types cannot have different effect types in this approach as they are seen as the same on the type level.

\subsection{Performance}
The event-driven implementation from this Chapter is around 60 - 70\% faster than the time-driven implementation from Chapter \ref{sec:timedriven_firststep}, which is non-monadic and uses the FRP library Yampa. For the monadic time-driven approach of Chapter \ref{sec:adding_env} the difference is much more dramatic as it is about 700 - 800\% faster. These results dramatically highlight the problem of time-driven ABS and shows that its performance cannot compete with an even-driven approach. This is exaggerated even more so when making use of MSFs as in Chapter \ref{sec:adding_env}. In this case, a time-driven approach becomes extremely expensive in terms of performance and one should consider an event-driven approach. In case the model is specified in a time-driven way, a transformation into an event-driven approach should always be possible as outlined above.

We compared an event-driven SIR implementation we did in Java to the Haskell one here. We run for 150 time steps with 1,000 susceptible and 1 infected agent, $\beta = \frac{1}{5}$, $\gamma = 0.05$, $\delta = 15$. Further, we fixed the random-number generators to guarantee identical dynamics in every run and averaged 8 runs. The Java implementation averages at 1.2 seconds, whereas the Haskell implementation at 6.8 seconds. These performance figures are closer than the ones in the time-driven approach of the previous chapter. This shows that event-driven is indeed much better performing and also more flexible as \cite{meyer_event-driven_2014} has pointed out. Curiously, the \textit{time-driven} Java implementation outperforms the event-driven one. Although, we have improved the performance substantially compared to the time-driven approach, we address it more in depth in the chapters on parallelism \ref{ch:parallelism_ABS} and concurrency \ref{ch:concurrent_abs}.

% EVENT-DRIVEN
% Java: 1262, 1259, 1159, 1202, 1182, 1152, 1238, 1238 Milliseconds
% Haskell: 6.8, 6.6, 6.3, 6.3, 6.5, 6.3, 6.3, 6.8 Seconds

\subsection{Conclusion}
Overall we think that this event-driven approach is quite feasible and is \textit{the way to go} to implement ABS in a pure functional way. The time-driven approach is quite expressive but is not as flexible and general as the event-driven one. Also performance is considerably better in event-driven approach.

We conclude that synchronous agent interaction is the most difficult part to figure out and get right and thus poses the greatest challenge. This concept is indeed cumbersome and clearly more complex than direct method invocation in object-oriented programming. Unfortunately, with the goal of staying pure we do not have much other options. Note, that we didn't aim to encapsulate its complexity behind domain-specific combinators but this is certainly possible and should reduce the difficulty and complexity considerably. This is left as further research and open work which should be untertaken in the future, when putting all the concepts of this thesis into a general purpose library for pure functional ABS in Haskell.

% direct MSF call, Problem is recursive nature. maybe try it with gintis Implementation. I just learned that what i want to achieve is actually: https://en.wikipedia.org/wiki/This_(computer_programming)#Open_recursion AND OPEN RECURSION IS PRETTY BAD