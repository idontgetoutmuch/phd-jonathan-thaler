\chapter{The structure of ABS computation}
\label{ch:structure_abs_computation}

TODO UNFINISHED MANDATORY CHAPTER

The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise. - Dijkstra, EWD340

generalising the structure of agent computation - with our case studies we explore them in a more practical / applied way and in this chapter we extract and distil the general concepts and abstractions behind agent computation: how can ABS, which is pure computation, can be seen structurally? This gives the ABS field for the first time a deeper understanding of the deeper structure of the computations behind agent-based simulation, which has so far always been more ad-hoc without a proper, more rigorous formulation. 

pure functional computation with effects can be seen as computations over some data-structure where the data-structure defines the structure of the computation as well e.g. monoids, applicatives, monads, traversable, foldable

Note that agent-based simulation is almost always entirely pure computation without the need for direct, synchronous user-interaction or impure IO. When IO is really needed we can keep purity by creating IO actions and pass them to the simulation kernel which executes them and communicates the result back if needed - in this case only the simulation kernel needs to run in IO monad but not the agents and the environment computations.

agentout as monoid with writer: solves the Problem of iteratively constructing it the output during an event.

BUT: isnt our approach similar to the early days IO of Haskell with continuations? if this is the case we should be able to get the direct method style by writing an agent monad?

NOTE: "And a closure is just a primitive form of object: the special case of an object with just one method." https://www.tedinski.com/2018/11/20/message-oriented-programming.html

- this is still research which needs to be done by reading the papers below and reflecting  and understanding on co-monads and my implementations in general.

- can we derive an agent-monad?

https://www.javiercasas.com/articles/codata-in-action/

- what about comonads? read essence dataflow paper \cite{uustalu_essence_2006}: monads not capable of stream-based programming and arrows too general therefor comonads, we are using msfs for abs therefore streambased so maybe applicable to our approach/agents=comonads. comonads structure notions of context-dependent computation or streams, which ABS can be seen as of. this paper says that monads are not capable of doing stream functions, maybe this is the reason why i fail in my attempt of defining an ABS in idris because i always tried to implement a monad family. stopped at comonad section, continue from there. understand comonads: https://www.schoolofhaskell.com/user/edwardk/cellular-automata and https://kukuruku.co/post/cellular-automata-using-comonads/ and https://chshersh.github.io/posts/2019-03-25-comonadic-builders

- Conal Elliott has examined a comonadic formulation of functional reactive programming http://conal.net/blog/posts/functional-interactive-behavior

- comonads https://fmapfixreturn.wordpress.com/2008/07/09/comonads-in-everyday-life/

- comonads are objects very important and closely related http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html

- if conal elliott can make a comonadic formulatin of FRP and comonads are objects, then i guess i am very close to a pure functional representation of objects? pure functional objects?


independent of time-driven or event-driven, our agents are MSFs.

in fact i am deriving pure functional objects

- i have the feeling that co-algebras might be an underlying structure, which in CS come up in infinite streams - ABS can be seen as this where the agents are such streams with their output and potentially running for an infinite time, depending on the model. Ionescus thesis might reveal more information / might be an additional source on that.

In general it is easy to see why agents can not be represented by pure functions: they change over time. This is precisely what pure functions cannot do: they can't rely on some surrounding context / or on history - everything what they do is determined by their input arguments and their output. In general we have two ways of approaching this: we either have the agents changing data and behaviour internalised as we did in the previous chapters or we externalise it e.g. in the simulation kernel and provide all necessary information through arguments which was the case in the sugarscape environment.

- FREE MONADS % http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html http://comonad.com/reader/2011/free-monads-for-less/, https://stackoverflow.com/questions/13352205/what-are-free-monads

\section{A Functional View}
Due to the fundamentally different approaches of FP, an ABS needs to be implemented fundamentally differently, compared to established OOP approaches. We face the following challenges:

\begin{enumerate}
	\item How can we represent an Agent, its local state and its interface?
	\item How can we implement direct agent-to-agent interactions?
	\item How can we implement an environment and agent-to-environment interactions? 
\end{enumerate}

\subsection{Agent representation}
The fundamental building blocks to solve these problems are \textit{recursion} and \textit{continuations}. In recursion a function is defined in terms of itself: in the process of computing the output it \textit{might} call itself with changed input data. Continuations are functions which allow to encapsulate the execution state of a program by capturing local variables (known as closure) and pick up computation from that point later on by returning a new function. As an illustratory example, we implement a continuation in Haskell which sums up integers and stores the sum locally as well as returning it as return value for the current step:

\begin{HaskellCode}
-- define the type of the continuation: it takes an arbitrary type a 
-- and returns a type a with a new continuation
newtype Cont a = Cont (a -> (a, Cont a))

-- an instance of a continuation with type a fixed to Int
-- takes an initial value x and sums up the values passed to it
-- note that it returns adder with the new sum recursively as 
-- the new continuation
adder :: Int -> Cont Int
adder x = Cont (\x' -> (x + x', adder (x + x')))

-- this function runs the given continuation for a given number of steps
-- and always passes 1 as input and prints the continuations output
runCont :: Int -> Cont Int -> IO ()
runCont 0 _ = return () -- finished
runCont n (Cont cont) = do -- pattern match to extract the function
  -- run the continuation with 1 as input, cont' is the new continuation
  let (x, cont') = cont 1
  print x
  -- recursive call, run next step
  runCont (n-1) cont'

-- main entry point of a Haskell program
-- run the continuation adder with initial value of 0 for 100 steps 
main :: IO ()
main = runCont 100 (adder 0)
\end{HaskellCode}

We implement an agent as a continuation: this lets us encapsulate arbitrary complex agent-state which is only visible and accessible from within the continuation - the agent has exclusive access to it. Further, with a continuation it becomes possible to switch behaviour dynamically e.g. switching from one mode of behaviour to another like in a state-machine, simply by returning new functions which encapsulate the new behaviour. If no change in behaviour should occur, the continuation simply recursively returns itself with the new state captured as seen in the example above.

The fact that we design an agent as a function, raises the question of the interface of it: what are the inputs and the output? Note that the type of the function has to stay the same (type \textit{a} in the example above) although we might switch into different continuations - our interface needs to capture all possible cases of behaviour. The way we define the interface is strongly determined by the direct agent-agent interaction. In case of Sugarscape, agents need to be able to conduct two types of direct agent-agent interaction: 1. one-directional, where agent A sends a message to agent B without requiring agent B to synchronously reply to that message e.g. repaying a loan or inheriting money to children; 2. bi-directional, where two agents negotiate over multiple steps e.g. accepting a trade, mating or lending. Thus it seems reasonable to define as input type an enumeration (algebraic data-type in Haskell, see example below) which defines all possible incoming messages the agent can handle. The agents continuation is then called every time the agent receives a message and can process it, update its local state and might change its behaviour.

As output we define a data-structure which allows the agent to communicate to the simulation kernel 1. whether it wants to be removed from the system, 2. a list of new agents it wants to spawn, 3. a list of messages the agent wants to send to other agents. Further because the agents data is completely local, it also returns a data-structure which holds all \textit{observable} information the agent wants to share with the outside world. Together with the continuation this guarantees that the agent is in full control over its local state, no one can mutate or access from outside. This also implies that information can only get out of the agent by actually running its continuation. It also means that the output type of the function has to cover all possible input cases - it cannot change or depend on the input. 

\begin{HaskellCode}
type AgentId    = Int
data Message    = Tick Int | MatingRequest AgentGender ... 
data AgentState = AgentState { agentAge :: Int, ... }             
data Observable = Observable { agentAgeObs :: Int, ... } 
data AgentOut   = AgentOut
  { kill       :: Bool
  , observable :: Observable
  , messages   :: [(AgentId, Message)] -- list of messages with receiver
  }
-- agent continuation has different types for input and output
newtype AgentCont inp out = AgentCont (in -> (out, AgentCont inp out))
-- taking the initial AgentState as input and returns the continuation
sugarscapeAgent :: AgentState -> AgentCont (AgentId, Message) AgentOut
sugarscapeAgent asInit = AgentCont (\ (sender, msg) -> 
  case msg of
    agentCont (sender, Tick t) = ... handle tick
    agentCont (sender, MatingRequest otherGender) = ... handle mating request)
\end{HaskellCode}

\subsection{Stepping the simulation}
The simulation kernel keeps track of the existing agents and the message-queue and processes the queue one element at a time. The new messages of an agent are inserted \textit{at the front} of the queue, ensuring that synchronous bi-directional messages are possible without violating resources constraints. The Sugarscape model specifies that in each tick all agents run in random order, thus to start the agent-behaviour in a new time-step, the core inserts a \textit{Tick} message to each agent in random order which then results in them being executed and emitting new messages. The current time-step has finished when all messages in the queue have been processed. See algorithm \ref{alg:stepping_simulation} for the pseudo-code for the simulation stepping.
%
%\begin{algorithm}
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%\Input{All agents \textit{as}}
%\Input{List of agent observables}
%shuffle all agents as\;
%messageQueue = schedule Tick to all agents\;
%agentObservables = empty List\;
%\While{messageQueue not empty} {
%  msg = pop message from messageQueue\;
%  a = lookup receiving agent in as\;
%  (out, a') = runAgent a msg\;
%  update agent with continuation a' in as\;
%  add agent observable from out to agentObservables\;
%  add messages of agent at front of messageQueue\;
%}
%return agentObservables\;
%\caption{Stepping the simulation.}
%\end{algorithm}
%\label{alg:stepping_simulation}

\subsection{Environment and agent-environment interaction}
The agents in the Sugarscape are located in a discrete 2d environment where they move around and harvest resources, which means the need to read and write data of environment. This is conveniently implemented by adding a State side-effect type to the agent continuation function. Further we also add a Random effect type because dynamics in most ABS in general and Sugarscapes in particular are driven by random number streams, so our agent needs to have access to one as well. All of this low level continuation plumbing exists already as a high quality library called Dunai, based on research on Functional Reactive Programming  \cite{hudak_arrows_2003} and Monadic Stream Functions \cite{perez_functional_2016,perez_extensible_2017}.

\section{Discussion}
The central question which needed to be answered first was \textit{how} ABS could be done pure functionally, as there didn't exist any research which offered a systematic solution to that problem. More specifically, it was unclear how to represent agents, how to express agent identity, local agents state, changing behaviour and interactions amongst agents and the environment. After all, this is straightforward in object-oriented programming due to method calls and mutable shared state encapsulated in objects.

% the answer
The solution was to use arrowized FRP both in the pure implementation of Yampa and the monadic version as in the library Dunai. Building on top of them allowed us to implement pro-activity of agents, encapsulation of local agent state, an environment as shared mutable state and synchronous agent-interactions based on an event-driven approach. The central concept behind these approaches are Signal Functions (SF), generalised in Dunai to Monadic Stream Functions (MSF), which are implemented using closures and continuations, fundamental building blocks and concepts of pure functional programming. 

% WARNING: don't fall again into the pitfall of opinionated OO criticism, this can burn me as it is too opinionated. simply refer to what we cannot do properly with java objects

% 1. SFs/MSFs can be seen as simple objects
SF and MSF can be seen as very simple \textit{immutable} objects with a single method following a \textit{shared nothing} semantics as discussed in Chapter \ref{ch:structure_abs_computation} already at length. This interpretation and the fact that we seem to achieve encapsulation of local agent state and interactions obviously raises the question if agents actually \textit{do} map naturally to objects - after all, despite being in a pure functional setting, we are talking about objects again!

% 2. it seems that agents map to objects, but what are objects?
It seems that we indeed have to agree that agents do actually map naturally to objects. However, throughout the course of this thesis it should have become clear that we have to think objects in a much broader context than the one of existing object-oriented terminology as in the popular family of Java, C++ and Python. The fact that we can represent agents as objects also in a purely functional way, leads us to the question, what actually constitutes objects and we have to be careful not to confuse the \textit{concept} of objects with their \textit{implementation} within a language.

\medskip

% 3. what are objects
There does not exist a commonly agreed upon definition of objects and object-oriented programming but rather a bunch of ideas and concepts \footnote{\url{http://wiki.c2.com/?DefinitionsForOo}}. It is agreed that the original ideas of objects and object-oriented programming were conceived by Kristen Nygaard, the inventor of Simula 67, the first object-oriented language \cite{dahl_birth_2002} and Alan Kay, the inventor of SmallTalk, another pioneering object-oriented language in the early 70s \cite{kay_early_1993}. %Their ideas about OO where the following: \footnote{\url{http://wiki.c2.com/?AlanKaysDefinitionOfObjectOriented} and \url{http://wiki.c2.com/?NygaardClassification}}:

Kristen Nygaard identified object-oriented programming by \textit{"A program execution is regarded as a physical model, simulating the behaviour of either a real or imaginary part of the world."}, thus he puts the focus on the modelling aspect of the problem. Alan Kay claims to have coined the term \textit{object-oriented} and defines it in more technical terms: everything is an object; every object is an instance of a class; the class holds the shared behaviour for its instances; objects communicate by sending and receiving messages. Alan Kay puts a strong emphasis on sending and receiving messages, with a shared-nothing interpretation. This becomes especially clear in a quote attributed to him: \textit{"The big idea is "messaging" ... "I invented the term Object-Oriented and I can tell you I did not have C++ in mind."}.

\medskip

%It is a fact that simulations are about consuming, processing and producing data. ABS being simulation methodology is no exception to that fact. Unfortunately, due to OO lack of rigour theoretical foundations, OO as it is used today is \textit{not} very good at representing and manipulating pure data and its data-flow because of two things: \textit{mutable shared state} and explicitly associate data-types and functions(methods)/code/behaviour.

%FROM https://www.youtube.com/watch?v=QM1iUe6IofM&feature=youtu.be
%Inheritance is not relevant any more: it has come to a widely agreement amongst OO developers that inheritance should be avoided: https://www.javaworld.com/article/2073649/why-extends-is-evil.html . Note that we are speaking about subclassing not implementing an interface, which is something entirely different
%Polymorphism: is not unique to OO and exists in non-OO languages as well and plays a central role in Haskell (and ML languages). Further it is possible to implement polymorphic code in C
%Encapsulation: this is seen as the major strength of OO but unfortunately it does not work at a fine grained level of code in todays OO. The original idea was indeed great and it is no coincidence that my implementation ended up with a variation of that as well as Erlang: encapsulate state behind a public interface and interact with it through messages (TODO: fill in alan kay). The very central point of messages though was that they followed "shared nothing" semantics, meaning that no references or pointers could be contained in that message as this would immediately result in a violation of the public interface and ultimately breaks encapsulation. 
%OO dominates the industry since around mid 90s. There are varying opinions on that but a major influence to popularise OO was Java, which made its first appearance in 1996. Java was a much easier approach to OO than existing ones e.g. in C++ and VB: it abandoned multiple inheritance, introduced interfaces, was cross-platform, provided high quality libraries including a GUI framework (GUI programming was the way to go in the 90s until it got abandoned in 00s with the emergence of Web 2.0), C/C++ syntax made it easy to pick up, avoided header-files, abandoned pointers and memory management and added garbage collection which made applications a lot safer.

% TODO: need to discuss the problem of shared state. state per se is not necessarily a problem and ever program has state in some form. how explicit it is represented is often used as classification between different kind of paradigms e.g. it has been said that functional programming is stateless but that is obviously not true, state is all over the place but it is very very contained, well behaved and explicit. with shared mutable state this is not the case anymore and we get into the troube of data-dependencies and orderings. this is exactly what we encountered when having introduced a global environment in Sugarscape: although our state is referential transparent and pure functional, they way we used it is globally and we run in ordering issues.

% TODO: isnt shared state also a problem in erlang? after all we can send Pids around and interact with those processes as soon as another process has access to a Pid. In which way is it different to reference passing in OO? There seems to be no difference... so maybe the anti OO argument is not that strong after all and my argument is simply weak or wrong? 

%TODO: i REALLY need to find proper literature / research / evidence which shows the problematic nature of modern OO: mutable shared state which is tied to code. Inheritance and open recursion gives the rest. the problem is that deeply linking \textit{shared mutable} state to its code is the path to failure: abstraction breaks, concurrency and parallelism becomes hard and breaks abstraction, data-driven programming becomes difficult (although that got addressed by adding functional features). NOTE: my approach and erlang have state and behaviour as well but in our case the state is shared nothing and immutable (yes in Haskell we update the agents state but that happens ultimately through closures and continuation in a referential transparent way and still no state is shared between agents. the environment is an exception to some extent as agents can access it globally: this works but requires a specific ordering either through sequential access or STM. this is no different than in an erlang implementation of sugarscape: there needs to be some arbitration of concurrent access). TODO: isnt there some fundamental research on that issue out there?
% TODO: maybe these act as a starting point?
% https://www.yegor256.com/2016/08/15/what-is-wrong-object-oriented-programming.html
% https://dl.acm.org/citation.cfm?id=1806847
% https://web.cs.ucdavis.edu/~filkov/papers/lang_github.pdf "Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing" "We also find that functional languages are somewhat better than procedural languages" but modest effects
% https://www.javaworld.com/article/2073649/why-extends-is-evil.html
% READ extension problem paper
% READ Ted Kaminskis thesis

%This was by no means clear in the early-to-mid 1990s where the OO paradigm was seen as a silver bullet to the problems of programming: a whole software industry had to re-learn best practices, patterns \cite{gamma_design_1994} and how to avoid pitfalls and bad code \cite{fowler_refactoring:_2012}. Thus we cannot blame \cite{epstein_growing_1996} for advertising OO as the ways to implement ABS, at that time it seemed indeed to be the right thing to do. 

%The combination of both was exactly the sales pitch of OO for the last 20+ years. Unfortunately this combination leads to nasty bugs due to shared mutable state, deeply complex object hierarchies due to inheritance overuse which also fix behaviour at compile time, open recursion which in the end costs the potential for higher degree of correctness, ease of parallelism and concurrency and the use of property-based testing. Thus we need to separate both: what we need is immutable, shared-nothing state allowing for a data-centric approach \textit{and} an interaction mechanism which allows agents to communicate with each other.

% 4. implementation of objects: the problems: data-driven programming is difficult, not really encapsulating and shared mutable state makes concurrency and testing a lot harder. this sound as a contradition but it has been shown that despite objects claim they compose and enforce encapsulation, they do not.
% https://dl.acm.org/citation.cfm?doid=242224.242415

So we see that the original \textit{concepts} of objects and object-oriented programming vary considerably from how objects and object-oriented programming is \textit{implemented} today in the family of popular object-oriented programming languages like Java, C++ and Python. The most substantial different to the original definition of Kay is that messages are not pure data - they do not follow a shared nothing semantics. This leads to the failure of objects to compose behaviour and encapsulate data properly \cite{bill_what_2017}, \cite{erkki_lindpere_why_2013}. Ironically, this has always been the main argument for advertising the use of object-oriented programming. The reason for this is that objects hide both \textit{mutation} and \textit{sharing through pointers or references} of object-internal data. Further, they expose multiple methods on how to operate on this encapsulated data. This makes data-flow mostly implicit due to the side-effects on the mutable data which is globally scattered across objects. To deal with the problem of composability and implicit data-flow the seminal work \cite{gamma_design_1994} put forward the use of \textit{patterns} to organize objects and their interaction. Other concepts, trying to address the problems, were the SOLID principles and Dependency Injection. 

% 4a this leads to an inherent difficulty to follow data-flow in an OO program and also makes it very difficult in concurrent settings as semantics of synchronisation "bleed" out of the object, breaking encapsulation. 
Despite these advances in understanding the object-oriented programming paradigm and how to use it properly, the increased complexity leads to an inherent difficulty to express and follow data-flow in an object-oriented program and exploit parallelism and concurrency due to mutable shared state. Even worse, concurrency breaks encapsulation of objects as well and prevents composing them. 

The rise of functional concepts in object-oriented languages in the last years are a strong indication that object-oriented programming is lacking features which have existed in FP for decades. Java 8 added lambda expressions and functional style programming using \textit{map, fold, reduce, filter} which together with lambdas allow a data-flow oriented approach to computing. Python, which surges in popularity within the object-oriented family of languages, allows very data-flow centric and functional style of programming through lambda functions, list comprehensions and other functional features as it does not require programmers to stick to the object-oriented programming paradigm. Popularisation of JavaScript frameworks like React, Elm and Purescript, which emphasise a functional, data-flow driven approach of web-programming are another indicator. Thus it seems that functional concepts overcome the weakness of object-oriented programming to model explicit, immutable data-flows which can be exploited towards easier parallelisation and concurrency.

% 5. our approach is one of very simple, pure functional, immutable objects and we have shown that they indeed allow us to apply concurrency and property-based testing
All these properties of explicit data-flow and applicability of parallelism and concurrency are highly desirable when implementing simulations: it is a fact that simulations are data-centric, they are all about about consuming, processing and producing data and they have to do it fast and correct. ABS, being a simulation methodology, is no exception to that fact.

The question is then why not use toolkits like Matlab or R - after all they are completely data-centric? This would be the other extreme, just like object-oriented programming is and we would run into difficulties as well. The point is that ABS is not purely data-centric either and is indeed richer: agents can interact with each other and with an environment. So we have a tension here: ABS is data-centric on the one hand, and interaction-centric on the other - can we combine both worlds? Our approach is \textit{one} answer to do that in a pure, strong statically typed language - Haskell. It can be seen as an object-centric approach, which \textit{implements} a very simple \textit{concept} of shared-nothing, immutable, pure functional objects.

%TODO: this tension between data and objects has its origins in the expression problem (TODO: cite the paper): https://www.tedinski.com/2018/03/06/more-on-the-expression-problem.html we want to have a general approach and thus abstraction:
%in the sugarscape implementation we used a tagless final approach to effects, which is extensible in two dimensions: vertically, we can add new interpreters; horizontally, we can add new effectful operations.  
%TODO: properly study
%- https://www.tedinski.com/2018/03/06/more-on-the-expression-problem.html
%- https://serokell.io/blog/2018/12/07/tagless-final
%- https://jproyo.github.io/posts/2019-03-17-tagless-final-haskell.html
%- The Expression Problem Revisited - Four new solutions using generics by Mads Torgersen