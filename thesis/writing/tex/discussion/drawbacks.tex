\chapter{Drawbacks}
\label{ch:drawbacks}

\section{Space-Leaks}
discuss the problem (and potential) of lazy evaluation for ABS: can under some circumstances really increase performance when some stuff is not evaluated (see STM study) but mostly it causes problems by piling up unevaluated thunks leading to crazy memory usage which is a crucial problem in simulation. Using strict pragmas, annotations and data-structures solves the problem but is not trivial and involves carefully studying the code / getting it right from the beginning / and using the haskell profiling tools (which are fucking great at least). TODO: show the stats of memory usage

Haskell is notorious for its memory-leaks due to lazy evaluation: data is only evaluated when required. Even for simple programs one can be hit hard by a serious space-leak where unevaluated code pieces (thunks) build up in memory until they are needed, leading to dramatically increased memory usage. It is no surprise that our highly complex Sugarscape implementation initially suffered severely from space-leaks, piling up about 40 MByte / second. In simulation this is a big issue, threatening the value of the whole implementation despite its other benefits: because simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays somewhat constant. As a remedy, Haskell allows to add so-called strictness pragmas to code-modules which forces strict evaluation of all data even if it is not used. Carefully adding this conservatively file-by file applying other techniques of forcing evaluation removed most of the memory leaks.

% Another memory leak was caused by selecting the wrong data-structure for our environment, for which we initially used an immutable array. The problem is that in the case of an update the whole array is copied, causing memory leaks AND a performance problem. We replaced it by an IntMap which uses integers as key (mapping 2d coordinates to unique integers is trivial) and is internally implemented as a radix-tree which allows for very fast lookups and inserts because whole sub-trees can be re-used.


\section{Efficiency}
ordering of the transformers
when to run a transformer,
lazyness vs. strictness

The main drawback of our approach is performance, which at the moment does not come close to OO implementations. There are two main reasons for it: first, FP is known for being slower due to higher level of abstractions, which are bought by slower code in general and second, updates are the main bottleneck due to immutable data requiring to copy the whole (or subparts) of a data structure in cases of a change. The first one is easily addressable through the use of data-parallelism and concurrency as we have done in our paper on STM \cite{thaler_tale_2018}. The second reason can be addressed by the use of linear types \cite{bernardy_linear_2017}, which allow to annotate a variable with how often it is used within a function. From this a compiler can derive aggressive optimisations, resulting in imperative-style performance but retaining the declarative nature of the code.

\section{Productivity and learning curve}
A case study in \cite{hanenberg_experiment_2010} hints that simply by switching to a static typesystem alone does not gain anything and can even be detriment. It also needs to have a certain level of abstractions like Haskell type system does or even dependent types as in Idris. This also mean that there is a substantial learning curve to master when one wants to enter pure functional ABS.
