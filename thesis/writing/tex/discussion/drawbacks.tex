\section{Drawbacks}
\label{sec:drawbacks}
Obviously nothing comes without drawbacks, and also our approach suffers from a few. We discuss them here and propose solutions where applicable.

\subsection{Efficiency}
\label{sec:drawback_efficiency}
Currently, the performance of our approaches does not come close to imperative implementations, as shown in the discussions of the respective chapters. There are two main reasons for it: first, functional programming is known for being slower due to higher level of abstractions, which are bought by slower code in general and second, updates are the main bottleneck due to immutable data requiring to copy the whole (or subparts) of a data structure in cases of a change. The first one is easily addressable through the use of data parallelism and concurrency as shown in Chapter \ref{ch:parallelism_ABS} and \ref{ch:concurrent_abs}. The second reason can be addressed by the use of linear types \cite{bernardy_linear_2017}, which allow to annotate a variable with how often it is used within a function. From this a compiler could derive aggressive optimisations, potentially resulting in imperative-style performance but retaining the declarative nature of the code - we leave this for further research. Also the use of Monad Transformer stacks has performance implications, which can be quite subtle but can be alleviated by careful usage and re-ordering of lifts which might result in increased performance e.g. instead of \textit{mapM (lift ...)} use \textit{lift (mapM ...)}.

However, it was shown by various people \cite{kqr_competing_2017, stewart_haskell_2008, stolarek_haskell_2013} that Haskell does not necessarily have to be slow and that it is indeed possible to reach C speed in Haskell. The direction to do this is using the worker/wrapper transformation \cite{gill_worker/wrapper_2009}, clever combination of techniques with strict \textit{foldl'} and data declaration with strictness annotation instead of lazy tuples and Stream Fusion \cite{coutts_stream_2007, mainland_haskell_2013}. The problem is of course that to apply these techniques one needs to have deep knowledge of Haskell and its subtle details of lazy evaluation, making this a highly non-trivial task. A problem is that those techniques seem only applicable in the context of a tight loop which crunches numbers of a list, thus it is not directly applicable in our case as we are clearly bound by the effectful computations: MSF and Monad Transformers are the limiting factor, not inner loops.

Concluding we can say that the current performance makes our approach not very attractive for real-world use \textit{at the moment}. Also the fact that the sequential object-oriented implementation seems to outperform the concurrent and parallel implementations as well, seems to question our motivation as to why we are using pure functional programming and parallel computation at all. Still, the bad performance results do not invalidate our research as this thesis aim is not the development of high-performance pure functional implementations but rather exploring concepts of ABS in pure functional programming. Thus, this work is seen as a first step which needs to be developed further into something to be used in the real world. We hypothesise that our pure functional approach will be able to reach the Java performance but we think that it should be possible to come considerably closer, making it more applicable for real-world usage. We leave a deeper investigation of this problem for further research.

\subsection{Space leaks}
Haskell is notorious for its memory leaks due to lazy evaluation: data is only evaluated when required. Even for simple programs one can be hit hard by a serious space leak where unevaluated code pieces (thunks) build up in memory until they are needed, leading to dramatically increased memory usage. It is no surprise that our highly complex Sugarscape implementation initially suffered severely from space leaks, piling up about 40 MByte / second. In simulation this is a big issue, threatening the value of the whole implementation despite its other benefits. Due to the fact that simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays within reasonable bounds. As a remedy, Haskell allows to add so-called strictness pragmas to code modules, which force strict evaluation of all data even if it is not used. %Carefully adding this conservatively, file-by-file applying other techniques of forcing evaluation removed most of the memory leaks.

Another memory leak was caused by selecting the wrong data structure for our environment, for which we initially used an immutable array. The problem is that in the case of an update the whole array is copied, causing memory leaks \textit{and} a performance problem. We replaced it by an \textit{IntMap} which uses integers as key and is internally implemented as a radix-tree which allows for very fast lookups and inserts because whole sub-trees can be re-used.

%discuss the problem (and potential) of lazy evaluation for ABS: can under some circumstances really increase performance when some stuff is not evaluated (see STM study) but mostly it causes problems by piling up unevaluated thunks leading to crazy memory usage which is a crucial problem in simulation. Using strict pragmas, annotations and data-structures solves the problem but is not trivial and involves carefully studying the code / getting it right from the beginning / and using the haskell profiling tools (which are fucking great at least). TODO: show the stats of memory usage

\subsection{Productivity and learning curve}
A case study in \cite{hanenberg_experiment_2010} hints that simply by switching to a static type system alone does not gain anything and can even be detrimental. To be useful, it needs to have a certain level of abstractions like Haskells' type system has. Although such case studies have to be taken with care, there is also some truth in it: working in a statically strong type system prevents the developer from moving quickly and making quick and dirty changes. This can be both a benefit and a drawback: in general it prevents from breaking changes which show up at compile time but at the same time the whole program is much more rigid and a proper structure needs to be thought out and designed often up-front, slowing down the process. However, this is a contribution of this thesis that it outlines exactly these structures within ABS so that implementers who want to use the same approach do not have to reinvent the wheel.

A more severe problem is that pure functional programming, especially Haskell, is seen as hard to learn with a steep learning curve, putting a high barrier to implementers picking up a pure functional approach to ABS. Thus, the lack of broad availability of Haskell expertise can be enough to pose a serious drawback even if the approach of this thesis seem to be desirable in a project.

\subsection{Agent interactions}
%This thesis is \textit{one} way of showing how to separate both and reap the benefits. A time-driven ABS like SIR or an ACE with simple agents not interaction with each other like ZI traders is heavily data-centric and very low on agent-interaction. Such data-driven ABS models are quite well expressed in a purely functional approach with the advantage that one can reap the benefits of reproducibility at compile time, using STM for concurrency and property-based testing for verification and validation. An event-driven simulation with complex agent state and agent-interactions like social simulations like Sugarscape or Chemical or Biology simulation with cell interactions are also possible in a pure functional setting as we have shown in the case of the Sugarscape model. 
Although we were able to give a solution to complex agent state and synchronous, direct agent interactions in our event-driven SIR and Sugarscape and they \textit{do} work in Haskell, they are cumbersome to get when building from scratch and as we pointed out in Chapter \ref{ch:concurrent_abs}, it seems that our approach to synchronous direct agent interactions is not applicable to concurrency with STM. 

This leads to the fundamental conclusion that in models which require complex agent interactions in a potentially concurrent environment, we are hitting the limits of our pure functional approach. The reason for it is that we have a conceptual mismatch, as in such a setting, agents are more naturally represented using the Actor Model. The Actor Model, a model of concurrency, was initially conceived by Hewitt in 1973 \cite{hewitt_universal_1973} and refined later on \cite{hewitt_what_2007}, \cite{hewitt_actor_2010}. It was a major influence in designing the concept of agents and although there are important differences between actors and agents there are important similarities thus the idea to use actors to build ABS comes quite natural. %The theory was put on firm semantic grounds first through Irene Greif by defining its operational semantics \cite{grief_semantics_1975} and then Will Clinger by defining denotational semantics \cite{clinger_foundations_1981}. In the seminal work of Agha \cite{agha_actors:_1986} he developed a semantic mode, he termed \textit{actors} which was then developed further \cite{agha_foundation_1997} into an actor language with operational semantics which made connections to process calculi and functional programming languages (see both below). 

An actor is a uniquely addressable entity which can do the following \textit{in response to a message}:
\begin{itemize}
	\item Send an arbitrary number of messages to other actors.
	\item Create an arbitrary number of actors.
	\item Define its own behaviour upon reception of the next message.
\end{itemize}

%In the Actor Model theory there is no restriction on the order of the above actions and so an actor can do all the things above in parallel and concurrently at the same time. This property and that actors are reactive and not pro-active is the fundamental difference between actors and agents. % so an agent is \textit{not} an actor but conceptually nearly identical and definitely much closer to an agent in comparison to an object.
When comparing this definition to the one of agents we give in Chapter \ref{sec:method_abs}, it is clear that the Actor Model was quite influential to the development of the concept of agents in ABS, which borrowed it from Multi Agent Systems \cite{wooldridge_introduction_2009}. Technically, it emphasises message-passing concurrency with share-nothing semantics (no implicitly shared state through side-effects between agents), which maps nicely to functional programming concepts.

Indeed, the programming model of actors \cite{agha_actors:_1986} was the inspiration for the functional programming language Erlang \cite{armstrong_erlang_2010}, which was created in the 1980's by Joe Armstrong for Eriksson for developing distributed high reliability software in telecommunications. We hypothesise that a true concurrent actor approach like Erlang is substantially more natural and much more performant especially in a concurrent setting. We believe that actor based ABS implementations have a big future as ABS tends to develop towards larger and larger, distributed, always-online simulations, for which Erlang is arguably perfectly suited. We have prototyped highly promising concurrent event-driven SIR and Sugarscape implementations in Erlang and they supports our hypothesis. Unfortunately, an in-depth discussion is beyond the scope of this thesis and we leave this topic for further research.

%This makes testing easier and also opens the way for property-based testing which is available in Erlang as well where it even allows to detect race conditions \cite{claessen_finding_2009}. 

%erlang processes can implement everything objects can but in a referential transparent and pure functional way: encapsulation, polymorphism, identity, message passing, even inheritance (which you wouldnt want to do)

%difference of erlang to objects is that although it encapsulate state it cannot be accessed at the same time but only through the message passing Interface with one message at a time. this means that state is not really shared and protected against mutation - the process is in full control. it is simply a function which captures the full state of the process in an immutable way: to change the state a recursive call needs to be done. so in the end although it seems conceptually related its technically difference is fundamental importance.
%
%my mistake was to confuse the concept of objects with their implementation. i was too focused on the drawbacks of e.g. java objects that i forgot that i was critisising its IMPLEMENTATION. the Original idea of alan kays objects IS a deep and strong idea, though it differes substantially from java objects, erlang comes closest. thus the concept is important and valid but different implementations have different benefits and drawbacks. 