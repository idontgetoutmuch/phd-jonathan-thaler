\section{Drawbacks}
\label{sec:drawbacks}
Obviously nothing comes without drawbacks, and also our approach suffers from a few. We discuss them here and propose solutions where applicable.

\subsection{Efficiency}
\label{sec:drawback_efficiency}
Currently, the performance our approach does not come close to imperative implementations. We compared the performance of the time-driven SIR as presented in Section \ref{sec:adding_env} to an implementation in Java using the ABS library RePast \cite{north_complex_2013}. We ran the simulation until $t = 100$ on a 51x51 (2,601 agents) with $\Delta t = 0.1$ (unknown in RePast) and averaged 8 runs. The performance results make the lack of speed of our approach quite clear: the pure functional approach needs around 72.5 seconds whereas the Java RePast version just 10.8 seconds on our machine to arrive at $t = 100$. It must be mentioned, that RePast does implement an event-driven approach to ABS, which can be much more performant \cite{meyer_event-driven_2014} than a time-driven one as ours, so the comparison is not completely valid. 

As a remedy we did a time- and event-driven SIR implementation in Java and compared the performance to  pure functional time- and event-driven implementations. In the Java implementations we tried to follow conceptually similar approaches to the pure functional implementations but obviously that is not possible for every aspect. For example, we are not using any reactive programming but we follow a similar time-sampling approach. In all cases we do not use an environment, run for 150 time-steps with 1,000 susceptible and 1 infected agent, $\beta = 5$, $\gamma = 0.05$, $\delta = 15$ and $\delta t = 0.01$ in both time-driven implementations. Further, we fixed the random-number generators to guarantee identical dynamics in every run and averaged 8 runs.

% TIME-DRIVEN
% Java: 507, 583, 513, 577, 516, 526, 520, 519 Milliseconds
% Haskell: 18.8, 18.9, 18.9,  Seconds

% EVENT-DRIVEN
% Java: 1262, 1259, 1159, 1202, 1182, 1152, 1238, 1238 Milliseconds
% Haskell: 6.8, 6.6, 6.3, 6.3, 6.5, 6.3, 6.3, 6.8 Seconds

The time-driven Java implementation averages at a performance of 0.5 seconds, compared to 18.8 seconds in Haskell. The event-driven Java implementation averages at 1.2 seconds, compared to 6.5 of Haskell. This underlines dramatically how far the pure functional approach is from an equivalent imperative, OO implementation.

\medskip

There are two main reasons for it: first, FP is known for being slower due to higher level of abstractions, which are bought by slower code in general and second, updates are the main bottleneck due to immutable data requiring to copy the whole (or subparts) of a data structure in cases of a change. The first one is easily addressable through the use of data-parallelism and concurrency as shown in Chapter \ref{ch:parallelism_ABS} and \ref{ch:concurrent_abs}. The second reason can be addressed by the use of linear types \cite{bernardy_linear_2017}, which allow to annotate a variable with how often it is used within a function. From this a compiler can derive aggressive optimisations, resulting in imperative-style performance but retaining the declarative nature of the code. Also the use of monad transformer stacks has performance implications which can be quite subtle but can be alleviated by careful usage and re-ordering of lifts which might result in increased performance e.g. instead of mapM (lift ...) do lift (mapM ...).

However, it was shown by various people  \cite{stewart_haskell_2008, stolarek_haskell_2013, kqr_competing_2017} that it is possible to reach C speed in Haskell as well. The direction to do this is using the worker/wrapper transformation \cite{gill_worker/wrapper_2009}, clever combination of techniques with strict \textit{foldl'} and data-declaration with strictness annotation instead of lazy tuples and Stream Fusion \cite{coutts_stream_2007, mainland_haskell_2013}. The problem is of course that to apply these techniques one needs to have deep knowledge of Haskell and its subtle details of (lazy) evaluation, making this a very non-trivial task. A problem is that those techniques seem only applicable in a context of a tight loop which crunches numbers of a list, thus it is not directly applicable in our case as we are clearly bound by the effectful computations: MSF and Monad Transformers are the limiting factor, not inner loops.

\medskip

Concluding we can say that the current performance makes our approach not very attractive for real-world use \textit{at the moment}. Also the fact that the sequential OO implementation seems to outperform the concurrent and parallel implementations as well, seems to question our motivation as why to use pure functional programming and parallel computation at all. Still, the bad performance results do not invalidate our research as this thesis aim is not the development of high-performance pure functional implementations but rather exploring concepts of ABS in pure functional programming which are only first steps and need to be developed further into something to be used in the real world. We don't think that our pure functional approach will be able to reach the Java performance but we think that it should be possible to come considerably closer, making it more applicable for real-world usage. We leave a deeper investigation of this problem for further research.

\subsection{Space-Leaks}
Haskell is notorious for its memory-leaks due to lazy evaluation: data is only evaluated when required. Even for simple programs one can be hit hard by a serious space-leak where unevaluated code pieces (thunks) build up in memory until they are needed, leading to dramatically increased memory usage. It is no surprise that our highly complex Sugarscape implementation initially suffered severely from space-leaks, piling up about 40 MByte / second. In simulation this is a big issue, threatening the value of the whole implementation despite its other benefits: because simulations might run for a (very) long time or conceptually forever, one must make absolutely sure that the memory usage stays somewhat constant. As a remedy, Haskell allows to add so-called strictness pragmas to code-modules which forces strict evaluation of all data even if it is not used. Carefully adding this conservatively file-by file applying other techniques of forcing evaluation removed most of the memory leaks.

Another memory leak was caused by selecting the wrong data-structure for our environment, for which we initially used an immutable array. The problem is that in the case of an update the whole array is copied, causing memory leaks AND a performance problem. We replaced it by an IntMap which uses integers as key (mapping 2d coordinates to unique integers is trivial) and is internally implemented as a radix-tree which allows for very fast lookups and inserts because whole sub-trees can be re-used.

%discuss the problem (and potential) of lazy evaluation for ABS: can under some circumstances really increase performance when some stuff is not evaluated (see STM study) but mostly it causes problems by piling up unevaluated thunks leading to crazy memory usage which is a crucial problem in simulation. Using strict pragmas, annotations and data-structures solves the problem but is not trivial and involves carefully studying the code / getting it right from the beginning / and using the haskell profiling tools (which are fucking great at least). TODO: show the stats of memory usage

\subsection{Productivity and learning curve}
A case study in \cite{hanenberg_experiment_2010} hints that simply by switching to a static type system alone does not gain anything and can even be detriment. It also needs to have a certain level of abstractions like Haskell type system does or even dependent types as in Idris. Although such case-studies have to be taken with care, there is also some truth in it: working in a statically strong type system prevents the developer from moving quickly and making quick and dirty changes. This can be both a benefit and a drawback: in general it prevents from breaking changes which show only up at compile time as in dynamic languages but at the same time the whole program is much more rigid and a proper structure needs to be thought out and designed often up-front, slowing down the process. However, this is a contribution of this thesis that it outlines exactly these structures within ABS so that implementers who want to use the same approach do not have to reinvent the wheel.

A more severe problem is that pure functional programming, especially Haskell is seen as hard to learn with a steep learning curve. This and the fact that pure functional ABS achieves similar things as existing OO approaches puts a high barrier to implementers picking up a pure functional approach to ABS. Thus the lack of availability of Haskell expertise can be enough to pose a serious drawback even if the approach of this thesis seem to be desirable in a project.

\subsection{Agent interactions}
This thesis is \textit{one} way of showing how to separate both and reap the benefits. A time-driven ABS like SIR or an ACE with simple agents not interaction with each other like ZI traders is heavily data-centric and very low on agent-interaction. Such data-driven ABS models are quite well expressed in a purely functional approach with the advantage that one can reap the benefits of reproducibility at compile time, using STM for concurrency and property-based testing for verification and validation. An event-driven simulation with complex agent state and agent-interactions like social simulations like Sugarscape or Chemical or Biology simulation with cell interactions are also possible in a pure functional setting as we have shown in the case of the Sugarscape model. Although we were able to give a good solution to complex agent state and synchronous, direct Agent-interactions in our event-driven SIR and Sugarscape and they \textit{do} work in Haskell, they are cumbersome to get right without library support and as we pointed out in the STM chapter, it seems that this approach to synchronous bi-directional agent-interactions is not applicable to concurrency with STM. 

This has lead to the fundamental conclusion of this thesis, that although we have shown the benefit of functional programming in ABS, in models which require complex agent-interactions in a potentially concurrent we are hitting the limits of the pure functional approach here. The reason for it is that ultimately agents do not map naturally to objects but agents map naturally to actors. 

The Actor-Model, a model of concurrency, was initially conceived by Hewitt in 1973 \cite{hewitt_universal_1973} and refined later on \cite{hewitt_what_2007}, \cite{hewitt_actor_2010}. It was a major influence in designing the concept of agents and although there are important differences between actors and agents there are huge similarities thus the idea to use actors to build agent-based simulations comes quite natural. The theory was put on firm semantic grounds first through Irene Greif by defining its operational semantics \cite{grief_semantics_1975} and then Will Clinger by defining denotational semantics \cite{clinger_foundations_1981}. In the seminal work of Agha \cite{agha_actors:_1986} he developed a semantic mode, he termed \textit{actors} which was then developed further \cite{agha_foundation_1997} into an actor language with operational semantics which made connections to process calculi and functional programming languages (see both below). 

An actor is a uniquely addressable entity which can do the following \textit{in response to a message}
\begin{itemize}
	\item Send an arbitrary number (even infinite) of messages to other actors.
	\item Create an arbitrary number of actors.
	\item Define its own behaviour upon reception of the next message.
\end{itemize}

In the actor model theory there is no restriction on the order of the above actions and so an actor can do all the things above in parallel and concurrently at the same time. This property and that actors are reactive and not pro-active is the fundamental difference between actors and agents, so an agent is \textit{not} an actor but conceptually nearly identical and definitely much closer to an agent in comparison to an object. The actor model can be seen as quite influential to the development of the concept of agents in ABS, which borrowed it from Multi Agent Systems \cite{wooldridge_introduction_2009}. Technically, it emphasises message-passing concurrency with share-nothing semantics (no shared state between agents), which maps nicely to functional programming concepts.

The programming-model of actors \cite{agha_actors:_1986} was the inspiration for the Erlang programming language \cite{armstrong_erlang_2010}, which was created in the 1980's by Joe Armstrong for Eriksson for developing distributed high reliability software in telecommunications. The implication is that, the focus would shift immediately to the use of the actor model for concurrent interaction of agents through messages. The languages type-system is strong and dynamic and thus lacks type-checking at compile-time. Thus the structure of computation plays naturally no role because we cannot look at it from the abstract perspective as we can in Haskell. Purity can not be guaranteed and due to agents being processes concurrency is everywhere, and even though it is very tamed through shared-nothing messaging semantics, this implies that repeated runs with same initial conditions might lead to different results. Obviously we could avoid implementing agents as processes but then we basically sacrifice the very heart and feature of the language.

Despite its focus on messages, Erlang is a functional languages, which puts you into the data-centric approach: messages are pure data with \textit{shared nothing semantics}. This makes testing easier and also opens the way for property-based testing which is available in Erlang as well where it even allows to detect race conditions \cite{claessen_finding_2009}. 

We hypothesize that an true concurrent actor approach like Erlang is substantially more natural, much more performant and opens up for concurrency. its main drawback is its dynamic type system which reduces guarantees we have about correctness at run-time, also concurrency leads to different dynamics. We have prototyped highly promising concurrent event-driven SIR and Sugarscape implementations in Erlang and they supports our hypothesis. Unfortunately, an in-depth discussion is beyond the scope of this thesis and which we leave this for highly promising further research in the conclusion.

%erlang processes can implement everything objects can but in a referential transparent and pure functional way: encapsulation, polymorphism, identity, message passing, even inheritance (which you wouldnt want to do)

%difference of erlang to objects is that although it encapsulate state it cannot be accessed at the same time but only through the message passing Interface with one message at a time. this means that state is not really shared and protected against mutation - the process is in full control. it is simply a function which captures the full state of the process in an immutable way: to change the state a recursive call needs to be done. so in the end although it seems conceptually related its technically difference is fundamental importance.
%
%my mistake was to confuse the concept of objects with their implementation. i was too focused on the drawbacks of e.g. java objects that i forgot that i was critisising its IMPLEMENTATION. the Original idea of alan kays objects IS a deep and strong idea, though it differes substantially from java objects, erlang comes closest. thus the concept is important and valid but different implementations have different benefits and drawbacks. 