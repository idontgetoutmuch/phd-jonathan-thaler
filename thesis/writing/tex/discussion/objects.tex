\section{Do agents map naturally to objects?}
One of the initial motivation of this thesis was the claim of North et al. \cite{north_managing_2007} that \textit{agents map naturally to objects}. At the very end of this thesis we want to revisit this claim in the new light of our pure functional approach and finally answer the questions whether agents do map naturally to objects or not.

\medskip

To give a satisfactory answer, we first need to reexamine the abstractions used in our pure functional approach, where the fundamental building blocks are \textit{recursion} and \textit{continuations}. In recursion a function is defined in terms of itself: in the process of computing the output it \textit{might} call itself with changed input data. \textit{Continuations} in turn, are functions which allow to encapsulate the execution state of a program by capturing local variables (known as closure) to pick up computation from that point later on by returning a new function.

As an explanatory example, we implement a continuation in Haskell which sums up integers and stores the sum locally as well as returning it as return value of the current step. First, we define the type of a general continuation, which takes a polymorphic type \texttt{i} as input and returns a polymorphic type \texttt{o} as output together with a new continuation

\begin{HaskellCode}
newtype Cont i o = C (i -> (o, Cont i o))
\end{HaskellCode}

Then we implement an actual instance of a continuation with input and output types fixed to \texttt{Int}. It takes an initial value \texttt{x} and sums up the values passed to it. It returns \texttt{adder} with the new sum recursively as the new continuation.

\begin{HaskellCode}
adder :: Int -> Cont Int Int
adder x = C (\x' -> let s = x + x' in
                    (s, adder s))
\end{HaskellCode}

To run a continuation, we implement a function which runs a given continuation for a given number of steps and always passes \texttt{x} as input and prints the continuations output.

\begin{HaskellCode}
runCont :: Int -> Int -> Cont Int Int -> IO ()
runCont 0 x _ = return () 
runCont n x (C cont) = do 
  -- run the continuation with x as input, cont' is the new continuation
  let (x', cont') = cont x
  print x'
  runCont (n-1) x cont' 
\end{HaskellCode}

When actually running the continuation \texttt{adder} with an initial value of -1 for 10 steps and increments of 2, we get the following output:

\begin{HaskellCode}
> runCont 100 2 (adder (-1))
1
3
...
17
19
\end{HaskellCode}

This explanatory example should make it clear that we can encapsulate arbitrary complex state, which is only visible and accessible from within the continuation. Further, with a continuation it becomes possible to switch behaviour dynamically, like switching from one mode of behaviour to another as in a state machine, simply by returning new functions which encapsulate the new behaviour. If no change in behaviour should occur, the continuation simply recursively returns itself with the new state captured as seen in the example above.

In fact, Yampas signal functions (SF) and Dunais Monadic Stream Functions (MSF) are nothing else than such continuations: SF are pure, without a monadic context, as can be seen in the implementation of the supersampling in Chapter \ref{sub:timedriven_results}; MSFs have an additional monadic context, which makes it possible to execute effectful computations within the continuation as can be seen in the implementation of the simulation stepping MSF in Chapter \ref{sub:timedriven_thirdstep_impl}. 

\medskip

When looking closer at the example from above, it becomes clear that the continuation \texttt{adder} is non-terminating and is a potentially infinite structure, possible through lazy evaluation of Haskell, where the function \texttt{runCont} deconstructs / consumes / observes the output of this infinite structure step-by-step. This is related to the concepts of \textit{corecursion} which are an even deeper underlying theory to continuations in general and our approach in particular. Technically speaking, corecursion is the dual to recursion where instead of starting with a data structure and reducing it stepwise until a base case is reached, corecursion starts with an initial value and iterates it ad infinity, producing an infinite data structure as output, enabled through lazy evaluation. 
Indeed, our agents produce infinite streams as output, potentially running for infinite time as it is implemented in the time-driven approach and to a lesser extent in the event-driven SIR. Now it is also easy to see why agents are not represented by pure functions: they have to change over time, which is precisely what pure functions cannot do as they can not rely on a context or history of a system.

The fact that we represented pure functional agents as SF and MSF is thus no coincidence and did not fall from the sky: they are in fact representations of \textit{coalgebras}, which is the way to express dynamical systems in mathematics and in pure functional programming: \textit{"In general, dynamical systems with a hidden, black-box state space, to which a user only has limited access via specified (observer or mutator) operations, are coalgebras of various kinds"} \cite{jacobs_tutorial_1997}. Informally speaking a coalgebra is of the form $S \xrightarrow[\text{}]{c} $ \fbox{... S ...}, with a state space \textit{S}, a function \textit{c} and a structured output (the box), which also contains the original domain \textit{S} \cite{jacobs_introduction_2017}. This is precisely what we see in the recursive type definition of \texttt{Cont} above.

This sounds very much like agents and indeed, coalgebras have been used (amongst others) in Process Theory to model communicating processes, a topic closely related to Actors and ABS; and objects in object-oriented programming \cite{jacobs_coalgebras_2003}. It seems that we have found an underlying theory which connects both object-oriented programming and our pure functional ABS approach. This hints that it might be indeed the case that agents map naturally to objects, as we discuss below. We refer to \cite{jacobs_introduction_2017, jacobs_tutorial_1997} for a proper, formal introduction and discussion of coalgebras as it is beyond the scope of this thesis.

% ionescus thesis \cite{ionescu_vulnerability_2009} somewhere here? 

When following the concepts of continuations and coalgebras from above and the viewpoint that \textit{"... a closure is just a primitive form of object: the special case of an object with just one method."} \footnote{https://www.tedinski.com/2018/11/20/message-oriented-programming.html} then one way to look at an SF and MSF is to see them as very simple immutable objects with a single method - the continuation - following a shared-nothing semantics. 

Like in coalgebras and in continuations we have some internal state which can be altered through specified set of operations (events/inputs) and the effect can be observed through the output but not directly. This is particularly clear in the Sugarscape model, where agents have indeed a complex internal state, which changes only through events and is only observable through the output of data type \textit{SugAgentObservable} as a result of sending an event. Further, we added a notion of agent identity, a clearly specified agent interface, local agent state and synchronous direct agent interactions through tagless final.

\medskip

% 2. it seems that agents map to objects, but what are objects?
This interpretation and the fact that we seem to have achieved all the relevant concepts like encapsulation of local agent state and interactions purely functional, it seems that we indeed have to agree that agents do actually map naturally to objects. However, we argue we have to think objects in a much broader context than the one of existing object-oriented terminology as in the popular family of Java, C++ and Python. The fact that we can represent agents as objects also in a purely functional way, with sound underlying theories like coalgebras, leads us to the question, what actually constitutes objects and we have to be careful not to confuse the \textit{concept} of objects with their \textit{implementation} within a language.

% 3. what are objects
There does not exist a commonly agreed upon definition of objects and object-oriented programming but rather a bunch of ideas and concepts \footnote{\url{http://wiki.c2.com/?DefinitionsForOo}}. It is agreed that the original ideas of objects and object-oriented programming were conceived by Kristen Nygaard, the inventor of Simula 67, the first object-oriented language \cite{dahl_birth_2002} and Alan Kay, the inventor of SmallTalk, another pioneering object-oriented language in the early 70s \cite{kay_early_1993}. %Their ideas about OO where the following: \footnote{\url{http://wiki.c2.com/?AlanKaysDefinitionOfObjectOriented} and \url{http://wiki.c2.com/?NygaardClassification}}:

Kristen Nygaard identified object-oriented programming by \textit{"A program execution is regarded as a physical model, simulating the behaviour of either a real or imaginary part of the world."}, thus he puts the focus on the modelling aspect of the problem. Alan Kay claims to have coined the term \textit{object-oriented} and defines it in more technical terms: everything is an object; every object is an instance of a class; the class holds the shared behaviour for its instances; objects communicate by sending and receiving messages. Alan Kay puts a strong emphasis on sending and receiving messages, with a shared-nothing interpretation. This becomes especially clear in a quote attributed to him: \textit{"The big idea is "messaging" ... "I invented the term Object-Oriented and I can tell you I did not have C++ in mind."}.

\medskip

So we see that the original \textit{concepts} of objects and object-oriented programming vary considerably from how objects and object-oriented programming is \textit{implemented} today in the family of popular object-oriented programming languages like Java, C++ and Python. %The most substantial different to the original definition of Kay is that messages are not pure data - they do not follow a shared nothing semantics.
Our approach is \textit{one} answer to do that in a pure, strong statically typed language - Haskell. It can be seen as an object-centric approach, which \textit{implements} a very simple \textit{concept} of shared-nothing, immutable, pure functional objects.

%It is a fact that simulations are about consuming, processing and producing data. ABS being simulation methodology is no exception to that fact. Unfortunately, due to OO lack of rigour theoretical foundations, OO as it is used today is \textit{not} very good at representing and manipulating pure data and its data flow because of two things: \textit{mutable shared state} and explicitly associate data-types and functions(methods)/code/behaviour.

%FROM https://www.youtube.com/watch?v=QM1iUe6IofM&feature=youtu.be
%Inheritance is not relevant any more: it has come to a widely agreement amongst OO developers that inheritance should be avoided: https://www.javaworld.com/article/2073649/why-extends-is-evil.html . Note that we are speaking about subclassing not implementing an interface, which is something entirely different
%Polymorphism: is not unique to OO and exists in non-OO languages as well and plays a central role in Haskell (and ML languages). Further it is possible to implement polymorphic code in C
%Encapsulation: this is seen as the major strength of OO but unfortunately it does not work at a fine grained level of code in todays OO. The original idea was indeed great and it is no coincidence that my implementation ended up with a variation of that as well as Erlang: encapsulate state behind a public interface and interact with it through messages (TODO: fill in alan kay). The very central point of messages though was that they followed "shared nothing" semantics, meaning that no references or pointers could be contained in that message as this would immediately result in a violation of the public interface and ultimately breaks encapsulation. 
%OO dominates the industry since around mid 90s. There are varying opinions on that but a major influence to popularise OO was Java, which made its first appearance in 1996. Java was a much easier approach to OO than existing ones e.g. in C++ and VB: it abandoned multiple inheritance, introduced interfaces, was cross-platform, provided high quality libraries including a GUI framework (GUI programming was the way to go in the 90s until it got abandoned in 00s with the emergence of Web 2.0), C/C++ syntax made it easy to pick up, avoided header-files, abandoned pointers and memory management and added garbage collection which made applications a lot safer.

% TODO: need to discuss the problem of shared state. state per se is not necessarily a problem and ever program has state in some form. how explicit it is represented is often used as classification between different kind of paradigms e.g. it has been said that functional programming is stateless but that is obviously not true, state is all over the place but it is very very contained, well behaved and explicit. with shared mutable state this is not the case anymore and we get into the troube of data-dependencies and orderings. this is exactly what we encountered when having introduced a global environment in Sugarscape: although our state is referential transparent and pure functional, they way we used it is globally and we run in ordering issues.

% TODO: isnt shared state also a problem in erlang? after all we can send Pids around and interact with those processes as soon as another process has access to a Pid. In which way is it different to reference passing in OO? There seems to be no difference... so maybe the anti OO argument is not that strong after all and my argument is simply weak or wrong? 

%TODO: i REALLY need to find proper literature / research / evidence which shows the problematic nature of modern OO: mutable shared state which is tied to code. Inheritance and open recursion gives the rest. the problem is that deeply linking \textit{shared mutable} state to its code is the path to failure: abstraction breaks, concurrency and parallelism becomes hard and breaks abstraction, data-driven programming becomes difficult (although that got addressed by adding functional features). NOTE: my approach and erlang have state and behaviour as well but in our case the state is shared nothing and immutable (yes in Haskell we update the agents state but that happens ultimately through closures and continuation in a referential transparent way and still no state is shared between agents. the environment is an exception to some extent as agents can access it globally: this works but requires a specific ordering either through sequential access or STM. this is no different than in an erlang implementation of sugarscape: there needs to be some arbitration of concurrent access). TODO: isnt there some fundamental research on that issue out there?
% TODO: maybe these act as a starting point?
% https://www.yegor256.com/2016/08/15/what-is-wrong-object-oriented-programming.html
% https://dl.acm.org/citation.cfm?id=1806847
% https://web.cs.ucdavis.edu/~filkov/papers/lang_github.pdf "Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing" "We also find that functional languages are somewhat better than procedural languages" but modest effects
% https://www.javaworld.com/article/2073649/why-extends-is-evil.html
% READ extension problem paper
% READ Ted Kaminskis thesis

%This was by no means clear in the early-to-mid 1990s where the OO paradigm was seen as a silver bullet to the problems of programming: a whole software industry had to re-learn best practices, patterns \cite{gamma_design_1994} and how to avoid pitfalls and bad code \cite{fowler_refactoring:_2012}. Thus we cannot blame \cite{epstein_growing_1996} for advertising OO as the ways to implement ABS, at that time it seemed indeed to be the right thing to do. 

%The combination of both was exactly the sales pitch of OO for the last 20+ years. Unfortunately this combination leads to nasty bugs due to shared mutable state, deeply complex object hierarchies due to inheritance overuse which also fix behaviour at compile time, open recursion which in the end costs the potential for higher degree of correctness, ease of parallelism and concurrency and the use of property-based testing. Thus we need to separate both: what we need is immutable, shared-nothing state allowing for a data-centric approach \textit{and} an interaction mechanism which allows agents to communicate with each other.

% 4. implementation of objects: the problems: data-driven programming is difficult, not really encapsulating and shared mutable state makes concurrency and testing a lot harder. this sound as a contradition but it has been shown that despite objects claim they compose and enforce encapsulation, they do not.
% https://dl.acm.org/citation.cfm?doid=242224.242415

%So we see that the original \textit{concepts} of objects and object-oriented programming vary considerably from how objects and object-oriented programming is \textit{implemented} today in the family of popular object-oriented programming languages like Java, C++ and Python. The most substantial different to the original definition of Kay is that messages are not pure data - they do not follow a shared nothing semantics. This leads to the failure of objects to compose behaviour and encapsulate data properly \cite{bill_what_2017}, \cite{erkki_lindpere_why_2013}. Ironically, this has always been the main argument for advertising the use of object-oriented programming. The reason for this is that objects hide both \textit{mutation} and \textit{sharing through pointers or references} of object-internal data. Further, they expose multiple methods on how to operate on this encapsulated data. This makes data-flow mostly implicit due to the side effects on the mutable data which is globally scattered across objects. To deal with the problem of composability and implicit data-flow the seminal work \cite{gamma_design_1994} put forward the use of \textit{patterns} to organize objects and their interaction. Other concepts, trying to address the problems, were the SOLID principles and Dependency Injection. 
%
%% 4a this leads to an inherent difficulty to follow data-flow in an OO program and also makes it very difficult in concurrent settings as semantics of synchronisation "bleed" out of the object, breaking encapsulation. 
%Despite these advances in understanding the object-oriented programming paradigm and how to use it properly, the increased complexity leads to an inherent difficulty to express and follow data flow in an object-oriented program and exploit parallelism and concurrency due to mutable shared state. Even worse, concurrency breaks encapsulation of objects as well and prevents composing them. 
%
%The rise of functional concepts in object-oriented languages in the last years are a strong indication that object-oriented programming is lacking features which have existed in functional programming for decades. Java 8 added lambda expressions and functional style programming using \textit{map, fold, reduce, filter} which together with lambdas allow a data-flow oriented approach to computing. Python, which surges in popularity within the object-oriented family of languages, allows very data-flow centric and functional style of programming through lambda functions, list comprehensions and other functional features as it does not require programmers to stick to the object-oriented programming paradigm. Popularisation of JavaScript frameworks like React, Elm and Purescript, which emphasise a functional, data-flow driven approach of web-programming are another indicator. Thus it seems that functional concepts overcome the weakness of object-oriented programming to model explicit, immutable data flows which can be exploited towards easier parallelisation and concurrency.
%
%% 5. our approach is one of very simple, pure functional, immutable objects and we have shown that they indeed allow us to apply concurrency and property-based testing
%All these properties of explicit data flow and applicability of parallelism and concurrency are highly desirable when implementing simulations: it is a fact that simulations are data-centric, they are all about about consuming, processing and producing data and they have to do it fast and correct. ABS, being a simulation methodology, is no exception to that fact.
%
%The question is then why not use toolkits like Matlab or R - after all they are completely data-centric? This would be the other extreme, just like object-oriented programming is and we would run into difficulties as well. The point is that ABS is not purely data-centric either and is indeed richer: agents can interact with each other and with an environment. So we have a tension here: ABS is data-centric on the one hand, and interaction-centric on the other - can we combine both worlds? 
%
%%TODO: this tension between data and objects has its origins in the expression problem (TODO: cite the paper): https://www.tedinski.com/2018/03/06/more-on-the-expression-problem.html we want to have a general approach and thus abstraction:
%%in the sugarscape implementation we used a tagless final approach to effects, which is extensible in two dimensions: vertically, we can add new interpreters; horizontally, we can add new effectful operations.  
%%TODO: properly study
%%- https://www.tedinski.com/2018/03/06/more-on-the-expression-problem.html
%%- https://serokell.io/blog/2018/12/07/tagless-final
%%- https://jproyo.github.io/posts/2019-03-17-tagless-final-haskell.html
%%- The Expression Problem Revisited - Four new solutions using generics by Mads Torgersen
%
%%\section{Discussion}
%%Building on top of these concepts allowed us to implement pro-activity of agents, encapsulation of local agent state, a pro-active environment with shared mutable state and synchronous agent interactions based on an event-driven approach. With our case studies we have explored the structure of agent computation in a more practical / applied way and we have extracted and distilled the general concepts and abstractions behind agent computation and showed how pure ABS computation can be seen structurally. This should give the ABS field a deeper understanding of the structure of the computations behind ABS, which has so far always been more ad-hoc. %As already mentioned in the introduction, this becomes possible through pure functional programming because it treats computations in a more structural way, where data structures and types defines the structure of various kinds of computation e.g. Monoids, Applicatives, Monads, Comonads, Arrows,...