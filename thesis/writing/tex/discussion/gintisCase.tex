\chapter{The Gintis Case}
\label{ch:gintis_case}
TODO UNFINISHED OPTIONAL CHAPTER

In \cite{axelrod_chapter_2006} Axelrod reports the vulnerability of ABS to misunderstanding. Due to informal specifications of models and change-requests among members of a research-team bugs are very likely to be introduced. He also reported how difficult it was to reproduce the work of \cite{axelrod_convergence_1995} which took the team four months which was due to inconsistencies between the original code and the published paper. The consequence is that counter-intuitive simulation results can lead to weeks of checking whether the code matches the model and is bug-free as reported in \cite{axelrod_advancing_1997}.
The same problem was reported in \cite{ionescu_dependently-typed_2012} which tried to reproduce the work of Gintis \cite{gintis_emergence_2006}. In his work Gintis claimed to have found a mechanism in bilateral decentralized exchange which resulted in walrasian general equilibrium without the neo-classical approach of a tatonement process through a central auctioneer. This was a major break-through for economics as the theory of walrasian general equilibrium is non-constructive as it only postulates the properties of the equilibrium \cite{colell_microeconomic_1995} but does not explain the process and dynamics through which this equilibrium can be reached or constructed - Gintis seemed to have found just this process. Ionescu et al. \cite{ionescu_dependently-typed_2012} failed and were only able to solve the problem by directly contacting Gintis which provided the code - the definitive formal reference. It was found that there was a bug in the code which led to the "revolutionary" results which were seriously damaged through this error. They also reported ambiguity between the informal model description in Gintis paper and the actual implementation.
This lead to a research in a functional framework for agent-based models of exchange as described in \cite{botta_functional_2011} which tried to give a very formal functional specification of the model which comes very close to an implementation in Haskell.
This was investigated more in-depth in the thesis by \cite{evensen_extensible_2010} who got access to Gintis code of \cite{gintis_emergence_2006}. They found that the code didn't follow good object-oriented design principles (all was public, code duplication) and - in accordance with \cite{ionescu_dependently-typed_2012} - discovered a number of bugs serious enough to invalidate the results. This reporting seems to confirm the above observations that proper object-oriented programming is hard and if not carefully done introduces bugs.
The author of this text can report the same when implementing \cite{epstein_growing_1996}. Although the work tries to be much more clearer in specifying the rules how the agents behave, when implementing them still some minor inconsistencies and ambiguities show up due to an informal specification.
The fundamental problems of these reports can be subsumed under the term of verification which is the checking whether the implementation matches the specification. Informal specifications in natural language or listings of steps of behaviour will notoriously introduce inconsistencies and ambiguities which result in wrong implementations - wrong in the way that the \textit{intended} specification does not match the \textit{actual} implementation. To find out whether this is the case one needs to verify the model-specification against the code. This is a well established process in the software-industry but has not got as much attention and is not nearly as well established and easy in the field of ABS as will become evident in the literature-review.
As ABS is almost always used for scientific research, producing often break-through scientific results as pointed out in \cite{axelrod_chapter_2006}, these ABS need to be \textit{free of bugs}, \textit{verified against their specification}, \textit{validated against hypotheses} and ultimately be \textit{reproducible}. One of the biggest challenges in ABS is the one of validation. In this process one needs to connect the results and dynamics of the simulation to initial hypotheses e.g. \textit{are the emergent properties the ones anticipated? if it is completely different why?} It is important to understand that we always \textit{must have} a hypothesis regarding the outcome of the simulation, otherwise we leave the path of scientific discovery. We must admit that sometimes it is extremely hard to anticipate \textit{emergent patterns} but still there must be \textit{some} hypothesis regarding the dynamics of the simulation otherwise we drift off into guesswork.

In the concluding remarks of \cite{axelrod_chapter_2006} Axelrod explicitly mentions that the ABS community should converge both on standards for testing the robustness of ABS and on its tools. However as presented above, we can draw the conclusion that there seem to be some problems the way ABS is done so far. We don't say that the current state-of-the-art is flawed, which it is not as proved by influential models which are perfectly sound, but that it always contains some inherent danger of embarrassing failure.

Discuss my developed techniques to the Gintis paper (and its follow ups: the Ionescu paper \cite{botta_functional_2011} and a Masterthesis \cite{evensen_extensible_2010} on it). Answer the following:

\begin{enumerate}
	\item Do the techniques transfer to this problem and model? 
	
	\item Could pure functional programming have prevented the bugs which Gintis made? 
	
	\item Would property-based tests have been of any help to preven the bugs?
	
	\item Could dependent and / or types have prevented the bugs which Gintis made? 
	
	\item How close is our (dependently typed) implementation to Ionescus functional specification? 
	
	\item When having Cezar Ionescu as external examiner, this chapter will be of great influence as it deals heavily with his work.

\end{enumerate}

TODO: my hypothesis is that with a clean and rigorous pure functional implementation it would have been more likely to spot the bug as it would have been stated more explicitly but it would not be guaranteed to be avoided - the same is true for dependent types unless one focuses on getting this bit explicitly right but that would be unfair comparison. however, i hypothesize that with an in-depth property-based testing he could have avoided / found that mistake - and he should have done in-depth property-based testing (verification and validation) due to the fundamental importance of his undertaking and the implications of a positive outcome

Not yet started, need to implement it but there exists code for it already (gintis and java implementations)

%
%after re-reading ionescu paper: too complex and out of scope, but ionescu work more directly applicable in a pure functional implementation than in e.g. c++ (that was what they used).
%
%we base our implementation on the existing gintis code from https://people.umass.edu/gintis/ 
%also we make use of the \cite{evensen_extensible_2010} on gintis work which revealed a few bugs
%
%NOTE: my hypothesis is that just by having used our pure functional approach would NOT have prevented gintis to have made the bugs as reported in the masterthesis \cite{evensen_extensible_2010} because they seemed to be like copy-paste bugs. Only rigorous code-testing (unit- / property-based) would have probably revealed these problems.
%

\subsection{Agent Based Computational Economics}
For many models, our techniques introduced in Part II are too powerful and a much simpler approach would suffice to implement it. In general too much power should always be avoided (at least in programming and software engineering) because with much power comes much responsibility: more power requires to pay more attention to details and thus there is more potential to make mistakes. Thus we should always look for the technique with minimal power, which solves our problem sufficiently.

A very important field, which picked up ABS in recent years is economics. The field of economics is an immensely vast and complex one with many facets to it, ranging from firms, to financial markets to whole economies of a country \cite{bowles_understanding_2005}. Today its very foundations rest on rational expectations, optimization and the efficient market hypothesis. The idea is that the macroeconomics are explained by the micro foundations \cite{colell_microeconomic_1995} defined through behaviour of individual agents. These agents are characterized by rational expectations, optimizing behaviour, having perfect information, equilibrium \cite{focardi_is_2015}.
This approach to economics has come under heavy critizism in the last years for being not realistic, making impossible assumptions like perfect information, not being able to provide a process under which equilibrium is reached \cite{kirman_complex_2010} and failing to predict crashes like the sub-prime mortgage crisis despite all the promises - the science of economics is perceived to be detached from reality \cite{focardi_is_2015}. 
ACE is a promise to repair the empirical deficit which (neo-classic) economics seem to exhibit by allowing to make more realistic, empirical assumptions about the agents which form the micro foundations. The ACE agents are characterized by bounded rationality, local information, restricted interactions over networks and out-of-equilibrium behaviour \cite{farmer_economy_2009}. 
Works which investigate ACE as a discipline and discuss its methodology are \cite{tesfatsion_agent-based_2002}, \cite{richiardi_agent-based_2007}, \cite{ballot_agent-based_2015}, \cite{blume_introduction_2015}.
%look into computable economics book: \url{http://www.e-elgar.com/shop/computable-economics}
Tesfatsion \cite{tesfatsion_agent-based_2017} defines ACE as \textit{[...] computational modelling of economic processes (including whole economies) as open-ended dynamic systems of interacting agents.}. She gives a broad overview \cite{tesfatsion_agent-based_2006} of ACE, discusses advantages and disadvantages and giving the four primary objectives of it which are:

\begin{enumerate}
	\item Empirical understanding: why have particular global regularities evolved and persisted, despite the absence of centralized planning and control?
	\item Normative understanding: how can agent-based models be used as laboratories for the discovery of good economic designs?
	\item Qualitative insight and theory generation: how can economic systems be more fully understood through a systematic examination of their potential dynamical behaviours under alternatively specified initial conditions?
	\item Methodological advancement: how best to provide ACE researchers with the methods and tools they need to undertake the rigorous study of economic systems through controlled computational experiments?
\end{enumerate}

It is important to understand, that ACE utilises ABS different than the social sciences do. The latter one focuses more on agent-interactions, where in ACE the rational and non-rational actions of individual agents are more important. Thus in many ACE models, the full power of the techniques introduced in Part II is not required. More specifically, agents of ACE models tend to have much simpler state, behave often in only one specific way, don't use synchronised agent-interactions and are very rarely located in a spatial environment but focus more on network connections \cite{wilhite_economic_2006, glasserman_contagion_2015} or avoid the notion of connectivity altogether.

\medskip

To investigate this point more in-depth we implemented \footnote{Freely available at \url{https://github.com/thalerjonathan/zerointelligence}} a simulation with so called Zero Intelligence traders \cite{gode_allocative_1993}, inspired by an implementation in Python \footnote{\url{http://people.brandeis.edu/~blebaron/classes/agentfin/GodeSunder.html}}. We don't go into any technical detail here but the implementation drives the main points home:

\begin{itemize}
	\item Even though it is an agent-based model and there is a clear notion of agents in the Python code, where they are represented as objects, the agents are extremely simple. They are characterised by a single floating-point value, identifying how much value they attribute to an asset. Their behaviour is also very simple and does not change over time: they always bid randomly within their profit range. Thus we do \textit{not} implement agents as MSFs in this case but represent them indeed only through a \textit{Double} value, reducing the complexity of the implementation considerably.

	\item There are no direct agent-interactions. Although agents trade with each other, this happens through a central authority (the simulation kernel), which acts like a market with a limit order book. This reduces the complexity of the implementation considerably because there is no need for the full approach of synchronised direct agent-interactions. We could have implemented it in that way but that would have only increased complexity through the use of a quite powerful technique, which is actually not really needed because the same effect can be achieved in much simpler terms.

	\item There is no environment whatsoever and a fully connected network is implicitly assumed because each agent can trade with all other agents. This implies that the full technique of applying an environment is not necessary, which makes the simulation a lot less complex. Still adding an environment e.g. a network would be quite simple and does not require any monadic code as the network information can be made read-only in the way as we do in Chapter \ref{sec:adding_env}.
	
	\item The only side-effect necessary in this simulation is to draw random-numbers. By fixing the seed, repeated runs of initial conditions will always lead to same output, which is guaranteed at compile time. This was already shown in Part II and is a direct consequence of Haskells type-system and explicit way of dealing with effects.	 Further, we focused on keeping as much code \textit{pure} as possible thus splitting code which does not require random numbers into pure functions and only having the basic structure of the implementation running in the Rand Monad. This makes testing and reasoning considerably easier than running everything in the Rand Monad.
\end{itemize}

We are very well aware that this simple example is only one of many ACE models but even though it implements very simple \textit{zero} intelligence agents, it shows that ABS in Haskell does not need to be as complex as the use-cases in Part II - on the contrary, ABS implementations can be very concise and highly performant in Haskell.

\subsection{Gintis Bilateral Bartering}