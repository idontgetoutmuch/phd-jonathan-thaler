\subsection{Monadic SIR}
We can try to apply the same techniques of parallelising the agents as we did in the previous section in the non-monadic version of the SIR model. There is but a fundamental problem in this case, as we have already outlined in the section on data-flow parallelism: we are running the simulation in the monadic context of a ReaderT and Random Monad stack. In monadic execution, depending on the monad (stack), we deal with side-effects, which immediately necessitates the ordering of execution: whether an effectful expression is evaluated before another one can have indeed very fundamental differences and in general we have to assume that it does.
Indeed: the way the agents are evaluated is through the \textit{mapM} function, which evaluates them sequentially applying their side-effects in sequence. It does not matter that the agents behave as if they are run in parallel without the possibility to interfere with each other, the simple fact that they are run within the ReaderT(Rand g) transformer stack requires sequencing. It is not the ReaderT which causes the delicate issue, it is rather the Rand monad, which basically behaves like a State monad with the random-number generator as internal state, which gets updated with each draw.
Due to this sequential evaluation, we can hypothesize that our approach is doomed from the beginning and that we will not see any speed up  when we apply parallelism - on the contrary, we can expect the performance to be worse with it due to the overhead caused by it.

Indeed, when we put our hypotheses to a test \footnote{We used the same experiment setup as in the non-monadic implementation.} we see exactly that behaviour: the sequential implementation, which does not use any parallelism and is not compiled with the -threaded option takes on average 41.76 seconds to finish. When adding parallelism with evaluation strategies in the same way as we did in non-monadic SIR, we end up with 49.63 seconds on average to finish - a clear performance \textit{decrease}! For the Par monad approach its even worse, which averages at 52.98 seconds to finish. These timings clearly show that 1) agents which are run in a monadic context with \textit{mapM} are not applicable to parallelism, 2) the parallelism mechanisms add a substantial overhead which is in accordance with the reports in \cite{marlow_parallel_2013}.

Still we don't give up completely and want to see if running the agents sequentially but some Par monadic code \textit{within} them could gain us some speed up. The function we target is the neighbourhood querying function, which looks up the 8 (moore) surrounding neighbours of an agent. It is a pure function and uses \textit{map} and is thus perfectly suitable to parallelism. We simply extend the transformer stack by putting the Par monad innermost and then run the \textit{neighbours} function within the Par monad:

\begin{HaskellCode}
-- type simplified for explanatory reasons
neighbours :: Disc2dCoord -> SIREnv -> Par [SIRState]
neighbours (x, y) e = do
    ivs <- mapM (\c -> spawn (return (e ! c))) nCoords
    mapM get ivs
  where
    nCoords = ... -- create neighbours coordinates
\end{HaskellCode}

Unfortunately the performance is even worse than without it, averaging at 66.68 seconds to finish. The workload seems to be too low for parallelism to pay off. Further, when keeping the Par monad as innermost monad but using the original pure \textit{neighbours} function without Par we arrive at an average of 55.9 seconds to finish when running multi threaded on 8 cores and 45.56 seconds when compiled with threading enabled but running on a single core. These measurements demonstrate that using the Par monad and parallelism in general can lead to an impressively \textit{reduced} performance, compared to a sequential implementation, due do massive overhead and too fine-grained parallelism.

This leaves us basically without any options of parallelism for the monadic SIR model, we will come back to this use-case in the concurrency section, where we will show that by using concurrency it is possible to achieve a substantial speed up by orders of magnitude.