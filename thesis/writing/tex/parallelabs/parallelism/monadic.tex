\subsection{Monadic SIR}
We can try to apply the same techniques of parallelising the agents as we did in the previous section in the non-monadic version of the SIR model. There is but a fundamental problem in this case, as we have already outlined in the section on data-flow parallelism: we are running the simulation in the monadic context of a \textit{ReaderT} and \textit{Rand} Monad stack. In monadic execution, depending on the monad (stack), we deal with side effects, which immediately necessitates the ordering of execution: whether an effectful expression is evaluated before another one can have indeed very fundamental differences and in general we have to assume that it does.
Indeed: the way the agents are evaluated is through the \textit{mapM} function, which evaluates them sequentially applying their side effects in sequence. It does not matter that the agents behave as if they are run in parallel without the possibility to interfere with each other, the simple fact that they are run within the \textit{ReaderT (Rand g)} transformer stack requires sequencing. It is not the \textit{ReaderT} which causes the delicate issue, it is rather the \textit{Rand} Monad, which basically behaves like a \textit{State} Monad with the random-number generator as internal state, which gets updated with each draw.
Due to this sequential evaluation, we can hypothesise that our approach is doomed from the beginning and that we will not see any speedup  when we apply parallelism - on the contrary, we can expect the performance to be worse with it due to the overhead caused by it.

Indeed, when we put our hypotheses to a test \footnote{We used the same experiment setup as in the non-monadic implementation.} we see exactly that behaviour: the sequential implementation, which does not use any parallelism and is not compiled with the -threaded option takes on average 41.76 seconds to finish. When adding parallelism with evaluation strategies in the same way as we did in non-monadic SIR, we end up with 49.63 seconds on average to finish - a clear performance \textit{decrease}! For the \textit{Par} Monad approach its even worse, which averages at 52.98 seconds to finish. These timings clearly show that 1) agents which are run in a monadic context with \textit{mapM} are not applicable to parallelism, 2) the parallelism mechanisms add a substantial overhead which is in accordance with the reports in \cite{marlow_parallel_2013}.

Still we don't give up completely and want to see if running the agents sequentially but some \textit{Par} monadic code \textit{within} them could gain us some speedup. The function we target is the neighbourhood querying function, which looks up the 8 (Moore) surrounding neighbours of an agent. It is a pure function and uses \textit{map} and is thus perfectly suitable to parallelism. We simply extend the transformer stack by putting the \textit{Par} Monad innermost and then run the \textit{neighbours} function within the \textit{Par} Monad:

\begin{HaskellCode}
-- type simplified for explanatory reasons
neighbours :: Disc2dCoord -> SIREnv -> Par [SIRState]
neighbours (x, y) e = do
    ivs <- mapM (\c -> spawn (return (e ! c))) nCoords
    mapM get ivs
  where
    nCoords = ... -- create neighbours coordinates
\end{HaskellCode}

Unfortunately the performance is even worse than without it, averaging at 66.68 seconds to finish. The workload seems to be too low for parallelism to pay off. Further, when keeping the \textit{Par} Monad as outermost Monad but using the original pure \textit{neighbours} function without \textit{Par} we arrive at an average of 55.9 seconds to finish when running multi-threaded on 8 cores and 45.56 seconds when compiled with threading enabled but running on a single core. These measurements demonstrate that using the \textit{Par} Monad and parallelism in general can lead to a substantially \textit{reduced} performance, due do massive overhead and too fine-grained parallelism.

This leaves us basically without any options of parallelism for the monadic SIR model. Still, we will come back to this use case in the chapter on concurrency, where we will show that by using concurrency it is possible to achieve a substantial speedup even in monadic computations.