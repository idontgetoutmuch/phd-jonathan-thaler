\section{Data-flow parallelism}
When relying on a lazy data structure to apply parallelism is not an option, evaluation strategies as presented before are not applicable. Further, although lazy evaluation brings compositional parallelism, it makes it hard to reason about performance. Data-flow parallelism offers an alternative over evaluation strategies, where the programmer can give more details but gains more control: data dependencies are made explicit and reliance on lazy evaluation is avoided.
Data-flow parallelism is implemented through the \textit{Par} Monad, which provides combinators for expressing data-flows: in this monad it is possible to \textit{fork} parallel tasks which communicate with each other through shared locations, so called \textit{IVar}s. Internally these tasks are scheduled by a work-stealing scheduler which distributes the work evenly on available processors at runtime. \textit{IVars} behave like futures or promises: they are initially empty and can be written once. Reading from an empty \textit{IVar} will cause the calling task (or main thread) to wait until it is filled. An example is a parallel evaluation of two fibonacci numbers:

\begin{HaskellCode}
runPar (do
  i <- new             -- create new IVar
  j <- new             -- create new IVar
  fork (put i (fib n)) -- fork new task compute fib n and put result into IVar i
  fork (put j (fib m)) -- fork new task compute fib m and put result into IVar j
  a <- get i           -- wait for the result from IVar i and collect it
  b <- get j           -- wait for the result from IVar j and collect it
  return (a,b)         -- return the sum
\end{HaskellCode}

Note that with this it is also possible to express parallel evaluation of a list or a tuple as with evaluation strategies. The difference though is, that it does avoid lazy evaluation. More importantly, putting a value into an \textit{IVar} requires the type of the value to have an instance of the \textit{NFData} typeclass. This simply means that a value of this type can be fully evaluated, not just to WHNF but to evaluate the full expression the value represents.

\subsection{Data-flow parallelism in ABS}
NOTE: running the agents in parallel with par doesn't work because we use mapM and are thus monadic, which involves sequencing. so this is really out of the window here. Also we cannot put a Par in a transformer stack because the library doesn't support it, what actually makes sense. But we can do the following: we can run an agents MSF only within the Par monad which gives agents the ability to spawn data-flow parallel computations - random-number streams are handled like in the non-monadic version. Note that this is only possible with the MSFs of dunai and not the SF because the latter one adds already the a ReaderT DTime which makes it impossible already. 
What is actually possible would be to write a combined monad for Par and ReaderT because the latter one is a read-only value and could thus potentially run in parallel - we leave this for further research. There exists also a combination of the Par with the Rand monad, so if the time-driven approach is not needed then this could be used to give the agents the ability to both draw random numbers AND do deterministic data-parallel computations. The agents can then be run in parallel through the par monad.
