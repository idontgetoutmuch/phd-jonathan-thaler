\subsection{Parallel Runs}
Often one needs to perform a large number of runs of the same simulation. The most prominent use-cases for this are:

\begin{itemize}
	\item Parameter Sweeps / Variations - To explore the parameter space and the dynamics under varying parameter configurations, the same simulation is run with varying parameters and the results recorded for statistical analysis.
	
	\item Stochastic replications - Due to ABS stochastic nature, running a simulation only once does not allow to generalise or predict overall behaviour - one might have just hit an (un)fortunate special case. To counter this problem, in ABS multiple replications of the  simulation are run with same initial model parameters but with different random-number streams. All the results are collected and analysed stochastically (averaged, median,...) from which then more general properties can be derived.
\end{itemize}

In each case thousands of runs of the same simulation with different model parameters and / or varying random-number streams are needed, requiring a considerable amount of computing power.

Parallelism is a remedy to this problem because in each of these cases individual runs do not interfere with each other and thus can be seen as isolated from each other, like referential, pure computations. Our approaches shown in the Part II make this very explicit: the top level functions can always be made pure computations because we are ruling out IO (so far) and thus even though Monads are employed in many cases, they are still pure. A benefit of our approach is that it is guaranteed at compile time, that individual runs do not interfere with each other and thus there is no danger that parallel runs influence each other. 

All this allows to implement parameter sweeps and stochastic replications both through evaluation and data-flow parallelism making another very compelling use-case - probably the most striking one - for the use of parallelism in ABS. We hypothesize that data-flow parallelism is better suited for this task because it makes parallelism more explicit as it is indeed a data-flow problem: we pass parameters to single replications which are run and return their results. To apply this we simply run the top level replication logic in the Par Monad where replications are run in parallel by forking tasks and results are handed back through IVars. If we want the convenience of having a monadic random-number generator within the Par monad as well, one can use the combined ParRand monad which provides both.