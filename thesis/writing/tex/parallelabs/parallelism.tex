\section{Parallelism in ABS}
The promise of parallelism in Haskell is compelling: speeding up the execution but retaining all static compile-time guarantees about determinism. In other words, using parallelism could give us a substantial performance improvement without sacrificing the static guarantees of reproducible outputs from repeated runs with initial conditions.

Generally, parallelism can be applied whenever the execution of code is order-independent, that is referential transparent, and has no implicit or explicit side-effects. Without going into too much technical detail, in this section we outline the parallelism techniques available in Haskell and briefly discuss how they can be used in ABS in general. We also discuss if and how parallelism can be added to our previously discussed use-cases of Chapters \ref{sec:timedriven_firststep}, \ref{sec:adding_env} and Sugarscape \ref{sec:eventdriven_implementation} and report performance improvements where applicable.

\subsection{Parallelism in Haskell}
The fundamental concept Haskell uses to achieve parallelism is its own non-strictness nature. Non-strictness means that expressions are not eagerly evaluated when defined, like in imperative programming languages but only evaluated when their result is actually needed. This is implemented internally using thunks, which are pointers to expressions. When the value of an expression is needed, this thunk is accessed and the expression is reduced until the next constructor or lambda is encountered. This is called Weak Head Normal Form (WHNF) evaluation because it only reduces the "head" of the expression, which could consist of sub expressions. This indirection, the separation of data creation from consumption / evaluation, indeed enables parallelism and Haskell provides two additional functions to support this:

\begin{itemize}
	\item \textit{par :: a $\rightarrow$ b $\rightarrow$ b} Returns the second argument \textit{b} but evaluates the first argument \textit{a} in parallel. It is used when the result of evaluating \textit{a} is required later.
	
	\item \textit{seq :: a $\rightarrow$ b $\rightarrow$ b} - Returns the second argument \textit{b} but is strict in its first argument, which means it forces its evaluation to WHNF. It is used when the result of evaluating \textit{a} is required now.
\end{itemize}

Internally, parallel evaluation is handled through so called \textit{sparks}, which are basically thunks which get evaluated in parallel. The Haskell runtime system manages sparks and distributes them to threads where they get executed. Due to their extremely light-weight nature, it is no problem to create tens of thousands of sparks.

These basic concepts of non-strictness and the functions \textit{par} and \textit{seq} have been used to build up more abstractions for different parallelism concepts \cite{marlow_parallel_2013}. We briefly introduce the important ones together with their potential use in ABS. We refer to \cite{marlow_parallel_2013} for a more in-depth discussion of parallel Haskell.

\paragraph{Evaluation parallelism}
Evaluation parallelism introduces strategies to evaluate lazy data-structures in parallel. Examples are strategies to evaluate a list, or tuples in parallel where for each element a spark is created. One has to bear in mind that even though evaluating in parallel through sparks is extremely cheap, it still has some overhead. Thus, if the work-load of each element in a list is too low for a spark, then one can distribute chunks of a list onto a single spark.
It is important to understand, that all this works without side-effects - the strategy combinators are all pure functions building on \textit{par} and \textit{seq}. This allows us to add parallelism to an algorithm by applying a parallel evaluation strategy to its result which e.g. is a lazy list - again this is possible through non-strictness, which separates the construction of data from its consumption.

Using compositional parallelism is exactly what we use to aim at adding evaluation parallelism for agent execution in the non-monadic SIR example \ref{sec:timedriven_firststep}. We know that the whole simulation is a completely pure computation because Yampa is non-monadic, thus it is guaranteed that there are no side-effects - thus agents are run conceptually in parallel e.g. using map. Now we should be able to add parallelism without needing to re-implement \textit{dpSwitch} which is the function which runs the agents in parallel (Also re-implementing switch functions would not get us very far because of WHNF evaluation it is the wrong end to start parallel evaluation: probably only the arguments would be evaluated but not the agent behaviour.)

The solution is to add evaluation parallelism in the agent-output collection phase: where the recursive switch into the \textit{stepSimulation} function happens. There we use a evaluation strategy to evaluate the outputs of all agents in parallel. The agents will then be evaluated in parallel due to compositional parallelism, when we force the output of each in parallel. We give more details in the short case-study \ref{parallel_nonmonadic_sir} below.

\paragraph{Data-flow parallelism}
Very important: "In the previous two chapters, we looked at the Eval monad and Strategies, which work in conjunction with lazy evaluation to express parallelism. A Strategy consumes a lazy data structure and evaluates parts of it in parallel. This model has some advantages: it allows the decoupling of the algorithm from the parallelism, and it allows parallel evaluation strategies to be built compositionally. But Strategies and Eval are not always the most convenient or effective way to express parallelism. We might not want to build a lazy data structure, for example. Lazy evaluation brings the nice modularity properties that we get with Strategies, but on the flip side, lazy evaluation can make it tricky to understand and diagnose performance."
Its laziness which allows that.\\

Par Monad: how does it work? can express data-flow networks where tasks are forked and then results are synchronised. all this happens deterministically by building on the same mechanics the Eval monad is using thus technically speaking they are equivalent. 

can use both par and eval monad but which is applicable? par seems to require strict data, eval works on lazy data-structure. can we use both inside an msf? what is the Advantage over simple rpar or rseq?\\

%https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch04.html

\paragraph{Data-structure parallelism}
An environment could be organised and accessed through such a data-structure, which could potentially lead to big speed ups. Agents could locally read the data-structure data-parallel and the simulation kernel could feed the output of the agents data-parallel back into this structure.

%https://learning.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch05.html

general solution we opt for is  to run agents in parallel in our approaches. in other abs models we could apply data-structure parallelism and/or data-flow parallelism with huge Performance potential but thats always highly model dependent thus we dont go in depth here

\subsection{Case-Studies}
In this section we go a little bit more into detail how we applied the parallelism concepts as already outline above to our use-cases from Chapters \ref{sec:timedriven_firststep}, \ref{sec:adding_env} and Sugarscape \ref{sec:eventdriven_implementation}. We only show briefly the technical details and refer to the full code in footnotes. Note that all timings are rough averages over multiple runs and not precise measurements because that is not the point here. We are only interested in showing what rough potential there is for speeding up computation through deterministic parallelism - we are not interested in high performance computation here but rather in conceptual comparisons between sequential and parallel implementations.

\subsubsection{Non-Monadic SIR}
\label{parallel_nonmonadic_sir}
As outlined above we want to apply parallelism to agent evaluation by composing the output with parallel evaluation by slightly changing the function \textit{switchingEvt}. This function receives the output of all agents from the current simulation step and generates an event to recursively switch back into \textit{stepSimulation} to compute the next simulation step. The code is as follows:

\begin{HaskellCode}
switchingEvt :: SF ((), [SIRState]) (Event [SIRState])
switchingEvt = arr (\ (_, newAs) -> parEvalAgents newAs)
  where
    -- NOTE: need a seq here otherwise would lead to GC'd sparks because
    -- the main thread consumes the output already when aggregating, so using seq 
    -- will force parallel evaluation at that point 
    parEvalAgents :: [SIRState] -> Event [SIRState]
    parEvalAgents newAs = newAs' `seq` Event newAs' 
      where
        -- NOTE: chunks of 200 agents seem to deliver the best performance
        -- when we are purely CPU bound and don't have any IO
        newAs' = withStrategy (parListChunk 200 rseq) newAs
        -- NOTE: alternative is to run every agent in parallel
        -- only use when IO of simulation output is required
        -- newAs' = withStrategy (parList rseq) newAs
\end{HaskellCode}

Which evaluation strategy resulted in the best performance increase turned out to depend on how we observe the results of the simulation. Due to Haskells non-strict nature, as long as no output is \textit{observed}, nothing would get computed ever. We have developed three (3) different ways to observe the output of this simulation and thus we measured the timings for all of them:

\begin{enumerate}
	% Parallel : 3.86, 3.77, 3.87, 3.83, 4.13, 3.77, 3.88, 4.15 = 3.9 (0.15 std)
	% Sequential: 16.74, 16.54, 16.69, 16.33, 16.68, 16.42, 16.57, 16.38 = 16.54 (0.15 std)
	% Factor = 4.24
	
	\item Printing the output of the last simulation step. This requires to run the simulation for the whole 150 time-steps because each step depends on the output of the previous one. Because the simulation is completely CPU bound, the best performance increase turned out to run agents in batches where for this model 200 seems to deliver the best performance. If each agent is run in parallel, we still achieved a substantial performance increase but not as high as the batched version. An analysis showed that around 1.5 million (!) sparks got created but most of them were never evaluated. There is a limit in the spark pool and we have obviously hit that.
	
	% Parallel: 9.37, 9.18, 9.2, 9.2, 9.3, 9.7, 9.95, 9.44 = 9.4175 (0.27541 std)
	% Sequential: 10.13, 10.42, 10.2, 10.12, 10.0, 10.2, 10.1, 10.2 = 10.171 (0.12135 std)
	% Factor = 1.08
	\item Writing the aggregated output of the whole simulation to an export file. This requires in principle to run the simulation through but due to non-strictness, the writing to the export file begins straight away. This interferes with parallelism due to system calls which get interleaved with parallelism, leading to less performance increase than the previous one. It turned out that in this case running each agent in parallel didn't lead to reduced performance, because we are IO bound (see below).
	
	% Parallel: 9.24, 9.44, 9.35, 9.61, 10.16, 10.45, 10.25, 9.4 =  9.7375 (0.47286 std)
	% Sequential: 10.07, 10.05, 10.0, 10.03, 10.04, 9.95, 10.06, 10.144 = 10.043 (0.055990 std)
	% Factor = 1.03
	\item Appending the aggregated output of the current step to an export file. This is necessary when we have a very long running simulation for which we want to write each step out (more or less) as soon as it is computed. The function which runs this simulation is tail-recursive and can thus run forever, which is not possible in the previous case where the function is not necessarily tail-recursive and aggregates the outputs. Here we use a strategy which evaluates each agent in parallel as well.
\end{enumerate}

The timings are reported in Table \ref{tab:parallel_nonmonadic_sir_timings}. All timings were measured with 1000 agents running for 150 time-steps, and $\Delta = 0.1$. We performed 8 runs and report the timings in seconds. The parallel version was compiled with the '-threaded' option and used all 8 cores with the '-N' option. For the sequential implementation the '-threaded' option was removed as well as the evaluation strategies - it is purely sequential code. All experiments were carried out on the same machine \footnote{Dell XPS 13 (9370) with Intel Core i7-8550U (8 cores), 16 GB Ram (plugged in).}

\begin{table}
	\centering
	\begin{tabular}{ c || c | c | c }
		Output type                   & Parallel & Sequential & Factor \\ \hline
		Print of last step (1)        & 3.9      & 16.38      & 4.24 \\ \hline
		Writing simulation output (2) & 9.41     & 10.17      & 1.08 \\ \hline
		Appending current step (3)    & 9.73     & 10.04      & 1.03 \\ \hline
	\end{tabular}
	
	\caption{Timings of parallel vs. sequential non-monadic SIR.}
	\label{tab:parallel_nonmonadic_sir_timings}
\end{table}

The table clearly indicates, that in case we are purely CPU bound we get a quite impressive speed up of 4.24 on 8 cores - parallelism clearly pays off here, especially after it is so easy to add. On the other hand it seems that as soon as we are IO bound, the parallelism performance benefit is completely wasted. This does not come as a surprise and it is well established that generally as soon as IO is involved, performance benefits from parallelism will suffer. This point will be addressed by the use of concurrency where due to concurrent evaluation the IO is decoupled from the computation, making the latter one completely CPU bound and resulting in an impressive speed-up in such a case as well.

What comes a bit as a surprise is that in the case of the sequential implementation, the CPU bound implementation, which does no IO is actually slower than the ones which do IO. This can be attributed to lazy evaluation which seems to increase performance . TODO why?

% NOTE: THESE ARE OLD COMMENTS, MADE OBSOLETE BECAUSE I MADE IT ACTUALLY WORK
%Inspired by the work of \cite{perez_60_2014}, which shows the potential of speeding up real-world Haskell programs using Yampa We conducted a comparison of an implementation which makes use of evaluation parallelism to run agents in parallel.
%
%OK, rephrase: compare performance of non-parallel implementation WITH threaded an -N option to non-parallel implementation without threaded and / or N1 to make sure that no performance improvement happens automatically by using threaded e.g. GCs or something else...
%I observed the behaviour in the following code: \url{https://github.com/thalerjonathan/phd/tree/master/public/purefunctionalepidemics/code/SIR_Yampa}
%
%I analysed a bit more using the threadscope tool. I ran the same program twice with different ghc-options:
%1. -O2 -Wall -Werror -eventlog 
%2. -O2 -Wall -Werror -eventlog -threaded -with-rtsopts=-N
%
%When looking at the event logs with threadscope it becomes appartent, that parallel garbage collection is the cause of the CPU usage above 100%:
%-  In the single-threaded case 0 sparks are created and everything runs indeed only on one core. There are two Garbage Collectors (Gen0 and Gen1) but nothing runs in parallel (Par collections are 0 for both).
%- In the multi-threaded case also 0 sparks are created but now 8 cores are used: all 'running' activity happens on only 1 core as expected but garbage collection happens on all 8 cores: the diagrams and the number of Par collections clearly indicates that. The time spent on parallel GC work is 10.76% (0 is completely serial and 100% is completely parallel).
%
%Now when we compare the timing between both runs we see the following: 
%- single-threaded: 11.68s total, 7.35s mutator, 4.34s GC,
%- multi-threaded: 10.70s total, 7.03s mutator, 3.68s GC
%
%This adds up: the ~ 10\% of parallel GC work done in multi-threaded are also the ~ 10\% it is faster over the single-threaded one. Of course I only did a single run in each case but I think the analysis is still valid and the point was made: when running a Haskell program which does not use any parallel features, running it with the -threaded option can lead to an increase in performance due to parallel GC.
%
%% https://www.reddit.com/r/haskell/comments/2jbl78/from_60_frames_per_second_to_500_in_haskell/\\
%
%Our use case:
%Unfortunately in our non-monadic Yampa implementation we see a negligible speedup of less than 10\% between running it on 1 or 8 cores and this difference is probably due to garbage collection. When analysing the problem more in-depth it becomes clear that 50\% of the parallel evaluation sparks (todo explain) are duplications and get never evaluated, which is due to the thunk being already evaluated before thus no need to run it actually in parallel. Unfortunately this seems reasonable in this example: the way the agent-behaviour is implemented forces the values, including the output, due to lots of comparisons, which results basically in a strict behaviour with the output already evaluated for many agents. It seems that it depends on the current state the agent is in otherwise we could not explain why some sparks are duplications and others not. Further it seems, that although work happens in parallel, the overhead eats up the benefit and thus we arrive at roughly the same performance of the non-parallel version. This might be completely different for much more computational intensive agent behaviour with a more complex agent-output data-structure - but we leave this for further research.

\subsubsection{Monadic SIR}
Unfortunately \textit{parMap} is not applicable to the monadic SIR version of Chapter \ref{sec:adding_env} because of the use of mapM, which cannot be replaced due to its inherent sequential nature: mapM runs monadic actions which have side-effects thus ordering matters. Even if the implementation in that chapter behaves as if the agents are run in parallel, technically they are run sequentially because of the need for the Random Monad effect. This leaves us basically without any options of parallelism for the monadic SIR model, we will come back to this use-case in the concurrency section, where we will show that by using concurrency it is possible to achieve a substantial speed up of orders of magnitude.

We can try to do the same as we did in the non-monadic SIR version but we can expect our approach to be doomed already. Although we collect a list of agent-outputs in the end, the way it is constructed is not lazy but sequential due to monadic execution. Thus we expect to see no speedup but a decreasing of performance due to overhead of parallelism. And indeed: the parallel version runs at the same speed as the non-parallel one. The reason is as follows: the list which is consumed by the mechanism for evaluation parallelism is indeed a lazy one but the elements are created sequentially one after another due to the monadic sequencing of mapM. 
Thus even though parallelism is happening it is of no use because the elements are already in whnf

TODO: the marlow book says: don't do repeated calls to runPar, so although the agent can in fact do that it should avoid it and if there is heavy parallel work in each agent then one should consider running the agent in the par monad with a single runPar outside

NOTE: running the agents in parallel with par doesn't work because we use mapM and are thus monadic, which involves sequencing. so this is really out of the window here. Also we cannot put a Par in a transformer stack because the library doesn't support it, what actually makes sense. But we can do the following: we can run an agents MSF only within the Par monad which gives agents the ability to spawn data-flow parallel computations - random-number streams are handled like in the non-monadic version. Note that this is only possible with the MSFs of dunai and not the SF because the latter one adds already the a ReaderT DTime which makes it impossible already. 
What is actually possible would be to write a combined monad for Par and ReaderT because the latter one is a read-only value and could thus potentially run in parallel - we leave this for further research. There exists also a combination of the Par with the Rand monad, so if the time-driven approach is not needed then this could be used to give the agents the ability to both draw random numbers AND do deterministic data-parallel computations. The agents can then be run in parallel through the par monad.

TODO: try the same thing as in the non-monadic SIR: parMap rpar id evaluating the output. Hypothesis is that it should not show any parallelism because of monadic code. Indeed it does not show any speedup

\subsubsection{Sugarscape}
The same case as in the monadic SIR: running the agents with evaluation or data-flow parallelism is not possible in a monadic context  \textit{in general}. We have shortly discussed how it could be achieved in specific circumstances where then agents are running in the Par monad only, but this is highly model specific and for the Sugarscape this approach does not work. 

There is though one tiny thing we could optimise.

use parmap for updating Pollution/regrow resources. still agents can't be run in parallel because of monadic effects, we show in the concurrent section how we can use concurrency to achieve a substantial speed up using STM.

compare Environment parallelism between sequential and concurrent sugarscape: should see alarger speedup in conc bcs the sequential percentage is larger there

unfortunately its a Map datastructure, so we cannot operate in parallel e.g. map. but we can compute pollution because it uses map

\input{./tex/parallelabs/parallelism/parallelruns.tex}

\input{./tex/parallelabs/parallelism/reflection.tex}