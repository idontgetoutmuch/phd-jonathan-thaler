\section{Parallelism in ABS}
The promise of parallelism in Haskell is compelling: speeding up the execution but retaining all static compile-time guarantees about determinism. In other words, using parallelism could give us a substantial performance improvement without sacrificing the static guarantees of reproducible outputs from repeated runs with initial conditions.

Generally, parallelism can be applied whenever the execution of code is order-independent, that is referential transparent, and has no implicit or explicit side-effects. Without going into too much technical detail, in this section we outline the parallelism techniques available in Haskell and briefly discuss how they can be used in ABS in general. We also discuss if and how parallelism can be added to our previously discussed use-cases of Chapters \ref{sec:timedriven_firststep}, \ref{sec:adding_env} and Sugarscape TODO and report the performance improvements where applicable.

\subsection{Parallelism in Haskell}
parallelism in haskell builds on laziness

We follow the book \cite{marlow_parallel_2013}, which can be seen as the main source for parallelism and concurrency in Haskell and refer to it for an in-depth discussions of parallel Haskell.

\paragraph{Evaluation parallelism}
The basis are the following functions: \textit{rpar :: a -> Eval a} and \textit{rseq :: a -> Eval a}, where Eval is a Monad which can be run with \textit{runEval :: Eval a -> a}. rpar runs the computation in parallel and immediately returns without waiting for the evaluation of the thunk - this will happen behind the sences. rseq runs the computation in parallel as well but waits for the evaluation to WFNF. Using this we can start evaluating multiple expressions in parallel with rpar and then wait for their result with rseq. Note that both cases evaluate their argument to weak head normal form (WHNF), thus if the argument is already in WHNF, then the computation does nothing. This becomes important when to understand how far we can to in evaluation of parallelism. TODO: need to say a bit about haskell as a lazy language. TODO: isnt this all the very basics of haskells parallelism together with lazy evaluation?

Put short, evaluation parallelism allows to build functions which run in parallel e.g. a parallel version of \textit{map}, which is called \textit{parMap}. This is achieved using the \textit{Eval Monad} which is run using a parallel evaluation strategy, arriving at a pure value - the evaluation of the \textit{Eval Monad} itself is pure and does not require the IO (this is exactly what we expect from parallelism: to be deterministic). Obviously this gives huge potential for speeding up programs because maps are omnipresent in a lot of functional code. Not only parmap! explain a little bit more in detail without going into too much technical stuff.


Very important: "In the previous two chapters, we looked at the Eval monad and Strategies, which work in conjunction with lazy evaluation to express parallelism. A Strategy consumes a lazy data structure and evaluates parts of it in parallel. This model has some advantages: it allows the decoupling of the algorithm from the parallelism, and it allows parallel evaluation strategies to be built compositionally. But Strategies and Eval are not always the most convenient or effective way to express parallelism. We might not want to build a lazy data structure, for example. Lazy evaluation brings the nice modularity properties that we get with Strategies, but on the flip side, lazy evaluation can make it tricky to understand and diagnose performance."
Its laziness which allows that.


%https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch02.html
%https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch03.html

\paragraph{Data-flow parallelism}
Par Monad: how does it work? can express data-flow networks where tasks are forked and then results are synchronised. all this happens deterministically by building on the same mechanics the Eval monad is using thus technically speaking they are equivalent. 

can use both par and eval monad but which is applicable? par seems to require strict data, eval works on lazy data-structure. can we use both inside an msf? what is the Advantage over simple rpar or rseq?\\

%https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch04.html

\paragraph{Data-structure parallelism}
An environment could be organised and accessed through such a data-structure, which could potentially lead to big speed ups. Agents could locally read the data-structure data-parallel and the simulation kernel could feed the output of the agents data-parallel back into this structure.

%https://learning.oreilly.com/library/view/parallel-and-concurrent/9781449335939/ch05.html

general solution we opt for is  to run agents in parallel in our approaches. in other abs models we could apply data-structure parallelism and/or data-flow parallelism with huge Performance potential but thats always highly model dependent thus we dont go in depth here

\subsection{Use-Cases}

\subsubsection{Non-Monadic SIR}
Although \textit{parMap} can be applied in all cases where a map us used, we are particularly interested in running agents in parallel. With \textit{parMap} this should become possible in our non-monadic SIR implementation built on Yampa from Chapter \ref{sec:timedriven_firststep}. Even thought the Eval Monad is used under the hood and Yampa is non-monadic, it is still applicable because running the monad is pure, resulting in a pure result - \textit{parMap} is a pure function. TODO: how can we apply this?

Inspired by the work of \cite{perez_60_2014}, which shows the potential of speeding up real-world Haskell programs using Yampa We conducted a comparison of an implementation which makes use of evaluation parallelism to run agents in parallel.

OK, rephrase: compare performance of non-parallel implementation WITH threaded an -N option to non-parallel implementation without threaded and / or N1 to make sure that no performance improvement happens automatically by using threaded e.g. GCs or something else...
I observed the behaviour in the following code: https://github.com/thalerjonathan/phd/tree/master/public/purefunctionalepidemics/code/SIR_Yampa

I analysed a bit more using the threadscope tool. I ran the same program twice with different ghc-options:
1. -O2 -Wall -Werror -eventlog 
2. -O2 -Wall -Werror -eventlog -threaded -with-rtsopts=-N

When looking at the event logs with threadscope it becomes appartent, that parallel garbage collection is the cause of the CPU usage above 100%:
-  In the single-threaded case 0 sparks are created and everything runs indeed only on one core. There are two Garbage Collectors (Gen0 and Gen1) but nothing runs in parallel (Par collections are 0 for both).
- In the multi-threaded case also 0 sparks are created but now 8 cores are used: all 'running' activity happens on only 1 core as expected but garbage collection happens on all 8 cores: the diagrams and the number of Par collections clearly indicates that. The time spent on parallel GC work is 10.76% (0 is completely serial and 100% is completely parallel).

Now when we compare the timing between both runs we see the following: 
- single-threaded: 11.68s total, 7.35s mutator, 4.34s GC,
- multi-threaded: 10.70s total, 7.03s mutator, 3.68s GC

This adds up: the ~ 10\% of parallel GC work done in multi-threaded are also the ~ 10\% it is faster over the single-threaded one. Of course I only did a single run in each case but I think the analysis is still valid and the point was made: when running a Haskell program which does not use any parallel features, running it with the -threaded option can lead to an increase in performance due to parallel GC.

% https://www.reddit.com/r/haskell/comments/2jbl78/from_60_frames_per_second_to_500_in_haskell/\\

The idea:
Using compositional parallelism we can add evaluation parallelism for agent execution, without needing to re-implement dpSwitch. Also re-implementing switch functions would not get us very far because of WHNF evaluation it is the wrong end to start parallel evaluation: probably only the arguments would be evaluated but not the agent behaviour. The solution is to add evaluation parallelism in the agent-output collection phase: where the recursive switch into the step Simulation function happens. There we use a parMap to evaulate the outputs of all agents in parallel simply using a parMap with id, which due to compositional parallelism because of lazy evaluation, should then run the whole agent when it is evaluated in parallel because the output is forced in parallel evaluation.

Our use case:
Unfortunately in our non-monadic Yampa implementation we see a negligible speedup of less than 10\% between running it on 1 or 8 cores and this difference is probably due to garbage collection. When analysing the problem more in-depth it becomes clear that 50\% of the parallel evaluation sparks (todo explain) are duplications and get never evaluated, which is due to the thunk being already evaluated before thus no need to run it actually in parallel. Unfortunately this seems reasonable in this example: the way the agent-behaviour is implemented forces the values, including the output, due to lots of comparisons, which results basically in a strict behaviour with the output already evaluated for many agents. It seems that it depends on the current state the agent is in otherwise we could not explain why some sparks are duplications and others not. Further it seems, that although work happens in parallel, the overhead eats up the benefit and thus we arrive at roughly the same performance of the non-parallel version. This might be completely different for much more computational intensive agent behaviour with a more complex agent-output data-structure - but we leave this for further research.

\subsubsection{Monadic SIR}
Unfortunately \textit{parMap} is not applicable to the monadic SIR version of Chapter \ref{sec:adding_env} because of the use of mapM, which cannot be replaced due to its inherent sequential nature: mapM runs monadic actions which have side-effects thus ordering matters. Even if the implementation in that chapter behaves as if the agents are run in parallel, technically they are run sequentially because of the need for the Random Monad effect. This leaves us basically without any options of parallelism for the monadic SIR model, we will come back to this use-case in the concurrency section, where we will show that by using concurrency it is possible to achieve a substantial speed up of orders of magnitude.

TODO: the marlow book says: don't do repeated calls to runPar, so although the agent can in fact do that it should avoid it and if there is heavy parallel work in each agent then one should consider running the agent in the par monad with a single runPar outside

NOTE: running the agents in parallel with par doesn't work because we use mapM and are thus monadic, which involves sequencing. so this is really out of the window here. Also we cannot put a Par in a transformer stack because the library doesn't support it, what actually makes sense. But we can do the following: we can run an agents MSF only within the Par monad which gives agents the ability to spawn data-flow parallel computations - random-number streams are handled like in the non-monadic version. Note that this is only possible with the MSFs of dunai and not the SF because the latter one adds already the a ReaderT DTime which makes it impossible already. 
What is actually possible would be to write a combined monad for Par and ReaderT because the latter one is a read-only value and could thus potentially run in parallel - we leave this for further research. There exists also a combination of the Par with the Rand monad, so if the time-driven approach is not needed then this could be used to give the agents the ability to both draw random numbers AND do deterministic data-parallel computations. The agents can then be run in parallel through the par monad.

TODO: try the same thing as in the non-monadic SIR: parMap rpar id evaluating the output. Hypothesis is that it hould not show any parallelism because of monadic code.

\subsubsection{Sugarscape}
The same case as in the monadic SIR: running the agents with evaluation or data-flow parallelism is not possible in a monadic context  \textit{in general}. We have shortly discussed how it could be achieved in specific circumstances where then agents are running in the Par monad only, but this is highly model specific and for the Sugarscape this approach does not work. 

There is though one tiny thing we could optimise.

use parmap for updating Pollution/regrow resources. still agents can't be run in parallel because of monadic effects, we show in the concurrent section how we can use concurrency to achieve a substantial speed up using STM.

compare Environment parallelism between sequential and concurrent sugarscape: should see alarger speedup in conc bcs the sequential percentage is larger there

unfortunately its a Map datastructure, so we cannot operate in parallel e.g. map. but we can compute pollution because it uses map

\input{./tex/parallelabs/parallelism/parallelruns.tex}

\input{./tex/parallelabs/parallelism/reflection.tex}