\chapter*{Introduction}
\label{ch:property}

When implementing an Agent-Based Simulation (ABS) it is of fundamental importance that the implementation is correct up to some specification and that this specification matches the real world in some way. This process is called verification and validation (V\&V), where \textit{validation} is the process of ensuring that a model or specification is sufficiently accurate for the purpose at hand whereas \textit{verification} is the process of ensuring that the model design has been transformed into a computer model with sufficient accuracy \cite{robinson_simulation:_2014}. In other words, validation determines if we are we building the \textit{right model}, and verification if we are building the \textit{model right} up to some specification \cite{balci_verification_1998}.

% there is no general validity, an approach is TDD: V&V particularly difficult in ABS
One can argue that ABS should require more rigorous programming standards than other computer simulations \cite{polhill_ghost_2005}. The fact that researchers in ABS are looking for an emergent behaviour in the dynamics of the simulation, they are always tempted to look for surprising behaviour and expect something unexpected from their simulation. 
Also, due to ABS' \textit{constructive / exploratory} nature \cite{epstein_chapter_2006, epstein_generative_2012}, there exists some uncertainty about the dynamics the simulation will produce before running it. The authors \cite{ormerod_validation_2006} see the current process of building ABS as a discovery process where models of an ABS often lack an analytical solution in general, which makes verification much harder if there is no such solution. Thus it is often very difficult to judge whether an unexpected outcome can be attributed to the model or has in fact its roots in a subtle programming error \cite{galan_errors_2009}.

In general this implies that it is not possible to prove that a model is valid in general but that the best we can do is to \textit{raise the confidence} in the correctness of the simulation. Therefore, the process of V\&V is not the proof that a model is correct but it is the \textit{process} of trying to show that the model is \textit{not incorrect}. The more checks one carries out which show that it is not incorrect, the more confidence we can place in the models validity. To tackle such a problem in software, software engineers have developed the concept of test-driven development (TDD).

Test-Driven Development (TDD) was popularised in the early 00s by Kent Beck \cite{beck_test_2002} as a way to a more agile approach to software-engineering, where instead of doing each step (requirements, implementation, testing,...) as separated from each other, all of them are combined in shorter cycles. Put shortly, in TDD tests are written for each feature before actually implementing it, then the feature is fully implemented and the tests for it should pass. This cycle is repeated until the implementation of all requirements has finished. Traditionally TDD relies on so called unit tests which can be understood as a piece of code which when run isolated, tests some functionality of an implementation. Thus we can say that test-driven development in general and unit testing together with code-coverage in particular, guarantee the correctness of an implementation to some informal degree, which has been proven to be sufficiently enough through years of practice in the software industry all over the world. 

\medskip

\section*{Related Work}
The work \cite{collier_test-driven_2013} was the first to discuss how to apply TDD to ABS, using unit testing to verify the correctness of the implementation up to a certain level. They show how to implement unit tests within the RePast Framework \cite{north_complex_2013} and make the important point that such a software needs to be designed to be sufficiently modular otherwise testing becomes too cumbersome and involves too many parts. The paper \cite{asta_investigation_2014} discusses a similar approach to DES in the AnyLogic software toolkit. 

The paper \cite{onggo_test-driven_2016} proposes Test Driven Simulation Modelling (TDSM) which combines techniques from TDD to simulation modelling. The authors present a case study for maritime search-operations where they employ ABS. They emphasise that simulation modelling is an iterative process, where changes are made to existing parts, making a TDD approach to simulation modelling a good match. They present how to validate their model against analytical solutions from theory using unit tests by running the whole simulation within a unit test and then perform a statistical comparison against a formal specification. %This approach will become of importance later on in our SIR and Sugarscape case studies.

The paper \cite{gurcan_generic_2013} gives an in-depth and detailed overview over verification, validation and testing of agent-based models and simulations and proposes a generic framework for it. The authors present a generic UML class model for their framework which they then implement in the two ABS frameworks RePast and MASON. Both of them are implemented in Java and the authors provide a detailed description how their generic testing framework architecture works and how it utilises unit testing with JUnit to run automated tests. To demonstrate their framework they provide also a case study of an agent-base simulation of synaptic connectivity where they provide an in-depth explanation of their levels of test together with code.

\section*{Towards Property-Based Testing}
According to \cite{diehl_what_nodate}, unit testing in Haskell is quite common and robust. Although generally speaking unit tests tend to be of less importance in Haskell since the type system makes an enormous amount of invalid programs completely inexpressible by construction. Unit tests tend to be written later in the development lifecycle and generally tend to be about the core logic of the program and not the intermediate plumbing \cite{diehl_what_nodate}. Although it would be interesting to see how we can apply unit testing to our approach, it is straight forward, nothing new and does not constitute unique research. 

Thus, in this chapter we introduce an additional technique for TDD: \textit{property-based testing}, which can be seen complementary to unit testing. Property-based testing has its origins in Haskell \cite{claessen_quickcheck_2000,claessen_testing_2002,runciman_smallcheck_2008}, where it was first conceived and implemented. It has been successfully used for testing Haskell code for years and also been proven to be useful in the industry \cite{hughes_quickcheck_2007}. We show and discuss how this technique can be applied to test pure functional ABS implementations. To our best knowledge property-based testing has never been looked at in the context of ABS and this thesis is the first one to do so.

\medskip

The main idea of property-based testing is to express model-specifications and laws directly in code and test them through \textit{automated} and \textit{randomised} test-data generation. Thus one hypothesis of this thesis is that due to ABS \textit{stochastic} and \textit{exploratory / generative / constructive } nature, property-based testing is a natural fit for testing ABS in general and pure functional ABS implementations in particular. It thus should pose a valuable addition to the already existing testing methods in this field, worth exploring.

To substantiate and test our hypothesis, we conducted a few case-studies. First, we look into how to express and test agent specifications for both the time- and event-driven SIR implementations in Chapter \ref{ch:agentspec}. Then we show how to encode model invariants of the SIR implementation and validate it against the formal specification from SD using property-tests in Chapters \ref{ch:sir_invariants} and \ref{ch:prop_sirspec}. We also show briefly how to use property-tests for hypothesis testing in the context of the exploratory Sugarscape model for which no ground-truth exists in Chapter \ref{ch:prop_exploratory}. We conclude with a deeper discussion on the connection between an equilibrium of a model and the totality of its simulation implementation in Chapter \ref{ch:equilibrium_totality}.
Note that we explicitly exclude obvious applications of property-testing like boundary-checks of the environment, helper functions of agents,... as although they are used within ABS, there is nothing new in testing them.
%Again, we emphasise that we see it as an addition to TDD, where it works in combination with unit testing to verify and validate a simulation to increase the confidence in its correctness. % and is a useful tool for expressing regression tests.

\medskip

Note that property-based testing has a close connection to model-checking \cite{mcmillan_symbolic_1993}, where properties of a system are proved in a formal way. The important difference is that the checking happens directly on code and not on the abstract, formal model, thus one can say that it combines model-checking and unit testing, embedding it directly in the software-development and TDD process without an intermediary step. We hypothesise that adding it to the already existing testing methods in the field of ABS is of substantial value as it allows to cover a much wider range of test-cases due to automatic data generation. This can be used in two ways: to verify an implementation against a formal specification and to test hypotheses about an implemented simulation. This puts property-based testing on the same level as agent- and system testing, where not technical implementation details of e.g. agents are checked like in unit tests but their individual complete behaviour and the system behaviour as a whole.

The work \cite{onggo_test-driven_2016} explicitly mentions the problem of test coverage which would often require to write a large number of tests manually to cover the parameter ranges sufficiently enough - property-based testing addresses exactly this problem by \textit{automating} the test-data generation. Note that this is closely related to data-generators \cite{gurcan_generic_2013}, load generators and random testing \cite{burnstein_practical_2010}. Property-based testing though goes one step further by integrating this into a specification language directly into code, emphasising a declarative approach and pushing the generators behind the scenes, making them transparent and focusing on the specification rather than on the data-generation. 