\chapter{Testing the SIR model specification}
\label{ch:prop_explanatory}

TODO REFINE WEAK ARGUMENTS
- FOCUS ON event-driven SIR and only mention time-driven that it delivers same results.
- argue a bit better why we fail randomly: the lists dont show no pattern despite using shrink feature
- statistical argument in SIR not convincing yet. why do we use a t-test when we know its bimodal already? also it seems all a bit hacky, restructure the Argumentation. maybe we can achieve better comparison with a different statistical test: look into graziellas paper. are there tests for bimodal distributions? maybe its enough to say that they generate different distributions
- read graziellas paper: \cite{figueredo_comparing_2014}
- read Comparing Simulation Output Accuracy of Discrete Event and Agent Based Models: A Quantitative Approach. in Dropbox/Phd/Papers
- read Deterministic epidemiological models at the individual level in Dropbox/Phd/Papers
- read Building epidemiological models from R 0 : an implicit treatment of transmission in networks in Dropbox/Phd/Papers
- could use DEVS as underlying specification for an approach for quickcheck testing?

TODO: replace maxFailPercent with cover and checkCoverage and configure quickcheck to run only 10 tests

In the previous chapters we have established the correctness of our event- and time-driven implementation up to our informal specification, we derived from the formal SD specification from Chapter \ref{sec:sir_model}. What we are lacking is a verification whether the implementations also match the formal SD specification or not. In the process of verification, we need to make sure it is correct up to some specification. We aim at connecting the agent-based implementation to the SD specification, by formalising it into properties within a property-test. The SD specification can be given through the differential equations shown in Chapter \ref{sec:sir_model}, which we repeat here:

\begin{equation}
\begin{split}
\frac{\mathrm d S}{\mathrm d t} = -infectionRate \\
\frac{\mathrm d I}{\mathrm d t} = infectionRate - recoveryRate \\
\frac{\mathrm d R}{\mathrm d t} = recoveryRate 
\end{split}
\quad
\begin{split}
infectionRate = \frac{I \beta S \gamma}{N} \\
recoveryRate = \frac{I}{\delta} 
\end{split}
\end{equation}
\label{eq:sir_delta_rates}

Solving these equations is done by integrating over time. In the SD terminology, the integrals are called \textit{Stocks} and the values over which is integrated over time are called \textit{Flows}. At $t = 0$ a single agent is infected because if there wouldn't be any infected agents, the system would immediately reach equilibrium - this is also the formal definition of the steady state of the system: as soon as $I(t) = 0$ the system won't change any more.

\begin{align}
S(t) &= N - I(0) + \int_0^t -infectionRate\, \mathrm{d}t \\
I(0) &= 1 \\
I(t) &= \int_0^t infectionRate - recoveryRate\, \mathrm{d}t \\
R(t) &= \int_0^t recoveryRate\, \mathrm{d}t
\end{align}

\section{Deriving the properties}
The key to encode these specifications into a property is to understand that the stocks of \textit{S}, \textit{I} and \textit{R} change \textit{per time-unit} by the given rates. This means that if we run an SD simulation for 1 time-unit, the differences between the S, I and R stocks are the values specified in equations \ref{eq:sir_delta_rates}. %This property has to hold for \textit{any} initial value for S, I and R.

Translating this into a property of our ABS implementation is analogous. We count the number of initially S, I and R agents and run the simulation for 1 time-unit to get new S, I and R numbers. The differences should \textit{average} at the values specified in equations \ref{eq:sir_delta_rates} as well. This property has to hold for \textit{any} agent population. Note that due to ABS stochastic nature it is not enough to run only one replication of the simulation for 1 time-unit but we actually need multiple replications ($> 100$) to get statistically robust results. We then use two-tailed t-tests to compare the expected averages to the actual averages. If all 3 tests pass the whole property-test passes.

\section{Implementing a property-test}
We start by defining the type of our property, which takes a list of \textit{SIRStates} and returns a \textit{Gen Bool}. Note that we run replications (100) of the simulation for 1.0 time-unit with same $\Delta t = 0.1$ as in Chapter \ref{sec:timedriven_firststep} to get lists of new S, I and R values.

\begin{HaskellCode}
prop_sd_rates :: [SIRState] -> Gen Bool
prop_sd_rates as = do
  let dt = 0.01
  -- run the sir-generator 100 times to generate 100 replications
  (ss, is, rs) <- unzip3 <\$> vectorOf 100 (sir as dt)
  -- check if the property holds
  return (checkSirSdSpec as ss is rs)
\end{HaskellCode}

Properties in QuickCheck are required to return \textit{Bool} to indicate success or failure - the arguments required for the function are then randomly generated and provided by QuickCheck. For QuickCheck to be able to generate random values of \textit{SIRState} we need to implement an instance of the \textit{Arbitrary} typeclass for \textit{SIRState}. This is straight forward: we \textit{uniformly} pick one out of the 3 possible values.

\begin{HaskellCode}
instance Arbitrary SIRState where
  -- arbitrary :: Gen SIRState
  -- Uniformly pick one of the 3 elements.
  arbitrary = elements [Susceptible, Infected, Recovered]
\end{HaskellCode}

If we want to have a different distribution of the Susceptible, Infected and Recovered states we can also provide a different \textit{Arbitrary} implementation:

\begin{HaskellCode}
instance Arbitrary SIRState where
  -- arbitrary :: Gen SIRState
  -- Susceptible are picked 3 times, Infected 2 times more often than Recovered
  arbitrary = frequency [ (3, return Susceptible)
                        , (2, return Infected)
                        , (1, return Recovered) ]
\end{HaskellCode}

We run multiple replications of the SIR simulation, each with a different random-number generator. To do that we implemented a custom Generator which takes the random population and the $\Delta t$. It simply generates a random-number generator by picking a random seed and then runs the SIR simulation for 1.0 time-unit and returns the number of S,I and R agents in the last step.

\begin{HaskellCode}
sir :: [SIRState] -> Double -> Gen (Int, Int, Int)
sir as dt = do
  seed <- choose (minBound, maxBound)
  let g = mkStdGen seed
  return $
    last $ 
    runSIRFor 1.0 dt as contactRate infectivity illnessDuration g
  where
    
\end{HaskellCode}

Next we encode the SD specification as explained above into code. Note that all values are \textit{Double} because of the continuous nature of SD.

% NOTE: we omited fromIntegral to make it more readable
\begin{HaskellCode}
sdSpec :: Double 
       -> Double
       -> Double
       -> Double 
       -> Double
       -> Double
       -> (Double, Double, Double)
sdSpec s0 i0 r0 beta gamma delta = (s, i, r)
  where
  	-- total population
    n = s0 + i0 + r0

    -- compute infection-rate
    ir = if n == 0 then 0 else (i0 * beta * s0 * gamma) / n
    -- recovery-rate
    rr = i0 / delta

    -- S value after 1 time-unit
    s = s0 - ir
    -- I value after 1 time-unit
    i = i0 + (ir - rr)
    -- R value after 1 time-unit
    r = r0 + rr
\end{HaskellCode}

Finally we implement the check whether the SIR dynamics matches the SD ones.

\begin{HaskellCode}
checkSirSDspec :: [SIRState] 
               -> [Int]
               -> [Int]
               -> [Int]
               -> Bool
checkSirSDspec as ss is rs = allPass
  where
    s0 = fromIntegral $ length $ filter (==Susceptible) as
    i0 = fromIntegral $ length $ filter (==Infected) as
    r0 = fromIntegral $ length $ filter (==Recovered) as
    
    -- compute SD values which act as expected means for the t-tests
    (s, i, r) = sdSpec s0 i0 r0 contactRate infectivity illnessDuration
    
    confidence = 0.95

    sTest = tTestSamples TwoTail s (1 - confidence) ss
    iTest = tTestSamples TwoTail i (1 - confidence) is
    rTest = tTestSamples TwoTail r (1 - confidence) rs

    allPass = sTest && iTest && rTest
\end{HaskellCode}

\section{Test results}
When testing the property with QuickCheck, by default 100 random test-cases will be generated and \textit{all} have to pass so that the whole property-test passes. Unfortunately, the whole property-test fails - not all 100 random test-cases go through even if we run the whole property-test repeatedly. QuickCheck prints out the random population it generated for the failing random test-case to give a counter-example for the assumption we encoded. Repeated runs show that the counter-examples seem to be lacking any regular pattern like Susceptibles only or 0 Infected - we seem to be failing randomly.

Indeed the fact that we are failing randomly reveals the fundamental difference between SD and ABS: due to ABS' stochastic nature, an ABS cannot match an SD exactly because it is much richer in its dynamics. This enables ABS to explore and reveal paths which are not possible in deterministic SD. In the case of the SIR model, such an alternative path would be the immediate recovery of the single infected agent at the beginning without infecting any other agent. This is not possible in the SD case: in case there is 1 infected agent, the whole epidemic will unfold.

The difficulty of comparing dynamics between SD and ABS and the impracticality to compare them \textit{exactly} was shown by \cite{macal_agent-based_2010} in the case of the SIR model, where the author shows that it is bimodal. Indeed, that is also supported by our observations. When looking at the samples of failed t-tests by plotting them in a histogram, it shows clearly that the values exhibit strong outliers, arriving at a skewed / fat tailed / bimodal histogram. %This means that they are not normally distributed, which is a base assumption and a necessity for t-tests.
The authors \cite{figueredo_comparing_2014} approach the problem of comparing ABS to SD more generally and propose different statistical techniques of how to approach the problem. 
We don't go into further statistical analysis of this problem here as it is not the aim of this thesis but we rather want to see what options we have in pushing the existing approach closer to the SD dynamics, giving us some measure of approximation.

\section{Accepting failure}
The different nature of ABS and SD has the implication that the binary approach of QuickCheck, where the whole property-test fails when a single random test-case fails, is too strict for testing ABS in general and our problem in particular. As a remedy, we can use \textit{maxFailPercent} \footnote{As of the time of writing this thesis (2nd April 2019), this only exists as a pull request \url{https://github.com/nick8325/quickcheck/pull/239} and has not been merged into the main branch of QuickCheck. Thus we use the QuickCheck from \url{https://github.com/stevana/quickcheck/tree/feat/max-failed-percent} who has provided the implementation of \textit{maxFailPercent}.} as a configuration argument to QuickCheck which allows the failure of a given percentage of random-tests cases. The argument behaves in a way that it tries to run up to the 100 default successful random test-cases but fails the overall property-test if the percentage of failed random test-cases is reached. By switching from a binary PASS/FAIL to a more probabilistic measure, reflecting reliability, we have now a tool we can use for measuring the gap between the SD specification and our ABS implementation. Note that a single run is not enough to create robust estimates about failure because QuickCheck always starts out with new seeds. Thus in our experiments, to get a more robust estimate we average 10 runs and report the average with the standard deviation. 

As a first test we run the same property-test again but allow a failure of 100\% to see how many tests will actually pass and how many will fail. In a first estimate run we get: \textit{*** Failed! Passed only 88 tests; 100 failed (53\%) tests}. This means QuickCheck ran 88 successful random test-cases for the property-test before reaching 100\% of failed tests, meaning that out of a total of 173 random test-cases 53\% were failed ones. When averaging 10 runs, 86.1 (4.3) pass with 100 failed tests amounting to a failure percentage of 53.3\% (1.05).
%1. *** Failed! Passed only 88 tests; 100 failed (53%) tests
%2. *** Failed! Passed only 83 tests; 100 failed (54%) tests.
%3. *** Failed! Passed only 86 tests; 100 failed (53%) tests.
%4. *** Failed! Passed only 82 tests; 100 failed (54%) tests.
%5. *** Failed! Passed only 94 tests; 100 failed (51%) tests.
%6. *** Failed! Passed only 88 tests; 100 failed (53%) tests.
%7. *** Failed! Passed only 82 tests; 100 failed (54%) tests.
%8. *** Failed! Passed only 92 tests; 100 failed (52%) tests.
%9. *** Failed! Passed only 82 tests; 100 failed (54%) tests.
%10. *** Failed! Passed only 84 tests; 100 failed (54%) tests.
%[88,83,86,82,94,88,82,92,82,84]
%[53,54,53,54,51,54,54,52,54,54]

\paragraph{Comparison to noise}
To make sure that our approach is going in the right direction at all, we replaced the SIR simulation with a rather simple noise-generator: it generates random values for S, I and R which have to sum up to the size of the population we are comparing against. Indeed, it performs considerably worse than our initial property-test: on average only 19.7 (2.7) tests pass with 100 failed, and a failure percentage of 83\% (1.7). 

%UNCORRELATED
%	*** Failed! Passed only 198 tests; 10000 failed (98%) tests.
%
%CORRELATED
%	*** Failed! Passed only 393 tests; 10000 failed (96%) tests.


%1. *** Failed! Passed only 15 tests; 100 failed (86%) tests.
%2. *** Failed! Passed only 20 tests; 100 failed (83%) tests.
%3. *** Failed! Passed only 23 tests; 100 failed (81%) tests.
%4. *** Failed! Passed only 20 tests; 100 failed (83%) tests.
%5. *** Failed! Passed only 23 tests; 100 failed (81%) tests.
%6. *** Failed! Passed only 20 tests; 100 failed (83%) tests.
%7. *** Failed! Passed only 23 tests; 100 failed (81%) tests.
%8. *** Failed! Passed only 18 tests; 100 failed (84%) tests.
%9. *** Failed! Passed only 17 tests; 100 failed (85%) tests.
%10. *** Failed! Passed only 18 tests; 100 failed (84%) tests.
%
%[15,20,23,20,23,20,23,18,17,18]
%[86,83,81,83,81,83,81,84,85,84]

\paragraph{Optimising $\Delta t$}
As already pointed out in \ref{sub:timedriven_results}, the selection of a sufficiently small $\Delta t$ is crucial and it might be very well the case that the original $\Delta t = 0.1$ we used in our implementation and in Chapter \ref{sub:timedriven_results} is not small enough. 

Indeed, when we half it to $\Delta t = 0.05$, 100 tests pass with 54 (10.3) failing on average, resulting in a failure percentage of 34.4\% (4.2) on average.
%1. +++ OK, passed 100 tests; 39 failed (28%).
%2. +++ OK, passed 100 tests; 48 failed (32%).
%3. +++ OK, passed 100 tests; 49 failed (32%).
%4. +++ OK, passed 100 tests; 65 failed (39%).
%5. +++ OK, passed 100 tests; 68 failed (40%).
%6. +++ OK, passed 100 tests; 46 failed (31%).
%7. +++ OK, passed 100 tests; 55 failed (35%).
%8. +++ OK, passed 100 tests; 55 failed (35%).
%9. +++ OK, passed 100 tests; 69 failed (40%).
%10. +++ OK, passed 100 tests; 46 failed (31%).
%[39,48,49,65,68,46,55,55,69,46]
%[28,32,32,39,40,31,35,35,40,31]

Lowering it to $\Delta t = 0.01$ decreases the failure percentage further with 100 tests passing and failed tests averaging at 15.9 (5.2) with 13.4\% (3.8).
%
%1. +++ OK, passed 100 tests; 25 failed (20%).
%2. +++ OK, passed 100 tests; 16 failed (13%).
%3. +++ OK, passed 100 tests; 25 failed (20%).
%4. +++ OK, passed 100 tests; 10 failed (9%).
%5. +++ OK, passed 100 tests; 10 failed (9%).
%6. +++ OK, passed 100 tests; 15 failed (13%).
%7. +++ OK, passed 100 tests; 14 failed (12%).
%8. +++ OK, passed 100 tests; 14 failed (12%).
%9. +++ OK, passed 100 tests; 15 failed (13%).
%10.+++ OK, passed 100 tests; 15 failed (13%).
%
%[25,16,25,10,10,15,14,14,15,15]
%[20,13,20,9,9,13,12,12,13,13]

Using such a property-based test can be used to find an optimally low $\Delta t$. The optimal $\Delta t$ is the lowest for which sufficiently enough tests go through.

\paragraph{Fixing random population size}
The size of the random population is random itself and we observed coverage of ranges from 0 up to 90. Due to ABS' discrete nature, an increased population size \textit{might} lead to a closer approximation to SD dynamics, which are continuous. When fixing the size of the random population to 100 ($\Delta t = 0.01$) we arrive at 100 passing tests and 35 (6) failed on average with 27.4\% (5.8) failure percentage.

%1. +++ OK, passed 100 tests; 30 failed (23%).
%2. +++ OK, passed 100 tests; 45 failed (31%)
%3. +++ OK, passed 100 tests; 32 failed (24%).
%4. +++ OK, passed 100 tests; 31 failed (23%).
%5. +++ OK, passed 100 tests; 36 failed (26%).
%
%[30,45,32,32,36]
%[23,31,24,23,36]

Fixing the population size leads to more failed tests. The reason for that might be that it is less likely to generate test-cases where the dynamics match exactly e.g. 0 agents of either Susceptible, Infected or Recovered. Still this property-test is not as general as varying also the size of the population.

\paragraph{Increasing confidence}
Another approach would be to increase the confidence in our t-tests e.g. to 99\% ($\alpha = 0.01$). When doing so ($\Delta t = 0.01$), 100 tests will pass and an average of 5.6 (2.9) fail with 4.6\% (2.9) of failure.

%
%1. +++ OK, passed 100 tests; 4 failed (3%).
%2. +++ OK, passed 100 tests; 5 failed (4%).
%3. +++ OK, passed 100 tests; 9 failed (8%).
%4. +++ OK, passed 100 tests; 3 failed (2%).
%5. +++ OK, passed 100 tests; 5 failed (4%).
%6. +++ OK, passed 100 tests; 10 failed (9%).
%7. +++ OK, passed 100 tests; 8 failed (7%).
%8. +++ OK, passed 100 tests; 2 failed (1%).
%9. +++ OK, passed 100 tests; 8 failed (7%).
%10 +++ OK, passed 100 tests; 2 failed (1%).
%
%[4,5,9,3,5,10,8,2,8,2]
%[3,4,8,2,4,9,7,1,7,1]

Although it seems that we have finally found a test configuration with a sufficiently low percentage of failure, changing the confidence can be problematic though. By increasing it we lower the risk of rejecting a test which matches the SD dynamics (type I error). On the other hand increasing the confidence also increases the risk of not rejecting tests which do not match the SD dynamics (type II error). 

\paragraph{Comparison to event-driven implementation}
We also implemented this property-test for our event-driven SIR implementation from Chapter \ref{sec:eventdriven_sir}. It has the main advantage that it does not suffer from the sampling issues of the time-driven approach and thus does not require the selection of an optimal $\Delta t$.
The results match on average the ones from the time-driven approach. Running it normally roughly matches the dynamics of $\Delta t = 0.01$, a fixed population size of 100 also matches the time-driven approach with a random population of size 100 and $\Delta t = 0.01$. Further we also got the same results when increasing confidence to 99\% ($\alpha = 0.01$) as in the time-driven approach.

%1. +++ OK, passed 100 tests; 25 failed (20%).
%2. +++ OK, passed 100 tests; 22 failed (18%).
%3. +++ OK, passed 100 tests; 26 failed (20%).
%4. +++ OK, passed 100 tests; 15 failed (13%).
%5. +++ OK, passed 100 tests; 29 failed (22%).
%6. +++ OK, passed 100 tests; 12 failed (10%).
%7. +++ OK, passed 100 tests; 17 failed (14%).
%8. +++ OK, passed 100 tests; 16 failed (13%).
%9. +++ OK, passed 100 tests; 13 failed (11%).
%10. +++ OK, passed 100 tests; 14 failed (12%).
%
%[25,22,26,15,29,12,17,16,13,14]
%>> mean (x)
%ans =  18.900
%>> std (x)
%ans =  6.0818

%TODO: what about using a fixed size random population instead of random size random population? this should increase the smoothness when always using e.g. 1000 or 10.000
%1. +++ OK, passed 100 tests; 30 failed (23%).
%2. +++ OK, passed 100 tests; 29 failed (22%).
%3. +++ OK, passed 100 tests; 44 failed (30%).
%4. +++ OK, passed 100 tests; 40 failed (28%).
%5. +++ OK, passed 100 tests; 41 failed (29%).
%6. +++ OK, passed 100 tests; 41 failed (29%).
%7. +++ OK, passed 100 tests; 38 failed (27%).
%8. +++ OK, passed 100 tests; 36 failed (26%).
%9. +++ OK, passed 100 tests; 30 failed (23%).
%10. +++ OK, passed 100 tests; 46 failed (31%).
%
%[30,29,44,40,41,41,38,36,30,46]
%>> mean (x)
%ans =  37.500
%>> std (x)
%ans =  6.0782

%TODO: confidence 0.99
%1. +++ OK, passed 100 tests; 6 failed (5%).
%2. +++ OK, passed 100 tests; 5 failed (4%).
%3. +++ OK, passed 100 tests; 7 failed (6%).
%4. +++ OK, passed 100 tests; 1 failed (0%).
%5. +++ OK, passed 100 tests; 5 failed (4%).
%6. +++ OK, passed 100 tests; 6 failed (5%).
%7. +++ OK, passed 100 tests; 6 failed (5%).
%8. +++ OK, passed 100 tests; 5 failed (4%).
%9. +++ OK, passed 100 tests; 10 failed (9%).
%10. +++ OK, passed 100 tests; 6 failed (5%).
%
%[6,5,7,1,5,6,6,5,10,6]
%>> mean (x)
%ans =  5.7000
%>> std (x)
%ans =  2.2136

This is a \textit{strong} indication, that although their underlying implementation technique is different, both implementations produce qualitatively same dynamics. It would be interesting to compare both implementations on using property-based testing, using an unpaired two-tailed t-test. We leave this for further research.

\section{Discussion}
By using QuickCheck, we showed how to connect the ABS implementation to the SD specification by deriving a property, based on the SD specification. This property is directly expressed in code and tested through generating random test-cases with random agent populations. We assumed that the underlying SIR implementation, more specific, that all agent behaviour, is correct - we explore testing of individual agent behaviour in the later chapters.

Although our initial idea of matching the ABS implementation to the SD specifications has not worked out in an exact way, we still showed a way of formalizing and expressing these relations in code and testing them using QuickCheck. By allowing failure in our tests using the \textit{maxFailPercent} parameter, we confirmed the importance of selecting an optimal $\Delta t$ as already pointed out in Chapter \ref{sub:timedriven_results}. By having measure of failure, we can use a property-test also as a way of systematically searching for the smallest optimal $\Delta t$, relieving us from making conservative guesses. For the event-driven implementation, there is no such issue and we have verified that it produces qualitatively the same results.

The results showed that the ABS implementation comes close to the original SD specification but does not match it exactly - it is indeed richer in its dynamics as \cite{macal_agent-based_2010, figueredo_comparing_2014} have already shown. Our approach might work out better for a different model, which has a better behaved underlying specification than the bimodal SIR. % for which a proper statistical analysis is not the aim and focus of this thesis is left for other researchers to dwell upon.