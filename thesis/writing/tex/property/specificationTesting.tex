\chapter{Testing the SIR model specification}
\label{ch:prop_sirspec}

In the previous chapters we have established the correctness of our event- and time-driven implementation up to our informal specification, we derived from the formal SD specification from Chapter \ref{sec:sir_model}. What we are lacking is a verification whether the implementations also match the formal SD specification or not. In the process of verification, we need to make sure it is correct up to some specification. We aim at connecting the agent-based implementation to the SD specification, by formalising it into properties within a property-test. The SD specification can be given through the differential equations shown in Chapter \ref{sec:sir_model}, which we repeat here:

\begin{equation}
\begin{split}
\frac{\mathrm d S}{\mathrm d t} = -infectionRate \\
\frac{\mathrm d I}{\mathrm d t} = infectionRate - recoveryRate \\
\frac{\mathrm d R}{\mathrm d t} = recoveryRate 
\end{split}
\quad
\begin{split}
infectionRate = \frac{I \beta S \gamma}{N} \\
recoveryRate = \frac{I}{\delta} 
\end{split}
\end{equation}
\label{eq:sir_delta_rates}

Solving these equations is done by integrating over time. In the SD terminology, the integrals are called \textit{Stocks} and the values over which is integrated over time are called \textit{Flows}. At $t = 0$ a single agent is infected because if there wouldn't be any infected agents, the system would immediately reach equilibrium - this is also the formal definition of the steady state of the system: as soon as $I(t) = 0$ the system won't change any more.

\begin{align}
S(t) &= N - I(0) + \int_0^t -infectionRate\, \mathrm{d}t \\
I(0) &= 1 \\
I(t) &= \int_0^t infectionRate - recoveryRate\, \mathrm{d}t \\
R(t) &= \int_0^t recoveryRate\, \mathrm{d}t
\end{align}

\section{Deriving a property}
TODO: make clear that we compare the numbers of susceptible, infected and recovered in the last step of the ABS and SD implementations.

The goal is now to derive a property which connects those equations with our implementation. We have to be careful and realis a fundamental difference between the SD and ABS implementations: SD is deterministic and continuous, ABS is stochastic and discrete. Thus we cannot compare single runs but we can only compare averages: stated informally, the property we want to implement is that the ABS dynamics matches the SD ones \textit{on average}, independent of the finite population size, model parameters $\beta$ (contact rate), $\gamma$ (infectivity) and $\delta$ (illness duration) and duration of the simulation. To be able to compare averages, we run 100 replications of the ABS simulation with same parameters except a different random-number generator in each replication. We then run a two-sided t-test on the replication values with the expected values from the SD dynamics.

\begin{HaskellCode}
compareSDToABS :: Int     -- ^ Initial number of susceptibles
               -> Int     -- ^ Initial number of infected
               -> Int     -- ^ Initial number of recovered
               -> [Int]   -- ^ Final Number of susceptibles in replications
               -> [Int]   -- ^ Final Number of infected in replications
               -> [Int]   -- ^ Final Number of recovered in replications
               -> Double  -- ^ beta (contact rate)
               -> Double  -- ^ gamma (infectivity)
               -> Double  -- ^ delta (illness duration)
               -> Time    -- ^ duration of simulation
               -> Bool
compareSDToABS s0 r0 i0
               ss is rs
               beta gamma delta t = sTest && iTest && rTest
  where
    -- run SD simulation to get expected averages
    (s, i, r) = simulateSD s0 i0 r0 beta gamma delta t
    
    confidence = 0.95
    sTest = tTestSamples TwoTail s (1 - confidence) ss
    iTest = tTestSamples TwoTail i (1 - confidence) is
    rTest = tTestSamples TwoTail r (1 - confidence) rs
\end{HaskellCode}

The implementation of \textit{simulateSD} is discussed in-depth in the Appendix \ref{app:sdSimulation}. We are very well aware that comparing the output against an SD simulation is dangerous: after all, why should be trust the SD implementation? As outlined in the Appendix \ref{app:sdSimulation}, great care has been taken to ensure the correctness: the formulas from the SIR specification are directly encoded in code, allowed by Yampas arrowized FRP which guarantees that at least that translation step is correct - we then only rely on a small enough sampling rate and the correctness of the Yampa library. The former one is very well in our reach and we pick a sufficiently small samply rate; the latter one is beyond our reach but we expect the library to me mature enough to be correct for our purposes.

\section{Implementing the test}
Implementing a property-test is straight-forward. Here we give the implementation for the time-driven SIR implementation, the implementation for the event-driven SIR implementation is exactly the same with the exception of \textit{genTimeSIRRepls}. We again make use of the \textit{checkCoverage} feature of QuickCheck to get statistical robust results: we expect that in 90\% of all test-cases the SD and ABS dynamics match \textit{on average}. QuickCheck will run as many tests as necessary to reach a statistically robust result which either allows to reject or accept this hypothesis.

\begin{HaskellCode}
prop_sir_time_spec :: Positive Double  -- ^ contact rate
                   -> UnitRange        -- ^ infectivity, within range (0,1)
                   -> Positive Double  -- ^ illness duration
                   -> TimeRange        -- ^ time to run
                   -> Property
prop_sir_time_spec 
    (Positive cor) (UnitRange inf) (Positive ild) (TimeRange t) = checkCoverage (do
  -- running 100 replications for the ABS SIR implementation
  let repls = 100
  -- generate large random population
  as <- resize 1000 (listOf genSIRState)
  -- run replications of time-driven SIR implementation
  (ss, is, rs) <- unzip3 <$> genTimeSIRRepls repls as cor inf ild t
  -- check if they match 
  let prop = compareSDToABS as ss is rs cor inf ild t
  -- we expect 95% to pass and use checkCoverage to get statistical robust result
  return $ cover 95 prop "SIR time-driven passes t-test with simulated SD" True
\end{HaskellCode}

\section{Running the test}
When running the tests for the time- and event-driven implementation, QuickCheck reports the following:

\begin{verbatim}
OK (5784.64s)
    +++ OK, passed 100 tests (71% SIR time-driven passes t-test with simulated SD).
    Only 71% SIR time-driven passes t-test with simulated SD, but expected 90%

OK (3339.98s)
    +++ OK, passed 100 tests (37% SIR event-driven passes t-test with simulated SD).
        Only 37% SIR event-driven passes t-test with simulated SD, but expected 90%
\end{verbatim}

TODO:
1. 71\% for time-driven is ok, this is what i roughly expected and is consistent with macals results
2. 37\% for event-driven is NOT ok, it should reach the same level as time-driven, there is something wrong
3. when testing the equal mean of random time- and even-driven property-tests they all failed, which is at least consistent with the event-driven implementation failing to to reach the same \% as the time-driven one.
4. TODO: investigate why event-driven is lacking behind and FIX IT! It has to reach roughly the same level as time-driven AND the comparison tests of paired t-test have to pass (most of them at least e.g. 95\%).
5. Performance is awful (for both tests above 9000seconds which is about 2.5 hours, this is unacceptable). Also we just ran for 1.0 time-unit, can we do something about it? e.g. parallelising?
6. Use Mann-Whittney U test because it does not assume normal distribution as t-test does, maybe this results in better result.

TODO: run event-driven properly with full random values, compare three runs:
-> MakeContact 1.0 dt with fixed number of contacts
-> MakeContact 1.0 dt with number of contacts exponentially distributed
-> MakeContact exponential dt with fixed number of contacts

FIXED EVENT-DRIVEN: generate on average 1 MakeContact event per time-unit and in each case make contact rate number of contacts

TODO: as figuerdo already put it: stochasticity in both  present outcomes with difference which are statistically but look look similar. further, abs can contribute additional insight through extra patterns, not possible with SD 

TODO: further research: inclusion of Environment: all  properties should still hold under different Environments which can be generated randomly as Well e.g. random Networks.

TODO: use mann whitney u test instead of two-sided t-test because our data is bi-modal and not normally distributed! This should arrive at a much higher coverage than with a two-sided t-test

TODO: note that comparing of averages is normally not sufficient and one also has to look at variance - this is not possible with SD because it is deterministic without any stochastic influence, thus we cant do any comparison of variance.

TODO: we can see quickcheck not only as testing but also as verification and validation allowing to test hypotheses and properties during the development process, also has connections to Monte-Carlo simulation

It seems that a much higher percentage of tests if failing than expected and our initial guess of 95\% is too high and a more realistic coverage is around 80\%, as computed by QuickChecks sequential hypothesis testing. Indeed, this very fact that we are failing more tests than expected reveals the fundamental difference between SD and ABS: due to ABS' stochastic nature, an ABS cannot match an SD exactly because it is much richer in its dynamics. This enables ABS to explore and reveal paths which are not possible in deterministic SD. In the case of the SIR model, such an alternative path would be the immediate recovery of the single infected agent at the beginning without infecting any other agent. This is not possible in the SD case: in case there is 1 infected agent, the whole epidemic will unfold.

The difficulty of comparing dynamics between SD and ABS and the impracticality to compare them \textit{exactly} was shown by \cite{macal_agent-based_2010} in the case of the SIR model, where the author shows that it generates a bimodal distribution. Indeed, that is also supported by our observations. When looking at the samples of failed t-tests by plotting them in a histogram, it shows clearly that the values exhibit strong outliers, arriving at a skewed / fat tailed / bimodal histogram. %This means that they are not normally distributed, which is a base assumption and a necessity for t-tests.
The authors \cite{figueredo_comparing_2014} approach the problem of comparing ABS to SD more generally and propose different statistical techniques of how to approach the problem.
We don't go into further statistical analysis of this problem here as it is not the aim of this thesis. A possible direction would be to use a different statistical test, more suitable to test for bi-modal distributions, as the t-test assumes normal distribution - we leave this for further research. We simply accept that we can only reach a proximity of 80\% between those two approaches but never get an exact verification due to both systematic difference.

\section{Discussion}
By using QuickCheck, we showed how to connect the ABS implementation to the SD specification by deriving a property, based on the SD specification. This property is directly expressed in code and tested through generating random test-cases with random agent populations. We assumed that the underlying SIR implementation, more specific, that all agent behaviour, is correct - we explore testing of individual agent behaviour in the later chapters.

Although our initial idea of matching the ABS implementation to the SD specifications has not worked out in an exact way, we still showed a way of formalizing and expressing these relations in code and testing them using QuickCheck. The results showed that the ABS implementation comes close to the original SD specification but does not match it exactly - it is indeed richer in its dynamics as \cite{macal_agent-based_2010, figueredo_comparing_2014} have already shown. Our approach might work out better for a different model, which has a better behaved underlying specification than the bimodal SIR. % for which a proper statistical analysis is not the aim and focus of this thesis is left for other researchers to dwell upon.