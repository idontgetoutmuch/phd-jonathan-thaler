\chapter{Testing the SIR model specification}
\label{ch:prop_sirspec}

In the previous chapters we have established the correctness of our event- and time-driven implementation up to our informal specification, we derived from the formal SD specification from Chapter \ref{sec:sir_model}. What we are lacking is a verification whether the implementations also match the formal SD specification or not. In the process of verification, we need to make sure it is correct up to some specification. We aim at connecting the agent-based implementation to the SD specification, by formalising it into properties within a property-test. The SD specification can be given through the differential equations shown in Chapter \ref{sec:sir_model}, which we repeat here:

\begin{equation}
\begin{split}
\frac{\mathrm d S}{\mathrm d t} = -infectionRate \\
\frac{\mathrm d I}{\mathrm d t} = infectionRate - recoveryRate \\
\frac{\mathrm d R}{\mathrm d t} = recoveryRate 
\end{split}
\quad
\begin{split}
infectionRate = \frac{I \beta S \gamma}{N} \\
recoveryRate = \frac{I}{\delta} 
\end{split}
\end{equation}
\label{eq:sir_delta_rates}

Solving these equations is done by integrating over time. In the SD terminology, the integrals are called \textit{Stocks} and the values over which is integrated over time are called \textit{Flows}. At $t = 0$ a single agent is infected because if there wouldn't be any infected agents, the system would immediately reach equilibrium - this is also the formal definition of the steady state of the system: as soon as $I(t) = 0$ the system won't change any more.

\begin{align}
S(t) &= N - I(0) + \int_0^t -infectionRate\, \mathrm{d}t \\
I(0) &= 1 \\
I(t) &= \int_0^t infectionRate - recoveryRate\, \mathrm{d}t \\
R(t) &= \int_0^t recoveryRate\, \mathrm{d}t
\end{align}

\section{Deriving a property}
TODO: make clear that we compare the numbers of susceptible, infected and recovered in the last step of the ABS and SD implementations.

The goal is now to derive a property which connects those equations with our implementation. We have to be careful and realis a fundamental difference between the SD and ABS implementations: SD is deterministic and continuous, ABS is stochastic and discrete. Thus we cannot compare single runs but we can only compare averages: stated informally, the property we want to implement is that the ABS dynamics matches the SD ones \textit{on average}, independent of the finite population size, model parameters $\beta$ (contact rate), $\gamma$ (infectivity) and $\delta$ (illness duration) and duration of the simulation. To be able to compare averages, we run 100 replications of the ABS simulation with same parameters except a different random-number generator in each replication. We then run a two-sided t-test on the replication values with the expected values from the SD dynamics.

\begin{HaskellCode}
compareSDToABS :: Int     -- ^ Initial number of susceptibles
               -> Int     -- ^ Initial number of infected
               -> Int     -- ^ Initial number of recovered
               -> [Int]   -- ^ Final Number of susceptibles in replications
               -> [Int]   -- ^ Final Number of infected in replications
               -> [Int]   -- ^ Final Number of recovered in replications
               -> Double  -- ^ beta (contact rate)
               -> Double  -- ^ gamma (infectivity)
               -> Double  -- ^ delta (illness duration)
               -> Time    -- ^ duration of simulation
               -> Bool
compareSDToABS s0 r0 i0
               ss is rs
               beta gamma delta t = sTest && iTest && rTest
  where
    -- run SD simulation to get expected averages
    (s, i, r) = simulateSD s0 i0 r0 beta gamma delta t
    
    confidence = 0.95
    sTest = tTestSamples TwoTail s (1 - confidence) ss
    iTest = tTestSamples TwoTail i (1 - confidence) is
    rTest = tTestSamples TwoTail r (1 - confidence) rs
\end{HaskellCode}

The implementation of \textit{simulateSD} is discussed in-depth in the Appendix \ref{app:sdSimulation}. We are very well aware that comparing the output against an SD simulation is dangerous: after all, why should be trust the SD implementation? As outlined in the Appendix \ref{app:sdSimulation}, great care has been taken to ensure the correctness: the formulas from the SIR specification are directly encoded in code, allowed by Yampas arrowized FRP which guarantees that at least that translation step is correct - we then only rely on a small enough sampling rate and the correctness of the Yampa library. The former one is very well in our reach and we pick a sufficiently small samply rate; the latter one is beyond our reach but we expect the library to me mature enough to be correct for our purposes.

\section{Implementing the test}
Implementing a property-test is straight-forward. Here we give the implementation for the time-driven SIR implementation, the implementation for the event-driven SIR implementation is exactly the same with the exception of \textit{genTimeSIRRepls}. We again make use of the \textit{checkCoverage} feature of QuickCheck to get statistical robust results: we expect that in 90\% of all test-cases the SD and ABS dynamics match \textit{on average}. QuickCheck will run as many tests as necessary to reach a statistically robust result which either allows to reject or accept this hypothesis.

\begin{HaskellCode}
prop_sir_time_spec :: Positive Double  -- ^ contact rate
                   -> UnitRange        -- ^ infectivity, within range (0,1)
                   -> Positive Double  -- ^ illness duration
                   -> TimeRange        -- ^ time to run
                   -> Property
prop_sir_time_spec 
    (Positive cor) (UnitRange inf) (Positive ild) (TimeRange t) = checkCoverage (do
  -- running 100 replications for the ABS SIR implementation
  let repls = 100
  -- generate large random population
  as <- resize 1000 (listOf genSIRState)
  -- run replications of time-driven SIR implementation
  (ss, is, rs) <- unzip3 <$> genTimeSIRRepls repls as cor inf ild t
  -- check if they match 
  let prop = compareSDToABS as ss is rs cor inf ild t
  -- we expect 95% to pass and use checkCoverage to get statistical robust result
  return $ cover 95 prop "SIR time-driven passes t-test with simulated SD" True
\end{HaskellCode}

\section{Running the test}
When running the tests for the time- and event-driven implementation, QuickCheck reports the following:

\begin{verbatim}
OK (5784.64s)
    +++ OK, passed 100 tests (71% SIR time-driven passes t-test with simulated SD).
    Only 71% SIR time-driven passes t-test with simulated SD, but expected 90%

OK (3339.98s)
    +++ OK, passed 100 tests (37% SIR event-driven passes t-test with simulated SD).
        Only 37% SIR event-driven passes t-test with simulated SD, but expected 90%
\end{verbatim}

TODO: run event-driven properly with full random values, compare three runs:
-> MakeContact 1.0 dt with fixed number of contacts
TODO

-> MakeContact 1.0 dt with number of contacts exponentially distributed
TODO

The results of the time-driven approach seem to be in accordance with the results reported in \cite{macal_agent-based_2010}, where the authors report that a 70\% envelopes contains both the results of the SD and ABS implementation. We reach 71\% 

The event-driven approach clearly fails to reach a similar percentage: the differences between the the dynamics generated by the event-driven and the SD seem to be statistically significant. This result is consistent with the one from Chapter \ref{ch:sir_invariants}, where showed that the distribution of the dynamics generated by the event- and time-driven approach vary from each other considerably, it would have been surprising if they both show a similar performance when compared with the SD approach. 

This is a very strong indication that although both implementation techniques try to implement the same underlying model, they generate different distributions and are thus not \textit{statistically} equal. It seems that the time-driven approach follows the SD one closer, whereas the event-driven one deviates substantially. 
Still the question is whether it actually even makes sense to compare the approaches to the SD one or even amongst each other - after all they can be seen as fundamentally different approaches. We can argue that they are qualitatively equal as \cite{figueredo_comparing_2014} has already emphasised in a different model: although dynamics of ABS models are statistically different from SD ones, they look similar. The main difference is that ABS can contribute additional insight through revealing extra patterns due to its stochasticity, something not possible with SD.
The implication from this is, that depending on what our intention is, picking a time-driven or an event-driven implementation can and will make a statistical difference. If one is transferring an SD to an ABS model, one might consider to follow the time-driven approach as it seems to come much closer to the SD dynamics than the event-driven approach. The event-driven approach on the other hand is quicker and often much more explicit in its behaviour as all events are revealed and testing is easier due to decoupled actions.
A technical explanation for the differences is that we are dealing with discrete events, jumping from time to time instead of moving forward in time continuously as it happens conceptually in the time-driven approach. Time is also continuous in SD, thus it seems intuitively clear that a time-driven approach is closer at the SD implementation than the event-driven one. This is especially visible when making contact which happens every 1.0 time-units.
In our implementation we follow the idea as presented in \cite{macal_agent-based_2010}. Their specification states that in each time-step of 1 time-unit the susceptible make contact with a fixed number (contact rate) of other agents. This is a valid ABS model but it deviates to come closer to the SD as it fixes the number of contacts instead of making contact \textit{on average}. In SD and the time-driven approach, these are all averages and thus we are drawing from the exponential distribution in the time-driven case. This is not the case in our event-drive implementation, following \cite{macal_agent-based_2010}. If we change this behaviour to incorporate the exponential distribution we get a higher match to the SD dynamics of 37\% which is still a far cry form the time-driven approach but it at least indicates that this is a viable option to push the results further towards the SD dynamics. 

It seems that a much higher percentage of tests if failing than expected and our initial guess of 95\% is too high and a more realistic coverage is around 80\%, as computed by QuickChecks sequential hypothesis testing. Indeed, this very fact that we are failing more tests than expected reveals the fundamental difference between SD and ABS: due to ABS' stochastic nature, an ABS cannot match an SD exactly because it is much richer in its dynamics. This enables ABS to explore and reveal paths which are not possible in deterministic SD. In the case of the SIR model, such an alternative path would be the immediate recovery of the single infected agent at the beginning without infecting any other agent. This is not possible in the SD case: in case there is 1 infected agent, the whole epidemic will unfold.

The difficulty of comparing dynamics between SD and ABS and the impracticality to compare them \textit{exactly} was shown by \cite{macal_agent-based_2010} in the case of the SIR model, where the author shows that it generates a bimodal distribution. Indeed, that is also supported by our observations. When looking at the samples of failed t-tests by plotting them in a histogram, it shows clearly that the values exhibit strong outliers, arriving at a skewed / fat tailed / bimodal histogram. %This means that they are not normally distributed, which is a base assumption and a necessity for t-tests.
The authors \cite{figueredo_comparing_2014} approach the problem of comparing ABS to SD more generally and propose different statistical techniques of how to approach the problem.
We don't go into further statistical analysis of this problem here as it is not the aim of this thesis. A possible direction would be to use a different statistical test, more suitable to test for bi-modal distributions, as the t-test assumes normal distribution - we leave this for further research. We simply accept that we can only reach a proximity of 80\% between those two approaches but never get an exact verification due to both systematic difference.

\section{Discussion}
By using QuickCheck, we showed how to connect the ABS implementation to the SD specification by deriving a property, based on the SD specification. This property is directly expressed in code and tested through generating random test-cases with random agent populations. We assumed that the underlying SIR implementation, more specific, that all agent behaviour, is correct - we explore testing of individual agent behaviour in the later chapters.

Although our initial idea of matching the ABS implementation to the SD specifications has not worked out in an exact way, we still showed a way of formalizing and expressing these relations in code and testing them using QuickCheck. The results showed that the ABS implementation comes close to the original SD specification but does not match it exactly - it is indeed richer in its dynamics as \cite{macal_agent-based_2010, figueredo_comparing_2014} have already shown. Our approach might work out better for a different model, which has a better behaved underlying specification than the bimodal SIR. % for which a proper statistical analysis is not the aim and focus of this thesis is left for other researchers to dwell upon.