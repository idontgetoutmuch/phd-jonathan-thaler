\section{Case study I: \\ Testing the SIR model specification}
\label{sec:case_SIR}
As first use-case we discuss property-based testing for the \textit{explanatory} agent-based SIR model as introduced in Chapter \ref{sec:sir_model}. We aim at connecting the agent-based implementation to the SD specification, by formalising it into properties within a property-test. The SD specification can be given through the differential equations shown in Chapter \ref{sec:sir_model}, which we repeat here:

\begin{equation}
\begin{split}
\frac{\mathrm d S}{\mathrm d t} = -infectionRate \\
\frac{\mathrm d I}{\mathrm d t} = infectionRate - recoveryRate \\
\frac{\mathrm d R}{\mathrm d t} = recoveryRate 
\end{split}
\quad
\begin{split}
infectionRate = \frac{I \beta S \gamma}{N} \\
recoveryRate = \frac{I}{\delta} 
\end{split}
\end{equation}
\label{eq:sir_delta_rates}

Solving these equations is done by integrating over time. In the SD terminology, the integrals are called \textit{Stocks} and the values over which is integrated over time are called \textit{Flows}. At $t = 0$ a single agent is infected because if there wouldn't be any infected agents, the system would immediately reach equilibrium - this is also the formal definition of the steady state of the system: as soon as $I(t) = 0$ the system won't change any more.

\begin{align}
S(t) &= N - I(0) + \int_0^t -infectionRate\, \mathrm{d}t \\
I(0) &= 1 \\
I(t) &= \int_0^t infectionRate - recoveryRate\, \mathrm{d}t \\
R(t) &= \int_0^t recoveryRate\, \mathrm{d}t
\end{align}

\subsection{Deriving the properties}
The key to encode these specifications into a property is to understand that the stocks of \textit{S}, \textit{I} and \textit{R} change \textit{per time-unit} by the given rates. This means that if we run an SD simulation for 1 time-unit, the differences between the S, I and R stocks are the values specified in equations \ref{eq:sir_delta_rates}. %This property has to hold for \textit{any} initial value for S, I and R.

Translating this into a property of our ABS implementation is analogous. We count the number of initially S, I and R agents and run the simulation for 1 time-unit to get new S, I and R numbers. The differences should \textit{average} at the values specified in equations \ref{eq:sir_delta_rates} as well. This property has to hold for \textit{any} agent population. Note that due to ABS stochastic nature it is not enough to run only one replication of the simulation for 1 time-unit but we actually need multiple replications ($> 100$) to get statistically robust results. We then use two-tailed t-tests to compare the expected averages to the actual averages. If all 3 tests pass the whole property-test passes.

\subsection{Implementing a property-test}
We start by defining the type of our property, which takes a list of \textit{SIRStates} and returns a \textit{Bool}.

\begin{HaskellCode}
prop_sd_rates :: [SIRState] -> Bool
\end{HaskellCode}

Properties in QuickCheck are required to return \textit{Bool} to indicate success or failure - the arguments required for the function are then randomly generated and provided by QuickCheck. For QuickCheck to be able to generate random values of \textit{SIRState} we need to implement an instance of the \textit{Arbitrary} typeclass for \textit{SIRState}. This is straight forward: we \textit{uniformly} pick one out of the 3 possible values.

\begin{HaskellCode}
instance Arbitrary SIRState where
  -- arbitrary :: Gen SIRState
  -- Uniformly pick one of the 3 elements.
  arbitrary = elements [Susceptible, Infected, Recovered]
\end{HaskellCode}

If we want to have a different distribution of the Susceptible, Infected and Recovered states we can also provide a different \textit{Arbitrary} implementation:

\begin{HaskellCode}
instance Arbitrary SIRState where
  -- arbitrary :: Gen SIRState
  -- Susceptible are picked 3 times, Infected 2 times more often than Recovered
  arbitrary = frequency [ (3, return Susceptible)
                        , (2, return Infected)
                        , (1, return Recovered) ]
\end{HaskellCode}

Because we are running replications, we need random seeds to create random-number generators for each replication. We don't make them an argument to the property-test because we don't want to let QuickCheck handle them for us. The reason for that is that if we make it a parameter to the function, QuickCheck tries to vary it as well, meaning that we have less variance and test-cases over the initial population. Thus we generate the seeds manually but relying on QuickChecks random functionality.

\begin{HaskellCode}
prop_sd_rates :: [SIRState] -> Gen Bool
prop_sd_rates as = do
    -- Draw a list of replications random Ints over the full Int range to  
    -- reach maximum variance (reduce probability of drawing identical seeds)
    seeds <- vectorOf replications (choose (minBound, maxBound))
    return (prop_sd_ratesAux seeds)
  where
    prop_sd_ratesAux :: [Int] -> Bool
    prop_sd_ratesAux seeds = allTTestsPass -- see below
\end{HaskellCode}

Next we encode the SD specification as explained above into code.

% NOTE: we omited fromIntegral to make it more readable
\begin{HaskellCode}
-- initial values of S,I and R and total number of agents N
s0 = length (filter (==Susceptible) as)
i0 = length (filter (==Infected) as)
r0 = length (filter (==Recovered) as)
n  = s0 + i0 + r0

-- explicit re-naming
beta  = contactRate
gamma = infectivity
delta = illnessDuration

-- infection-rate
ir = if n == 0 then 0 else (i0 * beta * s0 * gamma) / n
-- recovery-rate 
rr = i0 / delta

-- S value after 1 time-unit 
s = s0 - ir
-- I value after 1 time-unit
i = i0 + (ir - rr)
-- R value after 1 time-unit
r = r0 + rr
\end{HaskellCode}

Then we run replications (100) of the simulation for 1.0 time-unit with same $\Delta t = 0.1$ as in Chapter \ref{sec:timedriven_firststep} to get lists of new S, I and R values.

\begin{HaskellCode}
-- run for 1 time-unit
dur = 1.0
-- same dt as in time-driven chapter implementation
dt = 0.1
-- generate random-number generator for each replication
rngs = map mkStdGen seeds
-- compute simulated values for s, i and r
(ss, is, rs) = unzip (map (last . runSIR dur dt as beta gamma delta) rngs)
\end{HaskellCode}

Finally we run two-tailed t-tests with confidence of 0.95 ($\alpha = 0.05$) for all 3 lists of the new S,I and R values.

% NOTE: tTestSamples return a Maybe Bool but we dont care about that detail here
\begin{HaskellCode}
confidence = 0.95
sTest = tTestSamples TwoTail s (1 - confidence) ss
iTest = tTestSamples TwoTail i (1 - confidence) is
rTest = tTestSamples TwoTail r (1 - confidence) rs

-- property-test passes if all 3 t-tests pass
allTTestsPass = sTest && iTest && rTest
\end{HaskellCode}

\subsection{Results}
When testing the property with QuickCheck, by default 100 random test-cases will be generated and \textit{all} have to pass so that the whole property-test passes. Unfortunately, the whole property-test fails - not all 100 random-test cases go through even if we run the whole property-test repeatedly. When looking at the samples of failed t-tests and plotting them in a histogram, it shows clearly that the values exhibit strong outliers and skewed / fat tailed histograms. This means that they are not normally distributed, which is a base assumption and a necessity for t-tests. 

\subsubsection{Accepting failed random test-cases}
This means that the binary approach of QuickCheck, where the whole property-test fails when a single random test-case fails, is too strict for testing ABS in general and our problem in particular. As a remedy, we can use \textit{maxFailPercent} \footnote{As of the time of writing this thesis (2nd April 2019), this only exists as a pull request \url{https://github.com/nick8325/quickcheck/pull/239} and has not been merged into the main branch of QuickCheck. Thus we use the QuickCheck from \url{https://github.com/stevana/quickcheck/tree/feat/max-failed-percent} who has provided the implementation of \textit{maxFailPercent}.} as a configuration argument to QuickCheck to allow the failure of a given percentage of random-tests cases. The argument behaves in a way that it tries to run up to 100 successful random test-cases but fails the overall property-test if the percentage of failed random test-cases is hit.

We now have a tool to accept some failure and still get through with our tests. Lets first see how far we will get with a failure percentage of 100. One possible outcome is: \textit{*** Failed! Passed only 73 tests; 100 failed (57\%) tests}. This means the property-test had 73 successful and 100 failed random test-cases, meaning that out of a total of 173 random test-cases 57\% were failed ones. 

By switching from a binary PASS/FAIL to a more probabilistic measure, reflecting reliability, we have now a tool we can use for closing the gap between the SD specification and our ABS implementation. As already pointed out in \ref{sub:timedriven_results}, the selection of a sufficiently small $\Delta t$ is crucial and it might be very well the case that the original $\Delta t = 0.1$ we used in our implementation and in Chapter \ref{sub:timedriven_results} is not small enough. 

Indeed, when we half it to $\Delta t = 0.05$ only 29 tests out of 129 total fail in one particular run: +++ OK, passed 100 tests; 29 failed (22\%.). Lowering it to $\Delta t = 0.01$ doesn't seem to have a significant effect, we arrive at roughly the same ratio of failed tests.

TODO: try event-driven : no dt problems but should experience similar problems due to ABS vs. SD problems?

Another approach would be to increase the confidence in our t-tests e.g. to 99\%. This shows significant improvement, resulting in only about 10\% of a failed test ratio. Changing the confidence can be problematic: by increasing it we lower the risk of making type I errors, which means reduced risk of rejecting a test which matches the SD dynamics. On the other hand increasing the confidence also increases the risk of making type II errors, which means increased risk of not rejecting test which does not match the SD dynamics. 

\subsubsection{SD - ABS impendance mismatch}
This fact reveals the fundamental difference between SD and ABS: due to ABS' stochastic nature, an ABS cannot match an SD exactly because it is much richer in its dynamics. This enables ABS to explore and reveal paths which are not possible in deterministic SD. In the case of the SIR model, such an alternative path would be the immediate recovery of the single infected agent at the beginning without infecting any other agent. This is not possible in the SD case: in case there is 1 infected agent, the whole epidemic will unfold.

The difficulty of comparing dynamics between SD and ABS and the impracticality to compare them \textit{exactly} was shown by \cite{macal_agent-based_2010} in the case of the SIR model, where the author shows that it is bimodal. The authors \cite{figueredo_comparing_2014} approach the problem of comparing ABS to SD more generally and propose different techniques of how to approach the problem.

We will not go further into this problem as it is beyond the scope of this thesis but our tests reveal that very fact that we can come quite close but can't match exactly - the confidence parameter allows us to specify the "tightness" of our tests and guarantees us at least that we come close up to some limit.

\subsection{Discussion}
By using QuickCheck, we showed how to connect the ABS implementation to the SD specification by deriving a property, based on the SD specification. This property is directly expressed in code and tested by generating random test-cases where the random data is a random agent population. 

Although our initial idea of matching the ABS implementation to the SD specifications has not worked out in an exact way, we still showed a way of formalizing and expressing these relations in code and testing them using QuickCheck. Due to the "softness" of the tests through the confidence parameter, we can show that this specification comes close to the original SD implementation but does not match it exactly and is indeed richer in its dynamics as \cite{macal_agent-based_2010, figueredo_comparing_2014} have already shown. 

Our approach might work out better for a different model, which has a better behaved underlying specification than the bimodal SIR. Also, we showed how to use such a test to derive an optimally small $\Delta t$, which is not subject to under-sampling as discussed in Chapter \ref{sub:timedriven_results}. The optimal $\Delta t$ is the lowest for which all (or sufficiently enough) tests go through. % It can be found by starting out with 1.0 and always half it until sufficiently enough tests go through.

Next we look into testing individual agent-behaviour in the case of the event-driven ABS