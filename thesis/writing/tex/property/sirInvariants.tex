\chapter{Testing SIR Invariants}
\label{ch:sir_invariants}

So far, our tests were stateless: only one computational step of an agent was considered by feeding a single event and ignoring the agent continuation. Also the events didn't contain any notion of time as they would carry within the queue. Feeding follow-up events into the continuation would make testing inherently stateful as we introduce history into the system. Such tests would allow to test the full life-cycle of one  agent or a full population.

In this chapter we will discuss how we can encode properties and specifications which require stateful testing. We define stateful testing here as: evolving a simulation state consisting of one or more agents over multiple events. Note that this also includes running the whole simulation.

\section{Deriving the invariants}
By informally reasoning about the agent-specification and by realising that they are in fact a state-machine with a on-directional flow from Susceptible to Infected to Recovered, we can come up with a few invariants which have to hold for any SIR simulation run independent of the random-number stream and the population:

\begin{enumerate}
	\item Simulation time is monotonic increasing. For each event an output of the current number of S,I and R agents is generated together with the time when the events occurred. This event time can stay the same between steps, will eventually increase but must never decrease. Obviously this invariant is a fundamental assumption in most simulations: time advances into the future and does not flow backwards.
	
	\item The number of total agents $N$ stays constant in SIR. The SIR model does not specify the dynamic creation or removal of agents during simulation. This is in contrast to the Sugarscape where, depending on the model parameters, this can be very well the case.
	
	\item The number of \textit{Susceptible} agents $S$ is monotonic decreasing. Susceptile agents \textit{might} become infected, reducing the total number of susceptible agents but they can never increase because neither an infected nor recovered agent can go back to susceptible.
	
	\item The number of \textit{Recovered} agents $R$ is monotonic increasing. This is due to infected agents \textit{will} recover, leading to an increase of recovered agents but once the recovered state is reached, there is no escape from it.
	
	\item The number of \textit{Infected} agents respects the invariant of the equation $I = N - (S + R)$ for every step. This follows directly from the first property which says $N = S + I + R$.
\end{enumerate}

\section{Encoding the invariants}
All of those properties are easily expressed directly in code (TODO: add a note in the benefits how extremely well this is encoded in functional programming).

\begin{HaskellCode}
sirInvariants :: Int -> [(Time, (Int, Int, Int))] -> Bool
sirInvariants n aos = timeInc && aConst && susDec && recInc && infInv
  where
    (ts, sirs)  = unzip aos
    (ss, _, rs) = unzip3 sirs

    -- 1. time is monotonic increasing
    timeInc = mono (<=)  ts
    -- 2. number of agents N stays constant in each step
    aConst = all agentCountInv sirs
    -- 3. number of susceptible S is monotonic decreasing
    susDec = mono (>=) ss
    -- 4. number of recovered R is monotonic increasing
    recInc = mono (<=)  rs
    -- 5. number of infected I = N - (S + R)
    infInv = all infectedInv sirs

    agentCountInv :: (Int, Int, Int) -> Bool
    agentCountInv (s,i,r) = s + i + r == n

    infectedInv :: (Int, Int, Int) -> Bool
    infectedInv (s,i,r) = i == n - (s + r)

    mono :: (Ord a, Num a) => (a -> a -> Bool) -> [a] -> Bool
    mono f xs = all (uncurry f) (pairs xs)

    pairs :: [a] -> [(a,a)]
    pairs xs = zip xs (tail xs)
\end{HaskellCode}

Putting this property into a QuickCheck test is straightforward. Note that we use randomise the model parameters $\beta$ (contact rate), $\gamma$ (infectivity) and $\delta$ (illness duration) because the properties have to hold for all positive, finite model parameters.

\begin{HaskellCode}
prop_sir_invariants :: Positive Double -- ^ Random beta, contact rate
                    -> Positive Double -- ^ Random gamma, infectivity
                    -> Positive Double -- ^ Random delta, illness duration
                    -> Property
prop_sir_invariants (Positive cor) (Positive inf) (Positive ild) = property (do
  -- generate population with size of up to 1000
  ss <- resize 1000 (listOf genSIRState)
  -- total agent count
  let n = length ss

  -- run the SIR simulation with a new RNG for up to t = 150 
  ret <- genSimulationSIR ss cor inf ild (-1) 150
  -- check invariants and return result
  return (sirInvariants n ret)
\end{HaskellCode}

Unsurprisingly all 100 tests pass. Note that we put a time-limit of 150 on the simulations to run, meaning that if a simulation does not terminate before that limit, it will be terminated at $t=150$. This is actually not necessary because we can reason that the SIR simulation \textit{will always} reach an equilibrium in finite steps thus not requiring an actual time-limit - we discuss this more in-depth in Chapter \ref{ch:equilibrium_totality}.

\subsection{Random Event Sampling}
An interesting question is whether or not these properties depend on correct interdependencies of events the agents send to each other in reaction to events they receive. Put in other words: do these invariants also hold under \textit{random event sampling}? To test this, instead of using the actual SIR implementation, which inserts the events generated by the agents into the event-queue, we wrote a new SIR kernel. It completely ignores the events generated by the agents and instead makes use of an infinite stream of random queue-elements from which it executes a given number, 100,000 in our case. Note that queue-elements contain a time-stamp, the receiver agent id and the actual event: the time-stamp is ensured to be increasing, to hold up the monotonic time property, the receiver agent id is drawn randomly from the constant list of all agents in the simulation and the actual event is generated completely randomly. As it turns out, the implementation of the agents ensure that the SIR properties are also invariant under \textit{random event sampling} - all tests pass.

\section{Time-driven}
We can expect that the invariants above also hold for the time-driven implementation. The property-test is exactly the same, with the time-driven implementation running instead of the even-driven one. A big difference is that is not necessary to check the property of monotonic increasing time, as it is an invariant statically guaranteed by arrowized FRP through the Yampa implementation. Due to the fact that the flow of time is always implicitly forward and no time-variable is explicitly made accessible within the code, it is not possible to violate the forward flow of time. Because of this, there is no need to check this property explicitly.

When we run the property-test we get a big surprise though: after a few test-cases the property-test fails due to a violation of the invariants! After a little bit of investigation it becomes clear that the invariant \textit{(3) number of susceptible agents is monotonic decreasing} is violated: in the failing test-case the number of susceptible agents is monotonic decreasing with the exception of one step where it \textit{increases} by 1 just to decrease by 1 in the next step. A coverage test reveals that this happens in about 66\% of 1,000 test-cases. How can this happen?

\medskip

The source of the problem is the use of \textit{dpSwitch} in the implementation of the \textit{stepSimulation} function as shown in Chapter \ref{sec:timedriven_firststep}. The d in \textit{dpSwitch} stands for delayed observation, which means that the output of the switch at time of switching is the output of the \textit{old} signal functions \cite{courtney_yampa_2003}. Speaking more technically: \textit{dpSwitch} is non-strict in its switching event, which ultimately results in old signal functions being run on new output which were but produced by those old signal functions: the output of time-step t is only visible in the time-step t=t+dt but the signal functions at time-step t are actually one step ahead. This is particularly visible at $t = 0$ and $t = \Delta t$, where the outputs are the same but the signal functions are not: the one at $t = \Delta t$ has changed already.

This has the desired effect that in the case of our SIR implementation, the population the agents see at time $t = t + \Delta t$ is the one from the previous step t, generated with the signal functions at time $t$. Due to the semantics of \textit{dpSwitch}, in the next switching event those signal functions from time $t$ are run again with the new input to produce the next output \textit{and} the new signal functions - in each step output is produced but due to the delay of \textit{dpSwitch} and the use of \textit{notYet}, we get this alternating behaviour.

This leads to trouble if the very rare case happens when a susceptible agent makes the transition from susceptible to recovered within one time-step. This is indeed possible due to the semantics of \textit{switch}, which is employed to make the state-transitions. In case a switching event occurs, \textit{switch} runs the signal function into which was switched immediately, which makes it highly unlikely but possible, that the susceptible agent, which has just switched into infected  recovers immediately by making the \textit{switch} to recovered. Why does this violate the property then?

\medskip

Lets assume that at $t = t + \Delta t$ the agents receive as input the population from time $t$ which contains, say 42, susceptible agents. A susceptible agent then makes the highly unlikely transition to recovered, reducing the number of susceptible agents by 1 to 41. In the next step the old signal function is run again but with the new input, which is slightly different, thus leading to slightly different probabilities. A susceptible agent has become recovered, which reduces probabilities of a susceptible agent becoming infected. When now the old signal function is run, it leads to a different output due to different probabilities: the susceptible agent stays susceptible instead of becoming infected (and then recovering). 

One way to solve this problem would be to use \textit{pSwitch}. It is the non-delayed, stricht version of \textit{dpSwitch}, which output at time of switching is the output of the \textit{new} signal functions. Using \textit{pSwitch} instead of \textit{dpSwitch} solves the problem because it will use the new signal functions in the second run because it is strict. Indeed, when using this, the property-test passes. This comes though at a high cost: due to \textit{pSwitch} strict semantics, which runs all the signal functions before and at time of switching, all agents are run twice in each step! This is clearly an unacceptable solution, especially because the time-driven approach already suffers severe performance problems. A more performant solution is to delay the susceptible agents output, as we have done already in \ref{sec:timedriven_firststep}. This solves the problem as well and the property-test passes. 

\section{Comparing time- and event-driven}
TODO: run event-driven properly with full random values, compare three runs:
-> MakeContact 1.0 dt with fixed number of contacts
-> MakeContact 1.0 dt with number of contacts exponentially distributed
-> MakeContact exponential dt with fixed number of contacts

Having two conceptually different implementations of the same model, an obvious idea is to compare the outputs of the two and verify whether both are producing the same dynamics or not. Speaking more technically, we are checking whether both simulations produce the same distributions under random model parameters and simulation time. We use QuickCheck to generate random values for $\beta$ (contact rate), $\gamma$ (infectivity) and $\delta$ (illness duration) as well as a random population. Then both simulation types are run with the same random parameters and random duration for 100 replications, collecting the output in the final step. The samples of these replications are then compared using a Mann-Whitney test with a 95\% confidence (p-value of 0.05). The reason for choosing this statistical test over e.g. a two sample t-test is that the Mann-Whitney test does not require the samples to be normally distributed, whereas the t-test assumes this. We know that the both implementations produce a bi-modal distribution as already discussed in \cite{macal_agent-based_2010} thus we cannot employ a two sample t-test to compare the distributions.
Our initial assumption is that we will reach a coverage of 90-95\%, meaning that this percentage of tests pass, indicating similar distributions. When running the default 100 random-test case though only in 81\% of the test-cases the difference in the distributions are statistically not significant and thus pass the Mann-Whitney test. Although this gives some robust evidence that both implementations produce similar distributions, they are not as close as we initially assumed. We look into this more in-depth in the verification against the SD specification in Chapter \ref{ch:prop_sirspec}.

TODO: more discussion about this! this is the place where we can really introduce that. also macal only discusses event-driven and not time-driven, why can we then assume that time-driven also produces a bi-modal distribution? show some histograms of failing test-cases with t-tests

\section{Discussion}
TODO

In the case of the time-driven implementation we saw that our initial assumption, that the invariants will hold for this implementation as well was wrong: QuickCheck revealed a \textit{very} subtle bug in our implementation. Although the probability of this bug is very low, QuickCheck found it due to its random testing nature. This is another \textit{strong} evidence, that random property-based testing is an \textit{excellent} approach for testing Agent-Based Simulations. On the other hand, this bug revealed the difficulties in getting the subtle semantics of FRP right to implement pure functional ABS. This is a strong case that in general an event-driven approach should be preferred, which is also much faster and also not subject to the sampling issues discussed in Chapter \ref{sec:timedriven_firststep}.